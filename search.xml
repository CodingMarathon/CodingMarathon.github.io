<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>crf</title>
      <link href="/2020/03/26/crf/"/>
      <url>/2020/03/26/crf/</url>
      
        <content type="html"><![CDATA[<p>﻿@<a href="目录">toc</a></p><h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h1><p>​        条件随机场（conditional random field，CRF）是给定一组输入<strong>随机变量</strong>条件下另一组输出<strong>随机变量</strong>的条件概率分布模型，其特点是假设输出随机变量构成马尔科夫随机场。本文主要讨论它在标注问题上的应用，因此主要讲<strong>线性链（linear chain）条件随机场</strong>，这时问题变成了由输入序列对输出序列预测的判别模型，形式为对数线性模型，学习模型参数的方法通常是极大似然估计或者正则化的极大似然估计。</p><p>​        本文参考李航老师的《统计学习方法》,加上自己的理解。</p><h1 id="1-马尔科夫随机场"><a href="#1-马尔科夫随机场" class="headerlink" title="1 马尔科夫随机场"></a>1 马尔科夫随机场</h1><p>​        概率无向图模型（probabilistic undirected graphical model），又称为马尔科夫随机场（Markov random field），是一个可以由无向图表示的联合概率分布。</p><h2 id="1-1-概率图模型的定义"><a href="#1-1-概率图模型的定义" class="headerlink" title="1.1 概率图模型的定义"></a>1.1 概率图模型的定义</h2><p>​         图是由节点及连接节点的边组成的集合。节点和边分别记作$v$和$e$，节点和边的集合分别记作$V$和$E$，图记作$G=(V,E)$。无向图是指边没有方向的图，有向图是指边有方向的图。<br><img src="https://img-blog.csdnimg.cn/20200323164917172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>        概率图模型是用图来表示<strong>概率分布</strong>，设有联合概率分布$P(\boldsymbol{Y})$，$\boldsymbol{Y}$是一组随机变量。用图$G=(V,E)$来表示概率分布$P(\boldsymbol{Y})$。在图$G$中，节点$v \in V $表示一个随机变量$Y_v$，则$\boldsymbol{Y}=(Y_v)_{v \in V}$；边$e \in E$表示随机变量之间的概率依赖关系。常用的概率图模型分为两类：</p><ul><li>有向图：贝叶斯网络。信念网络</li><li>无向图：马尔科夫随机场、马尔科夫网络  </li></ul><p><strong>概率图模型是一门很深的学问，在大学里面也是专门的一门课来讲。这篇文章仅仅是代表我自己的认知，也仅仅限于标注模型。</strong><br><img src="https://img-blog.csdnimg.cn/2020032316495293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>现在已经用图来表示概率分布，那怎么根据图来计算概率分布呢？</p><ol><li><p>概率有向图模型的联合概率：</p><p>概率有向图模型的联合概率一般按照如下所示的公式求解：</p></li></ol><pre><code>$$P(X_1,X_2,...,X_N)=\prod\limits_{i=1}^NP(X_i|\pi(X_i))$$其中， $\pi(X_i)$是$X_i$的所有父节点，**按理说应该是$P(\boldsymbol{Y})$来表示概率，但是画图时用的是$X$，这是一个小错误**。![在这里插入图片描述](https://img-blog.csdnimg.cn/2020032316502081.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center) 由图可得：</code></pre><p>$$<br>P(X_1,X_2,…,X_5)=P(X_1)P(X_2|X_1)P(X_3|X_2)P(X_4|X_2)P(X_5|X_3,X_4)​<br>$$</p><ol start="2"><li><p>概率无向图模型的联合概率：</p><p>无向图一般就指的是马尔科夫网路，无向图的无向性导致不能用条件概率参数化表示联合概率，<em><u>而是需要从一组条件独立的原则中找出一系列局部函数的乘积来表示联合概率。</u></em></p></li></ol><p><img src="https://img-blog.csdnimg.cn/20200323165635475.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>如果无向图是一个很大的图，可以使用<strong>因子分解</strong>将$P(\boldsymbol{X})$写成若干个联合概率的乘积，所以问题来了，怎么分解呢？将图分解为若干个“小团”，注意每一个团必须是<strong>最大团</strong>（定义在后面介绍），用$Q\in C$表示，$C$是最大团的集合。下面给出$P(\boldsymbol{X})$的公式，<strong>也叫因子分解式</strong>:<br>$$<br>P(\boldsymbol{X})=\frac{1}{Z}\prod\limits_{Q \in C}\psi_Q(\boldsymbol{x}_Q)<br>$$<br>其中，$Z=\sum\limits_\boldsymbol{x}\prod\limits_{Q\in C}\psi_Q(\boldsymbol{x_Q})$为规范化因子，是为了保证概率之和为1，类似于softmax。因为在计算上式时并<strong>不是按照概率</strong>来计算的，而是定义了<strong>特征函数</strong>(见后文)，数值代表不了概率。在实际应用中，精确计算$Z$是很困难的，但许多任务往往并不需要获得$Z$的精确值。$\psi_Q(\boldsymbol{x_Q})$是一个最大团$Q$所包含的<strong>所有随机变量</strong>的联合概率，也称作<strong>势函数</strong>，这个函数必须是非负实数函数，一般取指数函数：<br>$$<br>\psi_Q(\boldsymbol{x}_Q)=\exp\{-E(Y_Q)\}<br>$$</p><h2 id="1-2-马尔科夫随机场"><a href="#1-2-马尔科夫随机场" class="headerlink" title="1.2 马尔科夫随机场"></a>1.2 马尔科夫随机场</h2><p>先给一个简单的马尔科夫随机场。<br><img src="https://img-blog.csdnimg.cn/2020032316533986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ol><li><p><strong>团的定义</strong>：</p><p>对于图中节点的一个子集，若任意子集中任意两个节点间都有边连接，则称该节点子集为一个“团”（clique）。若在一个团中加入任何一个节点都不再形成团，则称这个团为“最大团”。最小的最大团只有两个节点，如上图中的$\{x_2,x_4\}$。</p><p>团：$\{x_1,x_2\},\{x_1,x_3\},\{x_2,x_4\},\{x_2,x_5\},\{x_2,x_6\},\{x_3,x_5\},\{x_5,x_6\},\{x_2,x_5,x_6\}$</p><p>最大团：$\{x_1,x_2\},\{x_1,x_3\},\{x_2,x_4\},\{x_3,x_5\},\{x_2,x_5,x_6\}$</p></li></ol><p>按照上述的公式求解概率，只需用到最大团。原因：若$Q$不是最大团，则它一定会被一个另一个最大团$Q^<em>$完全包含，即$Q\in Q^</em>$，这意味着$Q$包含的变量之间的关系不仅体现在势函数$\psi_Q$中，还体现在$\psi_{Q^<em>}$中，这样就乱套了，所以联合概率<em>*必须使用最大团</em></em>来定义。</p><p>上述这个图的联合概率为：<br>$$<br>P(\boldsymbol{X})=\frac{1}{Z}\psi_{12}(x_1,x_2)\psi_{13}(x_1,x_3)\psi_{24}(x_2,x_4)\psi_{35}(x_3,x_5)\psi_{256}(x_2,x_5,x_6)<br>$$<br>其中，$\psi_{256}(x_2,x_5,x_6)$定义在最大团$\{x_2,x_5,x_6\}$上，由于它的存在不需要为团$\{x_2,x_5\},\{x_2,x_6\},\{x_5,x_6\}$构建势函数。</p><p>​        在马尔科夫随机场中，还需要得到“条件独立性”，即马尔科夫性，同样借助分离的概念。马尔科夫性是保证或者判断概率图是否为概率无向图的条件，一共三点内容：</p><ul><li><p>全局马尔科夫性</p></li><li><p>局部马尔科夫性</p></li><li><p>成对马尔科夫性</p><p>注：这个只作为判断一个概率图是不是马尔科夫随机场，可以不用重点关注。</p></li></ul><ol start="2"><li><p><strong>全局马尔科夫性</strong>：</p><p>分离集的定义：若从节点集A的节点到B中的节点都必须经过节点集C中的节点，则称节点集A和B被节点集C分离，C成为分离集。</p><p><img src="https://img-blog.csdnimg.cn/20200323165359343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>全局马尔科夫性：给定两个变量子集的分离集，则这两个变量子集条件独立。记作：<br>$$<br>P(x_A,x_B|x_C)=P(x_A|x_C)P(x_B|x_C)<br>$$<br>相关证明可以查看《机器学习_周志华》</p></li><li><p><strong>局部马尔科夫性</strong>：</p><p>给定某变量的邻接变量，则该变量条件独立于其他变量，设$W$是与v有边连接的所有节点集合，$O$是$v$，$W$以外的其他所有节点集合，则：<br>$$<br>P(X_V,X_O|X_W)=P(X_V|X_W)P(X_O|X_W)<br>$$<br>若$P(X_O|X_W)&gt;0$，则有：<br>$$<br>P(X_V|X_W)=P(X_V|X_W,X_O)<br>$$</p></li><li><p><strong>成对马尔科夫性</strong>：</p><p>设$u$和$v$是无向图中的任意两个没有边连接的节点，其他所有节点集合为$O$，成对马尔科夫性是指给定随机变量组$X_O$的条件下随机变量$X_u$和$X_v$是条件独立的，即：<br>$$<br>P(X_u,X-v|X_O)=P(X_u|X_O)P(X_v|X_O)<br>$$</p></li></ol><p>针对上述的定义，现在给出概率无向图模型的定义：</p><p>​        <strong>设有联合概率分布$P(\boldsymbol{X})$，由无向图$G=(V,E)$表示，在途中，节点表示随机变量，边表示随机变量之间的依赖关系。如果连个概率分布满足成对、局部和全局马尔科夫性，就称此联合概率分布为概率无向图模型，或马尔科夫随机场。</strong></p><h1 id="2-条件随机场"><a href="#2-条件随机场" class="headerlink" title="2 条件随机场"></a>2 条件随机场</h1><p>​        条件随机场是给定随机变量$\boldsymbol{X}$条件下，随机变量$\boldsymbol{Y}$的马尔科夫随机场。这里主要介绍定义在线性链上的特殊的条件随机场，成为线性链条件随机场。注意条件随机场是<strong>判别模型</strong>，是对条件概率$P(\boldsymbol{Y}|\boldsymbol{X})$进行建模。线性链条件随机场可以用于标注问题，这时，在条件概率模型$P(\boldsymbol{Y}|\boldsymbol{X})$中，$\boldsymbol{Y}$是输出变量，表示标记序列，也可以称为状态序列，$\boldsymbol{X}$是输入变量，表示需要标注的观测序列。在学习模型参数时，利用训练数据及通过极大似然估计或正则化的极大似然估计得到条件概率模型$\hat{P}(\boldsymbol{Y}|\boldsymbol{X})$；预测时，对于给定的输入序列$\boldsymbol{x}$，求出条件概率$\hat{P}(\boldsymbol{y}|\boldsymbol{x})$最大的输出序列$\hat{\boldsymbol{y}}$。</p><h2 id="2-1-条件随机场的定义"><a href="#2-1-条件随机场的定义" class="headerlink" title="2.1 条件随机场的定义"></a>2.1 条件随机场的定义</h2><p>​        <strong>定义</strong>：设$\boldsymbol{X}$与$\boldsymbol{Y}$是随机变量，$P(\boldsymbol{Y}|\boldsymbol{X})$是在给定$\boldsymbol{X}$的条件下$\boldsymbol{Y}$的条件概率分布，若随机变量$\boldsymbol{Y}$构成了一个由无向图$G=(V,E)$表示的马尔科夫随机场，即<br>$$<br>P(Y_v|X,Y_w,w\neq v)=P(Y_v|X,Y_w,w \sim v)<br>$$<br>对任意节点$v$成立，则称条件概率分布$P(\boldsymbol{Y}|\boldsymbol{X})$为条件随机场，其中$w\neq v$表示节点$v$以外的所有节点，$w \sim v$表示在图中与节点$v$有边连接的所有节点$w$，$Y_w,Y_v,Y_u$是与节点对应的随机变量。</p><p>​        在定义中并没有要求$\boldsymbol{X}$与$\boldsymbol{Y}$具有想听的结构，现实中，一般假设$\boldsymbol{X}$与$\boldsymbol{Y}$有$\color{red}相同的图结构$，本文也只考虑无向图为下图所示的线性链的情况，用符号表示为：<br>$$<br>G=(V=\{1,2,3,…,n\},\quad E=\{(i,i+1)\})<br>$$<br><img src="https://img-blog.csdnimg.cn/20200323165429546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>​        在此情况下，最大团集合就是两个相邻的节点的集合。</p><p><strong>线性链条件随机场的定义</strong>：设$\boldsymbol{X}=(X_1,X_2,X_3,…,X_n),\boldsymbol{Y}=(Y_1,Y_2,Y_3,…,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列$\boldsymbol{X}$的条件下，随机变量序列$\boldsymbol{Y}$的条件概率分布$P(\boldsymbol{Y}|\boldsymbol{X})$构成条件随机场，即满足马尔科夫性：<br>$$<br>P(Y_i|\boldsymbol{X},Y_1,Y_2,..,Y_{i-1},Y_{i+1},…,Y_n)=P(Y_i|\boldsymbol{X},Y_{i-1},Y_{i+1})<br>$$<br>则称$P(\boldsymbol{Y}|\boldsymbol{X})$为线性链条件随机场，可以看出，$Y_i$<strong>只与观测序列当前值和前后状态相关</strong>。在标注问题中，$\boldsymbol{Y}$是输出变量，表示标记序列，也可以称为状态序列，$\boldsymbol{X}$是输入变量，表示需要标注的观测序列。</p><p>​        这里引用博文<a href="https://blog.csdn.net/zhoubl668/article/details/7786290" target="_blank" rel="noopener">随机场</a>中的一个例子：</p><pre class="line-numbers language-txt"><code class="language-txt">随机场包含两个要素：位置（site），相空间（phase space）。当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。我们不妨拿种地来打个比方。“位置”好比是一亩亩农田； “相空间”好比是种的各种庄稼。我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。好了，明白了上面两点，就可以讲马尔可夫随机场了。还是拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。马尔可夫随机场，描述了具有某种特性的集合。拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-2-条件随机场的参数化形式"><a href="#2-2-条件随机场的参数化形式" class="headerlink" title="2.2 条件随机场的参数化形式"></a>2.2 条件随机场的参数化形式</h2><p>​        根据前面介绍的概率无向图模型的因子分解式，线性链条件随机场的各因子是定义在相邻两个节点上的函数。设$P(\boldsymbol{Y}|\boldsymbol{X})$为线性链条件随机场，则在随机变量$\boldsymbol{X}$取值为$x$的条件下，随机变量$\boldsymbol{Y}$取值为$y$的条件概率具有如下形式：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\prod \limits_Q\psi_Q(x,y)\<br>&amp;=\frac{1}{Z(x)}\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}<br>\end{aligned}<br>$$<br>其中，<br>$$<br>Z(x)=\sum\limits_y\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}<br>$$<br>$t_k$和$s_l$是特征函数，$\lambda_k$和$\mu_l$是对应的权值，$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p><strong>推导过程</strong>（一定要看下面的过程，可以帮助你很好理解）：</p><p>上文介绍的概率无向图模型只有$X$一种随机变量，现在图中有$X,Y$两种随机变量，所以$P(X)=\frac{1}{Z}\prod\limits_Q\psi_Q(X_Q)$变成：<br>$$<br>\begin{aligned}<br>P(X,Y)&amp;=\frac{1}{Z}\prod\limits_Q\psi_Q(X_Q,Y_Q)\<br>&amp;=\frac{\prod\limits_Q\psi_Q(X_Q,Y_Q)}{\sum\limits_{X,Y}\prod\limits_Q\psi_Q(X_Q,Y_Q)}\<br>则：\<br>P(x,y)&amp;=\frac{\prod\limits_Q\psi_Q(x_Q,y_Q)}{\sum\limits_{x,y}\prod\limits_Q\psi_Q(x_Q,y_Q)}<br>\end{aligned}<br>$$</p><p>令$Z(x)=\sum\limits_y\prod\limits_Q\psi_Q(x_Q,y_Q)$，在归一化的时候，只需对每种$x$求一个归一化因子<br>$$<br>P(y|x)=\frac{1}{Z(x)}\prod \limits_Q\psi_Q(x_Q,y_Q)<br>$$<br>根据线性链条件随机场的定义，对于每一个因子，有两种情况：</p><ol><li>因子处于两个输出序列上，和当前位置的观测，以及前一个状态有关，还要对每一个对应的特征求连积$\psi_Q(x_Q,y_Q)=\psi_Q(y_{i-1},y_i,x,i)=\exp(\lambda_1t_1(y_{i-1},y_i,x,i))\exp(\lambda_2t_2(y_{i-1},y_i,x,i))···\exp(\lambda_kt_k(y_{i-1},y_i,x,i))=\exp(\sum\limits_k\lambda_kt_k(y_{i-1},y_i,x,i))$</li></ol><p><img src="https://img-blog.csdnimg.cn/2020032316552818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ol start="2"><li><p>因子处于观测序列和输出序列之间，和当前位置的观测有关，还要对每一个对应的特征求连积$\psi_Q(x_Q,y_Q)=\psi_Q(y_i,x,i)=\exp(\mu_1s_1(y_i,x,i))\exp(\mu_2s_2(y_i,x,i))···\exp(\mu_ls_l(y_i,x,i))=\exp(\sum\limits_l\mu_ls_l((y_i,x,i))$</p><p><img src="https://img-blog.csdnimg.cn/20200323165541835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li></ol><p>则：$\psi_Q(x_Q,y_Q)=\exp(\sum\limits_k\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_l\mu_ls_l((y_i,x,i))$</p><p>对每一个因子求连积，即对每一个位置$i$求乘积，对指数来说是求和：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\prod \limits_Q\psi_Q(x_Q,y_Q)\<br>&amp;=\frac{1}{Z(x)}\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}\<br>&amp;=\frac{\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}}{\sum\limits_y\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}}<br>\end{aligned}<br>$$<br>定义特征的原因：根据<strong>最大熵原则</strong>，希望找到一个符合所有边缘分布，同时使得熵达到最大的模型，这个模型就是指数函数，每一个边缘分布对应指数模型中的一个特征。最大熵会在后序博文给出。</p><ul><li>$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前位置和前一个位置，是人为定义的特征；</li><li>$s_l$是定义在边上的特征函数，称为状态特征，依赖于当前位置，是人为定义的特征；</li><li>$t_k,s_l$是都依赖于位置，是局部特征函数，通常取值为1或0；</li><li>条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$确定。</li></ul><h2 id="2-3-条件随机场的例子"><a href="#2-3-条件随机场的例子" class="headerlink" title="2.3 条件随机场的例子"></a>2.3 条件随机场的例子</h2><p>​        设有一标注问题：输入观测序列为$\boldsymbol{X}=(X_1,X_2,X_3)$，输出标记序列为$\boldsymbol{Y}=(Y_1,Y_2,Y_3)$，$Y_1,Y_2,Y_3$取值空间为$\{1,2\}$。<strong>==假设==特征函数</strong>$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$如下：<br>$$<br>t_1=t_1(y_{i-1}=1,y_i=2,x,i),\quad i=2,3,\quad \lambda_1=1<br>$$<br>这表示当当前位置$y$取值为2，上一位置为1，则特征函数$t$取值为1，取值为0的条件省略，即：<br>$$<br>t_1(y_{i-1}=1,y_i=2,x,i)=\begin{cases}1,&amp;  y_{i-1}=1,y_i=2,x,i,(i=2,3)\<br>0,&amp; 其他\end{cases}<br>$$<br><img src="https://img-blog.csdnimg.cn/2020032316571777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一共有5个特征函数$t$，4个特征函数$s$。</p><p>对给定的观测序列$x$，求标记序列为$y=(y_1,y_2,y_3)=(1,2,2)$的非规范化条件概率，也就是没有除以规范化因子的条件概率，即:</p><p>$\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}$  </p><p>解：</p><p>​        根据因子分解式：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;\propto\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}\<br>&amp;\propto \exp{[\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1,y_i,x,i})+\sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i,x,i)}]<br>\end{aligned}<br>$$<br>计算过程：</p><p>当$k=1$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=2}^3t_1(y_{i-1,y_i,x,i})&amp;=t_1(y_1=1,y_2=2,x,i=2)+t_1(y_2=2,y_3=2,x,i=3)\<br>&amp;=1+0=1<br>\end{aligned}<br>$$<br>当$k=2$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=2}^3t_2(y_{i-1,y_i,x,i})&amp;=t_2(y_1=1,y_2=2,x,i=2)+t_2(y_2=2,y_3=2,x,i=3)\<br>&amp;=0+0=0<br>\end{aligned}<br>$$<br>当$k=3$时：<br>$$<br>\begin{aligned}\sum\limits_{i=2}^3t_3(y_{i-1,y_i,x,i})&amp;=t_3(y_1=1,y_2=2,x,i=2)+t_3(y_2=2,y_3=2,x,i=3)\\&amp;=0+0=0\end{aligned}<br>$$<br>当$k=4$时：<br>$$<br>\begin{aligned}\sum\limits_{i=2}^3t_4(y_{i-1,y_i,x,i})&amp;=t_4(y_1=1,y_2=2,x,i=2)+t_4(y_2=2,y_3=2,x,i=3)\\&amp;=0+0=0\end{aligned}<br>$$<br>当$k=5$时：<br>$$<br>\begin{aligned}\sum\limits_{i=2}^3t_5(y_{i-1,y_i,x,i})&amp;=t_5(y_1=1,y_2=2,x,i=2)+t_5(y_2=2,y_3=2,x,i=3)\\&amp;=0+1=1\end{aligned}<br>$$<br>所以：<br>$$<br>\begin{aligned}<br>\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1,y_i,x,i})&amp;=\lambda_1\sum\limits_{i=2}^3t_1(y_{i-1,y_i,x,i})+\lambda_5\sum\limits_{i=2}^3t_5(y_{i-1,y_i,x,i})\<br>&amp;=1\times 1+0.2\times 1\<br>&amp;=1.2<br>\end{aligned}<br>$$<br>同理：</p><p>当$l=1$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_1(y_1=1,x,i=1)+s_1(y_2=2,x,i=2)+s_1(y_3=2,x,i=3)\<br>&amp;=1+0+0\<br>&amp;=1<br>\end{aligned}<br>$$<br>当$l=2$时：<br>$$<br>\begin{aligned}\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_2(y_1=1,x,i=1)+s_2(y_2=2,x,i=2)+s_2(y_3=2,x,i=3)\<br>&amp;=0+1+0\<br>&amp;=1\end{aligned}<br>$$<br>当$l=3$时：<br>$$<br>\begin{aligned}\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_3(y_1=1,x,i=1)+s_3(y_2=2,x,i=2)+s_3(y_3=2,x,i=3)\<br>&amp;=0+0+0\<br>&amp;=0<br>\end{aligned}<br>$$<br>当$l=4$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_4(y_1=1,x,i=1)+s_4(y_2=2,x,i=2)+s_4(y_3=2,x,i=3)\<br>&amp;=0+0+1\<br>&amp;=1<br>\end{aligned}<br>$$<br>所以：<br>$$<br>\begin{aligned}<br>\sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=\mu_1s_1+\mu_2s_2+\mu_4s_4\<br>&amp;=1\times 1+0.5\times 1+0.5\times 1\<br>&amp;=2<br>\end{aligned}<br>$$<br>故：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;\propto\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}\<br>&amp;\propto \exp{[\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1,y_i,x,i})+\sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i,x,i)}]\<br>&amp;\propto\exp(1.2+2)=\exp(3.2)<br>\end{aligned}<br>$$</p><h2 id="2-4-条件随机场的简化形式"><a href="#2-4-条件随机场的简化形式" class="headerlink" title="2.4 条件随机场的简化形式"></a>2.4 条件随机场的简化形式</h2><p>​        条件随机场还可以由简化形式表示.注意到条件随机场因子分解式中，同一特征在各个位置都有定义，可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><p>​        为简便起见，首先将转移特征和状态特征及其权值用统一的符号表示，设有$K_1$个转移特征，$K_2$个状态特征，$K＝K_1＋K_2$，记<br>$$<br>f_k(y_{i-1},y_i,x,i)=\begin{cases}t_k(y_{i_1},y_i,x,i),&amp;k=1,2,..,K_1\<br>s_l(y_i,x,i),&amp;k=K_1+l;\quad l=1,2,…,K_2<br>\end{cases}<br>$$<br>然后，对转移与状态特征在各个位置$i$求和，记作：<br>$$<br>f_k(y,x)=\sum\limits_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,…,K<br>$$<br>用$w_k$表示特征$f_k(y,x)$的权值，即：<br>$$<br>w_k=\begin{cases}\lambda_k,&amp;k=1,2,…,K_1\<br>\mu_l,&amp;k=K1+l;\quad l=1,2,…,K_2<br>\end{cases}<br>$$<br>于是，条件随机场就可以表示成：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\<br>Z(x)&amp;=\sum\limits_y\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}<br>\end{aligned}<br>$$<br>若以$\boldsymbol{w}$表示权值向量，即$\boldsymbol{w}=(w_1,w_2,…,w_k)^T$，以$F(\boldsymbol{y},\boldsymbol{x})$来表示全局特征向量，即$F(\boldsymbol{y},\boldsymbol{x})=(f_1(\boldsymbol{y},\boldsymbol{x}),f_2(\boldsymbol{y},\boldsymbol{x}),…,f_k(\boldsymbol{y},\boldsymbol{x}))^T$</p><p>则条件随机场可以写成向量$\boldsymbol{w}$与$F(\boldsymbol{y},\boldsymbol{x})$的内积的形式：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\exp{(\boldsymbol{w}\cdot F(\boldsymbol{y},\boldsymbol{x}))}\<br>Z(x)&amp;=\sum\limits_y\exp{(\boldsymbol{w}\cdot F(\boldsymbol{y},\boldsymbol{x}))}<br>\end{aligned}<br>$$</p><p>符号“$\cdot$”点积表示对应元素相乘后再相加。</p><h2 id="2-5-条件随机场的矩阵形式"><a href="#2-5-条件随机场的矩阵形式" class="headerlink" title="2.5 条件随机场的矩阵形式"></a>2.5 条件随机场的矩阵形式</h2><p>​        条件随机场还可以用矩阵来表示，假设$P_w(\boldsymbol{y}|\boldsymbol{x})$是由<br>$$<br>\begin{aligned}P(y|x)&amp;=\frac{1}{Z(x)}\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\\Z(x)&amp;=\sum\limits_y\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\end{aligned}<br>$$<br>给出的线性链条件随机场，表示对给定观测序列$\boldsymbol{x}$，相应的标记序列$\boldsymbol{y}$的条件概率。引进特殊的起点和终点状态标记$y_0=start,y_{n+1}=stop$，这时$P_w(\boldsymbol{y}|\boldsymbol{x})$可以通过矩阵形式表示。</p><p>​        对观测序列$\boldsymbol{x}$的每一个位置$i=1,2,…,n+1$，定义一个$m$阶矩阵（$m$是标记$\boldsymbol{y}$的取值的个数）：</p><p>$$<br>\begin{aligned}<br>M_i(x)&amp;=[M_i(y_{i-1},y_i|x)]\<br>M_i(y_{i-1},y_i|x)&amp;=\exp(W_i((y_{i-1},y_i|x))\<br>W_i((y_{i-1},y_i|x)&amp;=\sum\limits_{i=1}^Kw_kf_k(y_{i-1},y_i|x)<br>\end{aligned}<br>$$<br>​        这样，给定观测序列$\boldsymbol{x}$，标记序列$\boldsymbol{y}$的非规范化概率可以通过$n+1$个矩阵的乘积$\prod\limits_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示，每一次乘就代表了一次状态转移，于是，条件概率$P_w(\boldsymbol{y}|\boldsymbol{x})$是：<br>$$<br>P_w(\boldsymbol{y}|\boldsymbol{x})=\frac{1}{Z_w(x)}\prod\limits_{i=1}^{n+1}M_i(y_{i-1},y_i|x)<br>$$<br>其中，$Z_w(x)$是规范化因子，是$n+1$个矩阵的乘积的(start,stop)元素：<br>$$<br>Z_w(x)=(M_1(x)M_2(x)…M_{n+1}(x))_{start,atop}<br>$$<br>注意，$y_0=start$与$y_{n+1}=stop$表示开始状态与终止状态，规范化因子$Z_w(x)$是以start为起点stop为终点通过状态的所有路径$y_1y_2…y_n$的非规范化概率$\prod\limits_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。下面举个例子来说明。</p><p>例：给定一个由下图所示的线性链条件随机场，观测序列$\boldsymbol{x}$，标记序列$\boldsymbol{y}$，$i=1,2,3,n=3$，标记$y_i\in\{1,2\}$，假设$y_0=start=1,y_4=stop=1$，各个位置的随机矩阵为$M_1(x),M_2(x),M_3(x),M_4(x)$分别是：<br>$$<br>M_1(x)=\begin{bmatrix}<br>a_{01} &amp; a_{02}\<br>0 &amp; 0<br>\end{bmatrix},<br>M_2(x)=\begin{bmatrix}<br>b_{11} &amp; b_{12}\<br>b_{21} &amp; b_{22}<br>\end{bmatrix},<br>M_3(x)=\begin{bmatrix}<br>c_{11} &amp; c_{12}\<br>c_{21} &amp; c_{22}<br>\end{bmatrix},<br>M_4(x)=\begin{bmatrix}<br>1 &amp; 0\<br>1 &amp; 0<br>\end{bmatrix}<br>$$<br>试求状态序列$\boldsymbol{y}$以start为起点stop为终点所有路径的非规范化概率及规范化因子。</p><p><img src="https://img-blog.csdnimg.cn/20200323165759107.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>解：</p><p>从start到stop一共有8条路径：<br>$$<br>(1,1,1),(1,1,2),(1,2,1),(1,2,2),(2,1,1),(2,1,2)(2,2,1),(2,2,2)<br>$$<br>分别对应的非规范化概率为：<br>$$<br>a_{01}b_{11}c_{11},a_{01}b_{11}c_{12},a_{01}b_{12}c_{21},a_{01}b_{12}c_{22}\<br>a_{02}b_{21}c_{11},a_{02}b_{21}c_{12},a_{02}b_{22}c_{21},a_{02}b_{22}c_{12}<br>$$<br>根据规范化因子公式求解：<br>$$<br>\begin{aligned}Z_w(x)&amp;=(M_1(x)M_2(x)…M_{n+1}(x))_{start,atop}\<br>&amp;=M_1(x)M_2(x)M_3(x)M_4(x)\<br>&amp;=\begin{bmatrix}a_{01}b_{11}+a_{02}b_{21} &amp; a_{01}b_{12}+a_{02}b_{22}\<br>0&amp;0\end{bmatrix}M_3(x)M_4(x)\<br>&amp;=\begin{bmatrix}a_{01}b_{11}c_{11}+a_{02}b_{21}c_{11}+a_{01}b_{12}c_{21}+a_{02}b_{22}c_{21}&amp;a_{01}b_{11}c_{12}+a_{02}b_{21}c_{12}+a_{01}b_{12}c_{22}+a_{02}b_{22}c_{22}\<br>0&amp;0\end{bmatrix}\begin{bmatrix}1&amp;0\\1&amp;0\end{bmatrix}\<br>&amp;=\begin{bmatrix}a_{01}b_{11}c_{11}+a_{02}b_{21}c_{11}+a_{01}b_{12}c_{21}+a_{02}b_{22}c_{21}+a_{01}b_{11}c_{12}+a_{02}b_{21}c_{12}+a_{01}b_{12}c_{22}+a_{02}b_{22}c_{22}&amp;0\<br>0&amp;0\end{bmatrix}<br>\end{aligned}<br>$$<br>其第一行第一列的元素为<br>$$<br>a_{01}b_{11}c_{11}+a_{02}b_{21}c_{11}+a_{01}b_{12}c_{21}+a_{02}b_{22}c_{21}+a_{01}b_{11}c_{12}+a_{02}b_{21}c_{12}+a_{01}b_{12}c_{22}+a_{02}b_{22}c_{22}<br>$$<br>恰好是从start到stop的所有路径的非规范概率之和，即$Z(x)$。从这个例子可以看出，要计算$Z(x)$需要考虑到所有可能的路径，当序列长度增加，状态数量增加，计算很困难。</p><h2 id="2-5-条件随机场的三个问题"><a href="#2-5-条件随机场的三个问题" class="headerlink" title="2.5 条件随机场的三个问题"></a>2.5 条件随机场的三个问题</h2><p>类似HMM模型，条件随机场也有三个问题</p><ul><li>概率计算问题</li><li>学习问题</li><li>预测问题</li></ul><h1 id="3-条件随机场的概率计算问题"><a href="#3-条件随机场的概率计算问题" class="headerlink" title="3 条件随机场的概率计算问题"></a>3 条件随机场的概率计算问题</h1><p>​        <strong>条件随机场的概率计算问题是给定条件随机场$P(Y|X)$，输入序列$\boldsymbol{x}$和输出序列$\boldsymbol{y}$，计算条件概率$P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望的问题</strong>。为了方便起见，像HMM模型那样，引进前向-后向向量，递归的计算以上概率及期望值，这个算法被称作前向-后向算法。这里的前向-后向算法和HMM模型很像，具体参考<a href="https://blog.csdn.net/Elenstone/article/details/104902120" target="_blank" rel="noopener">一文读懂NLP之隐马尔科夫模型</a>。</p><h2 id="3-1-前向-后向算法"><a href="#3-1-前向-后向算法" class="headerlink" title="3.1 前向-后向算法"></a>3.1 前向-后向算法</h2><p>​        对每个指标$i=0,1,2,…,n+1$，定义前向向量$\alpha_i(x)$:<br>$$<br>\alpha_0(y|x)=\begin{cases}1,&amp;y=start\<br>0,&amp;否则<br>\end{cases}<br>$$<br>​        递推公式：<br>$$<br>\alpha_i^T(y_i|x)=\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x),\quad i=1,2,3,…,n+1<br>$$<br>​        也可以表示为：<br>$$<br>\alpha_i^T(x)=\alpha_{i-1}^T(x)M_i(x)<br>$$<br>$\alpha_i(y_i|x)$表示在位置$i$的标记是$y_i$并且到位置$i$的<strong>前部分标记序列的非规范化概率</strong>（只是一个有概率意义的数值，不是真正意义的概率），$y_i$可取的值有$m$个，所以$\alpha_i(y_i|x)$是m维列向量。</p><p>​        同样，对每个指标$i=0,1,2,…,n+1$，定义后向向量$\beta_i(x)$:<br>$$<br>\beta_{n+1}(y_{n+1}|x)=\begin{cases}1,&amp;y_{n+1}=stop\<br>0,&amp;否则\end{cases}\<br>\beta_i(y_i|x)=M_i(y_i,y_{i+1}|x)\beta_{i-1}(y_{i+1}|x)<br>$$<br>​        又可以表示为：<br>$$<br>\beta_i(x)=M_{i+1}(x)\beta_{i+1}(x)<br>$$<br>$\beta_i(y_i|x)$表示在位置$i$的标记为$y_i$并且从$i+1$到$n$的后部分标记序列的非规范化概率。</p><p>​        由前向-后向向量定义可以得到：<br>$$<br>Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}=\boldsymbol{1}^T\cdot \beta_1(x)<br>$$<br>​        这里，$\boldsymbol{1}$是元素均为1的m维列向量。</p><h2 id="3-2-概率计算"><a href="#3-2-概率计算" class="headerlink" title="3.2 概率计算"></a>3.2 概率计算</h2><p>​            按照前向-后向向量的定义，很容易计算标记序列在位置$i$是标记$y_i$的条件概率和在位置$i-1$与$i$是标记$y_{i-1}$和$y_i$的条件概率:<br>$$<br>\begin{aligned}<br>P(Y_i=y_i|x)&amp;=\frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}\<br>P(Y_{i-1}=y_{i-1},Y_i=y_i|x)&amp;=\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}<br>\end{aligned}<br>$$<br>其中，$Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}$</p><h2 id="3-3-期望值的计算"><a href="#3-3-期望值的计算" class="headerlink" title="3.3 期望值的计算"></a>3.3 期望值的计算</h2><p>​        利用前向-后向向量，可以计算特征函数关于联合分布$P(X,Y)$和条件分布$P(Y|X)$的数学期望。</p><p>​        特征函数$f_k$关于条件分布$P(Y|X)$的数学期望是：<br>$$<br>\begin{aligned}<br>E_{P(Y|X)}[f_k]&amp;=\sum\limits_yP(y|x)f_k(y,x)\<br>&amp;=\sum\limits_{i=1}^{n+1}\sum\limits_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)},\quad k=1,2,…,K<br>\end{aligned}<br>$$<br>其中，$Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}$，从公式来理解，对特征函数求期望，就是对图中的每条边求期望，而一条边的出现跟这条边上的两个节点有关，对应特征$f_k(y_{i-1},y_i,x,i)$第$i$时刻的某条边，出现这条边的概率为$P(y_{i-1},y_1|x)$。</p><p>​        假设经验分布为$\tilde P(X)$，特征函数$f_k$关于联合分布$P(X,Y)$的数学期望是：<br>$$<br>\begin{aligned}<br>E_{P(X,Y)}[f_k]&amp;=\sum\limits_{x,y}P(x,y)\sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\<br>&amp;=\sum\limits_x\tilde P(x)\sum\limits_yP(y|x)\sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\<br>&amp;=\sum\limits_x\tilde P(x)\sum\limits_{i=1}^{n+1}\sum\limits_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)},\quad k=1,2,…,K<br>\end{aligned}<br>$$<br>其中，$Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}$。</p><p>​        上两个式子是特征函数数学期望的一般计算公式。对于转移特征$t_k$和状态特征$s_l$，可以将$f_k$换成$t_k,s_l$求相应的特征函数期望。</p><h1 id="4-条件随机场的学习算法"><a href="#4-条件随机场的学习算法" class="headerlink" title="4. 条件随机场的学习算法"></a>4. 条件随机场的学习算法</h1><p>​        条件随机场的学习算法：给定训练数据集，估计条件随机场模型参数的问题，即条件随机场的学习问题。条件随机场模型实际上是定义在时序数据上的对数线性模型，其学习方法包括极大似然估计和正则化的极大似然估计。具体的优化实现算法有改进的迭代尺度法IIS、梯度下降法以及拟牛顿法。</p><h2 id="4-1-改进的迭代尺度法"><a href="#4-1-改进的迭代尺度法" class="headerlink" title="4.1 改进的迭代尺度法"></a>4.1 改进的迭代尺度法</h2><p>​        已知训练数据集，由此可知经验概率分布$\tilde P(X,Y)$，可以通过极大化训练数据的对数似然函数来求模型参数。</p><p>​        训练数据的对数似然函数为：<br>$$<br>L(w)=L_{\tilde P}(P_w)=\log{\prod \limits_{x,y}P_w(y|x)^{\tilde P(x,y)}}=\sum\limits_{x,y}\tilde P(x,y)\log{P_w(y|x)}<br>$$<br>当$P_w$是下列式子：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\<br>Z(x)&amp;=\sum\limits_y\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}<br>\end{aligned}<br>$$<br>给出的条件随机场模型时，对数似然函数为：<br>$$<br>\begin{aligned}<br>L(w)&amp;=\sum\limits_{x,y}\tilde P(x,y)\log{P_w(y|x)}\<br>&amp;=\sum\limits_{x,y}[\tilde P(x,y)\sum\limits_{k=1}^Kw_kf_k(y,x)-\tilde P(x,y)\log{Z_w(x)}]\<br>&amp;=\sum\limits_{j=1}^N\sum\limits_{k=1}^Kw_kf_k(y_j,x_j)-\sum\limits_{j=1}^NZ_w(x_j)<br>\end{aligned}<br>$$<br>​        改进的迭代尺度法通过迭代的方法不断优化对数似然函数改变量的下界，达到极大化对数似然函数的目的。假设模型的当前参数向量为$\boldsymbol{w}＝(w_1,w_2,…,w_K)^T$，向量的增量为$\boldsymbol{\delta}＝(\delta)_1,\delta_2,…,\delta_K)^T$，更新参数向量$\boldsymbol{w}＋\boldsymbol{\delta}＝(w_1+\delta_1,w_2+\delta_2,..,w_K+\delta_K)$。在每步迭代过程中，改进的代尺度法通过依次求解下列式子$E_{\tilde P}[t_k]$和$E_{\tilde P}[s_l]$得到$\boldsymbol{\delta}=(\delta_1,\delta_2,…,\delta_K)^T$。</p><p>​        关于转移特征t的更新方程为：<br>$$<br>\begin{aligned}<br>E_{\tilde P}[t_k]&amp;=\sum\limits_{x,y}\tilde P(x,y)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\<br>&amp;=\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_kT(x,y))},\quad k=1,2,…,K_1<br>\end{aligned}<br>$$<br>​        关于状态特征s的更新方程为:<br>$$<br>\begin{aligned}<br>E_{\tilde P}[s_l]&amp;=\sum\limits_{x,y}\tilde P(x,y)\sum\limits_{i=1}^{n+1}s_l(y_i,x,i)\<br>&amp;=\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}s_l(y_i,x,i)\exp{(\delta_{K_1+l}T(x,y))},\quad k=1,2,…,K_2<br>\end{aligned}<br>$$<br>这里，$T(x,y)$是在数据$(x,y)$中出现的所有特征数的总和：<br>$$<br>T(x,y)=\sum\limits_kf_k(y,x)=\sum\limits_{k=1}^K\sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)<br>$$<br>算法过程：</p><ul><li>输入：特征函数$t_1,t_2,…,t_{K_1},s_1,s_2,…,s_{K_2}$；经验分布$\tilde P(x,y)$;</li><li>输出：餐数据及值$\hat{w}$；模型$P_{\hat{w}}$。</li></ul><ol><li><p>对所有$k\in{1,2,…,K}$，取初值$w_k=0$</p></li><li><p>对每一$k\in{1,2,…,K}$：</p><p>(a). 当$k=1,2,…,K_{1}$时，令$\delta_k$是方程<br>$$<br>\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_kT(x,y))}=E_{\tilde P}[t_k]<br>$$<br>的解</p><p>​        当$k=K_1+l,l=1,2,…,K_2$时，令$\delta_{k+l}$是方程<br>$$<br>\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}s_l(y_i,x,i)\exp{(\delta_{K_1+l}T(x,y))}=E_{\tilde P}[s_l]<br>$$<br>(b). 更新$w_k$值：$w_k\leftarrow w_k+\delta_k$</p></li><li><p>如果不是所有$w_k$都收敛，重复步骤2.</p><p>上式中，$T(x,y)$表示数据$(x,y)$中的特征总数，对不同的数据$(x,y)$取值可能不同，为了处理这个问题，定义松弛特征$s(x,y)=S-\sum\limits_{i=1}^{n+1}\sum\limits_{k=1}^Kf_k(y_{i-1},y_i,x,i)$，其中S是一个常数，选择足够大的常数S使得对训练数据集的所有数据$(x,y)$，$s(x,y)\geq 0$成立，这是特征总数可取S。</p><p>因此，对于转移特征$t_k,\delta_k$的更新方程是：<br>$$<br>\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_{K}S)}=E_{\tilde P}[t_k]\<br>\delta_k=\frac{1}{S}\log\frac{E_{\tilde P}[t_k]}{E_P[t_k]}<br>$$<br>其中，<br>$$<br>E_P(t_k)=\sum\limits_x\tilde P(x)\sum\limits_{i=1}^{n+1}\sum\limits_{y_{i-1}y_i}t_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}<br>$$<br>同样，对于状态特征$s_l,\delta_k$的更新方程是：</p></li></ol><pre><code>$$\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}s_l(y_{i-1},y_i,x,i)\exp{(\delta_{K+l}S)}=E_{\tilde P}[s_l]\\\delta_{k+l}=\frac{1}{S}\log\frac{E_{\tilde P}[s_l]}{E_P[s_l]}$$其中，$$E_P(s_l)=\sum\limits_x\tilde P(x)\sum\limits_{i=1}^{n}\sum\limits_{y_i}s_l(y_i,x,i)\frac{\alpha_{i}^T(y_{i}|x)\beta_i(y_i|x)}{Z(x)}$$以上算法称为算法S，在算法S中需要是常数S取足够大，这样一来，每部迭代的增量向量会变大，算法收敛会变慢。算法T视图解决这个问题，算法T对每个观测序列$x$计算其特征总数最大值$T(x)$:$$T(x)=\max\limits_yT(x,y)$$利用前向-后向递推公式，可以很容易地计算$T(x)=t$这时关于转移特征参数的更新方程可以写成:$$\begin{aligned}E_{\tilde P}[t_k]&amp;=\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_{K}T(x))}\\&amp;=\sum\limits_{x}\tilde P(x)\sum\limits_{y}P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_{K}T(x))}\\&amp;=\sum\limits_{x}\tilde P(x)a_{k,t}\exp(\delta_k\cdot t)\\&amp;=\sum\limits_{t=0}^{\max}a_{k,t}\beta_k^t\end{aligned}$$这里，$a_{k,t}$是特征$t_k$的期待值，$\delta_k=\log{\beta_k},\beta_k$是上述多项式方程唯一的实根，可以用牛顿法求得，从而得到相关的$\delta_k$。同样，可以用于关于状态特征的参数更新。</code></pre><h2 id="4-2-拟牛顿法"><a href="#4-2-拟牛顿法" class="headerlink" title="4.2 拟牛顿法"></a>4.2 拟牛顿法</h2><p>​        条件随机场模型学习还可以应用牛顿法或拟牛顿法。对于条件随机场模型<br>$$<br>P_w(y|x)=\frac{\exp{(\sum\limits_{i=1}^nw_if_i(x,y))}}{\sum\limits_y\exp{(\sum\limits_{i=1}^nw_if_i(x,y))}}<br>$$<br>学习的优化目标函数是：<br>$$<br>\min\limits_{w\in R^n}f(w)=\sum\limits_x\tilde P(x)\log \sum\limits_yexp(\sum\limits_{i=1}^nw_if_i(x,y))<br>$$<br>其梯度函数是：<br>$$<br>g(w)=\sum\limits_{x,y}\tilde P(x)P_w(y|x)f(x,y)-E_{\tilde P}(f)<br>$$<br>拟牛顿法的BFGS算法如下：</p><p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-cNyvgEa2-1584953186799)(F:\Markdown\image\image-20200323164004538.png)]</p><p>​        条件随机场的预测问题是给定条件随机场$P(Y|X)$和输入序列（观测序列）$x$，求条件概率最大的输出序列（标记序列）$y^<em>$，即对观测序列进行标注。条件随机场的预测算法是注明的维特比算法，参考<a href="https://blog.csdn.net/Elenstone/article/details/104902120" target="_blank" rel="noopener">一文读懂NLP之隐马尔科夫模型</a>。<br>$$<br>\begin{aligned}<br>y^</em>&amp;=\arg\max_yP(y|x)\<br>&amp;=\arg\max_y\frac{\exp(w\cdot F(y,x))}{Z_w(x)}\<br>&amp;=\arg\max_y\exp(w\cdot F(y,x))\<br>&amp;=\arg\max_y(w\cdot F(y,x))<br>\end{aligned}<br>$$<br>​        于是，条件随机场的预测问题称为求非规范化概率最大的最优路径问题：$\max\limits_y(w\cdot F(y,x))$，这里，路径表示标记序列。其中，<br>$$<br>\begin{aligned}<br>\boldsymbol{w}&amp;=(w_1,w_2,…,w_K)^T\<br>F(y,x)&amp;=(f_1(y,x),f_2(y,x),…,f_K(y,x))^T\<br>f_k(y,x)&amp;=\sum\limits_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,…,K<br>\end{aligned}<br>$$<br>注意，这时只需计算飞归发话概率，而不必计算概率，可以大大提高效率。为了求解最优路径，可以将$\max\limits_y(w\cdot F(y,x))$写成以下形式：<br>$$<br>\max\limits_y(w\cdot F_i(y_{i-1},y_i,x))<br>$$<br>其中，$F_i(y_{i-1},y_i,x)=(f_1(y_{i-1},y_i,x,i),f_2(y_{i-1},y_i,x,i),…,f_K(y_{i-1},y_i,x,i))^T$是局部特征向量。</p><h2 id="5-1-维特比算法"><a href="#5-1-维特比算法" class="headerlink" title="5.1 维特比算法"></a>5.1 维特比算法</h2><p>首先求出位置1的各个标记$j=1,2,…,m$的非规范化概率：<br>$$<br>\delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x),\quad j=1,2,…m<br>$$<br>一般的，由递推公式，求出到位置$i$的各个标记$l=1,2,…,m$的非规范化概率的最大值，同时记录非规范化概率最大值的路径<br>$$<br>\delta_i(l)=\max\limits_{1\leq j\leq m}\{\delta_{i-1}(j)+w\cdot F_i(y_{i-1}=j,y_i=l,x)\},\quad l=1,2,…,m\<br>\Psi_i(l)=\arg\max\limits_{1\leq j\leq m}\{\delta_{i-1}(j)+w\cdot F_i(y_{i_1}=j,y_i=l,x)\},\quad l=1,2,…,m<br>$$<br>知道$i=n$时终止，这时求得非规范化概率的最大值为$\max\limits_y(w\cdot F(y,x))=\max\limits_{1\leq j\leq m}\delta_n(j)$及最优路径的终点$y^<em>=\arg\max\limits_{1\leq j\leq m}\delta_n(j)$，再由此最优路径终点依次往前回溯$y_i^</em>=\Psi_{i+1}(y_{i+1}^<em>),i=n-1,n-2,…,1$求得最优路径$y^</em>=(y_1^<em>,y_2^</em>,…,y_n^*)^T$。以上就是维特比算法的思路流程。</p><p><img src="https://img-blog.csdnimg.cn/20200323165909892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="5-2-维特比算法举例"><a href="#5-2-维特比算法举例" class="headerlink" title="5.2 维特比算法举例"></a>5.2 维特比算法举例</h2><p>​        设有一标注问题：输入观测序列为$\boldsymbol{X}=(X_1,X_2,X_3)$，输出标记序列为$\boldsymbol{Y}=(Y_1,Y_2,Y_3)$，$Y_1,Y_2,Y_3$取值空间为$\{1,2\}$。假设特征函数$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$如下：<br>$$<br>t_1=t_1(y_{i-1}=1,y_i=2,x,i),\quad i=2,3,\quad \lambda_1=1<br>$$<br>这表示当当前位置$y$取值为2，上一位置为1，则特征函数$t$取值为1，取值为0的条件省略，即：<br>$$<br>t_1(y_{i-1}=1,y_i=2,x,i)=\begin{cases}1,&amp;  y_{i-1}=1,y_i=2,x,i,(i=2,3)\<br>0,&amp; 其他\end{cases}<br>$$<br><img src="https://img-blog.csdnimg.cn/20200323165949191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一共有5个特征函数$t$，4个特征函数$s$。</p><p>用i特比算法求给定输入序列$x$对应的最有输出序列$y^<em>=(y_1^</em>,y_2^<em>,y_3^</em>)$</p><p>求解最优路径，即求解$\max\sum\limits_{i=1}^3w\cdot F_i(y_{i-1},y_i,x)$</p><ol><li>初始化</li></ol><p>$$<br>\delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x),\quad j=1,2<br>$$</p><p>64422.png” style=”zoom:75%;” /&gt;</p><h2 id="5-2-维特比算法举例-1"><a href="#5-2-维特比算法举例-1" class="headerlink" title="5.2 维特比算法举例"></a>5.2 维特比算法举例</h2><p>​        设有一标注问题：输入观测序列为$\boldsymbol{X}=(X_1,X_2,X_3)$，输出标记序列为$\boldsymbol{Y}=(Y_1,Y_2,Y_3)$，$Y_1,Y_2,Y_3$取值空间为$\{1,2\}$。假设特征函数$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$如下：<br>$$<br>t_1=t_1(y_{i-1}=1,y_i=2,x,i),\quad i=2,3,\quad \lambda_1=1<br>$$<br>这表示当当前位置$y$取值为2，上一位置为1，则特征函数$t$取值为1，取值为0的条件省略，即：<br>$$<br>t_1(y_{i-1}=1,y_i=2,x,i)=\begin{cases}1,&amp;  y_{i-1}=1,y_i=2,x,i,(i=2,3)\<br>0,&amp; 其他\end{cases}<br>$$<br><img src="https://img-blog.csdnimg.cn/20200323170047742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一共有5个特征函数$t$，4个特征函数$s$。</p><p>用i特比算法求给定输入序列$x$对应的最有输出序列$y^<em>=(y_1^</em>,y_2^<em>,y_3^</em>)$</p><p>求解最优路径，即求解$\max\sum\limits_{i=1}^3w\cdot F_i(y_{i-1},y_i,x)$</p><ol><li>初始化</li></ol><p>$$<br>\delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x),\quad j=1,2<br>$$</p><p><img src="https://img-blog.csdnimg.cn/2020032317003261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>代码实现见我的<a href="https://github.com/CodingMarathon/All_Algorithm/tree/master/CRF" target="_blank" rel="noopener">Github</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>hmm</title>
      <link href="/2020/03/26/hmm/"/>
      <url>/2020/03/26/hmm/</url>
      
        <content type="html"><![CDATA[<p>@<a href="目录">TOC</a></p><h1 id="1-隐马尔科夫模型"><a href="#1-隐马尔科夫模型" class="headerlink" title="1 隐马尔科夫模型"></a>1 隐马尔科夫模型</h1><p>本文主要介绍NLP领域中很重要的一个模型——<strong>隐马尔科夫模型（Hidden Markov Model，HMM）</strong>。希望读完本文后，大家能够清楚地理解<strong>HMM</strong>并能够应用到实际中。  </p><p>隐马尔科夫模型是结构最简单的<strong>动态贝叶斯网（dynamic Bayesian network，也被称作有向图模型）</strong>，HMM是可以用于标注问题的统计数学模型，描述由隐藏的<strong>马尔科夫链</strong>随机生成观测序列的过程，属于<strong>生成模型</strong>。HMM模型在语音识别、自然语言处理、生物信息、模式识别等领域有广泛的应用。</p><h2 id="1-1-HMM解决的问题"><a href="#1-1-HMM解决的问题" class="headerlink" title="1.1 HMM解决的问题"></a>1.1 HMM解决的问题</h2><p>首先看看什么样的问题可以使用HMM模型解决。</p><p>使用HMM模型来解决的问题一般有两个特征：</p><ul><li>1） 问题是基于序列的，比如时间序列、状态序列。</li><li>2 ）问题中有两类数据，一类序列数据是可以观测到的，即<strong>观测序列</strong>；而另一类数据是不能观察到的，即<strong>隐藏状态序列</strong>，简称<strong>状态序列</strong>。</li></ul><p>如果问题有了这两个特征，那么这个问题一般可以使用HMM模型尝试解决，这样的问题在生活中是很多的。例如，说一句话，发出的声音是观测序列，想表达的意思是隐藏状态序列；写文章的过程可以想象为先在脑海中构思好一个满足语法的词性序列，然后再将每个词性填充为具体的词语。</p><h2 id="1-2-HMM模型的定义"><a href="#1-2-HMM模型的定义" class="headerlink" title="1.2 HMM模型的定义"></a>1.2 HMM模型的定义</h2><p>隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的<strong>马尔科夫链</strong>随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程（摘自《统计学习方法》）。隐藏的马尔科夫链随机生成的状态的序列，称为<strong>状态序列（state sequence）</strong>，记作$\boldsymbol{y}$；每个状态生成一个观测，而由此产生的观测的随机序列，称为<strong>观测序列（observation sequence）</strong>，记作$\boldsymbol{x}$。序列的每一个位置又可以看作一个时刻。<br><img src="https://img-blog.csdnimg.cn/20200316182133870.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="HMM模型示意图"></p><center><font size="4"><strong>HMM模型示意图</strong></font></center><h3 id="1-2-1HMM的两个假设"><a href="#1-2-1HMM的两个假设" class="headerlink" title="1.2.1HMM的两个假设"></a>1.2.1HMM的两个假设</h3><p>首先介绍一下马尔科夫假设：每个事件的发生概率只取决于前一个事件。将满足该假设的连续多个事件串联在一起，就构成了<strong>马尔科夫链</strong>。在NLP语境下，可以将事件具象为单词，于是马尔科夫模型就是二元语法。</p><ul><li>第一个假设：将马尔科夫假设作用于状态序列，假设当前状态$y_t$仅仅依赖于前一个状态$y_{t-1}$，连续多个状态构成隐马尔科夫链$\boldsymbol{y}$。数学表达式为：<br>  $$p(y_t|y_{t-1},x_{t-1},y_{t-2},x_{t-2},y_{t-3},x_{t-3},…,y_{1},x_1)=p(y_t|y_{t-1}), t=1,2,3,…,T$$</li></ul><p>有了隐马尔科夫链，如何建立与观测序列$\boldsymbol{x}$的联系呢？HMM做了第二个假设:</p><ul><li>第二个假设：任意时刻的观测$x_t$只依赖于该时刻的状态$y_t$，与其他时刻的状态或观测独立无关。数学表达式为：<br> $$p(x_t|y_T,x_T,y_{t-1},x_{t-1},…,y_{t+1},x_{t+1},y_t,y_{t-1},x_{t-1},…y_1,x_1)=p(x_t|y_t),t=1,2,3,…,T$$</li></ul><h3 id="1-2-2-HMM模型"><a href="#1-2-2-HMM模型" class="headerlink" title="1.2.2 HMM模型"></a>1.2.2 HMM模型</h3><p>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合。<br>$$Q=\{q_1,q_2,…,q_N\}，V=\{v_1,v_2,v_3,…,v_N\}$$<br>其中，N是可能的状态数，M是可能的观测数。<br>HMM模型用三元组来表示$\lambda=(\boldsymbol{\pi},A,B)$：</p><ul><li>$\boldsymbol{\pi}$ : 初始状态概率向量</li><li>A：状态转移概率矩阵</li><li>B：发射概率矩阵</li></ul><p><strong>系统怎么从零开始呢？</strong> 观测值是由隐藏状态产生的，所以系统最初应该是生成隐藏状态。</p><p><strong>初始概率向量</strong>指的是系统启动时进入的第一个状态$y_1$成为<strong>初始状态</strong>，$\boldsymbol{y}$有$N$种取值，从$Q$集合中选取一个，即$\boldsymbol{y} \in \{q_1,q_2,…,q_N\}$。由于$y_1$是第一个状态，是一个独立的离散随机变量，可以用$p(y_1|\boldsymbol{\pi})$来描述，$y_1$只由$\boldsymbol{\pi}$来控制，其中$\boldsymbol{\pi}=(\pi_1,\pi_2,\pi_3,…,\pi_N)^T,0\leq\pi_i\leq1,\sum\limits^{N}_{i=1}{\pi_i}$=1。$\boldsymbol{\pi}$是概率分布的参数向量，称为<strong>初始状态概率向量</strong>。给定$\boldsymbol{\pi}$，初始状态$y_1$的取值分布就确定了。以中文分词问题为例，采用{B,M,E,S}标记时，其中B代表一个词的第一个字，M代表词的中间字，E代表词的末尾字，S代表单字成词。$y_1$所有可能的取值及对应的概率如下：</p><p>$$p(y_1=B)=0.7$$</p><p>$$p(y_1=M)=0$$</p><p>$$p(y_1=E)=0$$</p><p>$$p(y_1=s)=0.3$$</p><p>则$\pi=[0.7,0,0,0.3]$，也就是说句子第一个字可能是一个单字或者一个词的首字，不可能是一个词的中间或者尾字。</p><p>$y_1$<strong>确定之后，怎么产生</strong>$x_1$<strong>呢？如何确定</strong>$x_1$<strong>的概率分布呢？</strong></p><p>根据第二个假设：当前观测值$x_1$仅取决于当前的状态$y_1$，对于从$Q$中取出的每一种状态$y_1$，$x_1$都可以从$V$集合中的$M$个值选一个，所以对于每一个$y,x$都是一个独立的离散随机变量，其概率参数对应一个向量，维度为$M$，即$\boldsymbol{x} \in \{v_1,v_2,…,v_N\}$。由于一共有$N$种$y$，所以这些向量构成了一个$N\times M$矩阵，称为<strong>发射概率矩阵</strong>$\boldsymbol{B}$。<br>$$\boldsymbol{B}=[b_{ij}]_{N\times M}=[p(x_t=v_i|y_t=q_j)]_{N\times M}$$</p><p>其中，$p(x_t=v_i|y_t=q_j)$代表t时刻，隐藏状态$y_t$是$q_j$，由这个状态产生的观测值$x_t$等于$v_i$的概率。</p><table><thead><tr><th>状态</th><th>观测值1</th><th>观测值2</th><th>…</th><th>观测值M</th></tr></thead><tbody><tr><td>状态1</td><td></td><td></td><td></td><td></td></tr><tr><td>状态2</td><td></td><td></td><td></td><td></td></tr><tr><td>…</td><td></td><td></td><td></td><td></td></tr><tr><td>状态N</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>发射概率矩阵是一个非常形象的术语：可以将$\boldsymbol{y}$想象成为不同的彩弹枪，$\boldsymbol{x}$为不同颜色的子弹，每把彩弹枪内的颜色子弹比例不一样，导致有的彩弹枪红色子弹较多比较容易发射红色彩弹，一些彩弹枪绿色子弹较多更容易发射绿色彩弹。<br>发射概率在中文分词中也具有实际意义，有些字符构词的位置比较固定，比如一把作为词首的枪，不容易发射出“餮”，因为“餮”一般作为“饕餮”的词尾出现。通过给$p(x_1=餮|y_1=B)$较低的概率，HMM模型可以有效的防止“饕餮”被错误的切分。</p><p>$y_1$<strong>确定之后，如何转移状态到</strong>$y_2$<strong>?乃至</strong>$y_n$<strong>？</strong></p><p>根据HMM模型的第一个假设：$t+1$时刻的状态仅仅取决于$t$时刻的状态。类似发射概率矩阵，对于$t$时刻的每一种状态，$y_{t+1}$是一个离散的随机变量，取值有$N$种。$t$时刻一共可能有$N$种状态，所以从$t$时刻到$t+1$时刻的状态转移矩阵为$N\times N$的方阵，称为<strong>状态转移概率矩阵</strong>$\boldsymbol{A}$:<br>$$\boldsymbol{A}=[a_{ij}]_{N\times N}=[p(y_{t+1}=v_j|y_t=v_i)]_{N\times N}$$</p><p>其中，$p(y_{t+1}=s_j|y_t=s_i)$表示$t$时刻的状态为$v_i$，$t+1$时刻的状态为$v_j$的概率。</p><table><thead><tr><th>当前状态</th><th>下一状态是状态1</th><th>状态2</th><th>…</th><th>状态N</th></tr></thead><tbody><tr><td>状态1</td><td></td><td></td><td></td><td></td></tr><tr><td>状态2</td><td></td><td></td><td></td><td></td></tr><tr><td>…</td><td></td><td></td><td></td><td></td></tr><tr><td>状态N</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>例如，在中文分词中，标签B后面不可能是S，于是给$p(y_{t+1}=S|y_t=B)=0$就可以防止B后面接S的情况出现。</p><p><strong>初始状态概率向量、状态转移概率矩阵与发射概率矩阵</strong>称为HMM模型的三元组$\lambda=(\boldsymbol{\pi},A,B)$，只要三元组确定了，HMM模型就确定了。</p><p>举一个经典的例子：<br>假设有四个盒子，每个盒子里都装有红白两种颜色的球，如下表：</p><table><thead><tr><th>盒子</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>红球数</td><td>5</td><td>3</td><td>6</td><td>8</td></tr><tr><td>白球数</td><td>5</td><td>7</td><td>4</td><td>2</td></tr></tbody></table><p><img src="https://img-blog.csdnimg.cn/20200317083002437.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="盒子示意"><br>按照下面的方法抽球，产生一个球的颜色观察序列：</p><ol><li><p>从4个盒子里以等概率随机选取一个盒子，从这个盒子里随机抽取一个球，记录颜色后，放回；</p></li><li><p>从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1，那么下一个盒子一定是盒子2，如果当前是盒子2或者3，那么分别以概率0.4和0.6转移到左边或右边的盒子，如果当前盒子是4，那么各以0.5的概率停留在盒子4或者转移到盒子3；</p></li><li><p>确定转移盒子后，再从这个盒子里随机抽取一个球，记录其颜色，放回；</p></li><li><p>重复2,3步骤3次</p><p>一共抽取出来5个球，得到一个球的颜色观察序列：<br>$$\boldsymbol{x}=\{\color{red}红，红，\color{black}白，白，\color{red}红\}$$</p><p>在这个过程中，观察者只能观测到球的颜色序列，观测不到球是从哪个盒子取出的，即观察不到盒子的序列。<br>这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球的颜色的观测序列（观测序列），前者是隐藏的，后者是可以观测的。根据所给的条件可以明确状态集合、观测集合、序列长度以及模型的三要素。</p><ul><li>状态集合是$Q=\{盒子1，盒子2，盒子3，盒子4\}，N=4$。</li><li>观测集合是$V=\{\color{red}红，\color{black}白\}$。</li><li>状态序列和观测序列的长度$T=5$，原因是一共观测了5次。</li><li>初始状态概率向量为$\boldsymbol{\pi}=(0.25,0.25,0.25,0.25)^T$，原因是第一次是等概率随机抽取一个盒子。</li><li>状态转移概率矩阵$A=\begin{bmatrix}<br>0 &amp; 1 &amp; 0 &amp; 0 \<br>0.4 &amp; 0 &amp; 0.6 &amp; 0\<br>0 &amp; 0.4 &amp; 0 &amp;0.6\<br>0 &amp; 0 &amp; 0.5 &amp; 0.5<br>\end{bmatrix}$</li><li>发射概率矩阵$B=\begin{bmatrix}<br>0.5 &amp; 0.5 \<br>0.3 &amp; 0.7 \<br>0.6 &amp; 0.4 \<br>0.8 &amp; 0.2 \<br>\end{bmatrix}$</li></ul></li></ol><h2 id="1-3-HMM模型的三个基本问题"><a href="#1-3-HMM模型的三个基本问题" class="headerlink" title="1.3 HMM模型的三个基本问题"></a>1.3 HMM模型的三个基本问题</h2><p>有了HMM模型之后，如何使用模型呢？HMM模型一个可以解决三个问题：</p><ol><li><strong>概率计算问题</strong>：给定模型参数$\lambda=(\boldsymbol{\pi},A,B)$，和一个观测序列$\boldsymbol{x}$,，计算在这个模型参数$\lambda$下，观测序列出现的最大概率，即$p(\boldsymbol{x}|\lambda)$的最大值。</li><li><strong>模型训练问题</strong>：给定训练集$(\boldsymbol{x}^{(i)},\boldsymbol{y}^{(i)})$，估计模型参数$\lambda=(\boldsymbol{\pi},A,B)$，使得在该模型下观测序列概率$p(\boldsymbol{x}|\lambda)$最大，即使用极大似然估计得方法估计参数。</li><li><strong>序列预测问题</strong>：也称为解码问题，已知模型参数$\lambda=(\boldsymbol{\pi},A,B)$，给定观测序列$\boldsymbol{x}$，求最有可能的状态序列$\boldsymbol{y}$，即求$p(\boldsymbol{y}|\boldsymbol{x})$的最大值。</li></ol><h1 id="2-概率计算问题及算法"><a href="#2-概率计算问题及算法" class="headerlink" title="2 概率计算问题及算法"></a>2 概率计算问题及算法</h1><p>概率计算问题，也就是在给定的模型参数三元组的条件生成观测序列的过程。给定模型参数$\lambda=(\boldsymbol{\pi},A,B)$和一个观测序列$\boldsymbol{x}$,，计算在这个模型参数$\lambda$下，观测序列出现的最大概率，即$p(\boldsymbol{x}|\lambda)$的最大值。先介绍概念上可行但计算上不行的<strong>直接计算法（暴力解法）</strong>，然后介绍<strong>前向算法</strong>与<strong>后向算法</strong>。</p><h2 id="2-1-直接计算法"><a href="#2-1-直接计算法" class="headerlink" title="2.1 直接计算法"></a>2.1 直接计算法</h2><p>给定模型参数$\boldsymbol{\lambda}=(\boldsymbol{\pi},A,B)$和一个观测序列$\boldsymbol{x}=\{x_1,x_2,x_3,…x_T\}$，计算观测序列出现的概率$p(\boldsymbol{x}|\lambda)$，最直接的方法就是按照概率公式直接计算，通过列举所有可能的长度为$T$的状态序列$\boldsymbol{y}=\{y_1,y_2,y_3,…,y_T\}$，求各个状态序列$\boldsymbol{y}$与观测序列$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$的联合概率$p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})$,，然后对所有可能的状态序列求和，得到$p(\boldsymbol{x}|\boldsymbol{\lambda})$。<br>状态序列$\boldsymbol{y}=(y_1,y_2,y_3,…,y_T)$发生的概率是：<br>$$p(\boldsymbol{y}|\boldsymbol{\lambda})=\pi_{y_1}a_{y_1y_2}a_{y_2y_3}…a_{y_{T-1}y_T}$$</p><p>对固定的状态序列$\boldsymbol{y}=(y_1,y_2,y_3,…,y_T)$并且观测序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$的概率是：<br>$$p(\boldsymbol{x}|\boldsymbol{y},\boldsymbol{\lambda})=b_{y_1x_1}b_{y_2x_2}b_{y_3x_3}…b_{y_Tx_T}$$</p><p>在给定HMM模型参数$\boldsymbol{\lambda}$的条件下，$\boldsymbol{x},\boldsymbol{y}$同时出现的联合概率为：<br>$$\begin{aligned}p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})&amp;=p(\boldsymbol{x}|\boldsymbol{y},\boldsymbol{\lambda})p(\boldsymbol{y}|\boldsymbol{\lambda})\<br>&amp;=\pi_{y_1}b_{y_1x_1}a_{y_1y_2}b_{y_2x_2}a_{y_2y_3}b_{y_3x_3}…a_{y_{T-1}y_T}b_{y_Tx_T}<br>\end{aligned}$$</p><p>然后，对所有可能的状态序列$\boldsymbol{y}$求和，得到观测序列$\boldsymbol{x}$的概率$p(\boldsymbol{x}|\lambda)$，即：<br>$$\begin{aligned}<br>p(\boldsymbol{x}|\lambda)&amp;=\sum\limits_\boldsymbol{y}p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})\<br>&amp;=\sum\limits_\boldsymbol{y}p(\boldsymbol{x}|\boldsymbol{y},\boldsymbol{\lambda})p(\boldsymbol{y}|\boldsymbol{\lambda})\<br>&amp;=\sum\limits_{y_1,y_2,y_3,…,y_T}\pi_{y_1}b_{y_1x_1}a_{y_1y_2}b_{y_2x_2}a_{y_2y_3}b_{y_3x_3}…a_{y_{T-1}y_T}b_{y_Tx_T}<br>\end{aligned}$$</p><p>利用这个公式计算量很大，复杂度是$O(TN^T)$，在实际中是不可行的。</p><h2 id="2-2-前向算法"><a href="#2-2-前向算法" class="headerlink" title="2.2 前向算法"></a>2.2 前向算法</h2><p>首先需要定义<strong>前向概率</strong>：给定HMM模型$\boldsymbol{\lambda}$，定义时刻$t$部分观测序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$且状态为$q_i$的概率为<strong>前向概率</strong>，记作：<br>$$\alpha_t(i)=p(x_1,x_2,x_3,…,x_t,y_t=q_i|\boldsymbol{\lambda})$$</p><p><strong>前向算法</strong>：</p><ul><li><p>输入：HMM模型参数$\boldsymbol{\lambda}$，观测序列$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$；</p></li><li><p>输出：观测序列概率$p(\boldsymbol{x}|\boldsymbol{\lambda})$。</p><p>算法步骤：</p><ol><li>初始：<br>根据$\boldsymbol{\pi}$生成$t=1$时刻的状态$y_1=q_i$，概率为$\pi_i$，并且根据<strong>发射概率矩阵</strong>$\boldsymbol{B}$由$y_1=q_i$生成$x_1$，概率为$b_{ix_1}$，则：<br>$$\alpha_1(i)=\pi_ib_{ix_1}$$</li></ol></li></ul><ol start="2"><li>递推：<br>当$t=2$时，根据<strong>状态转移概率矩阵</strong>$\boldsymbol{A}$，系统的状态由$y_1=q_j$变为$y_2=q_i$，概率为$a_{ji}$，$p(y_2=q_i,y_1=q_j,x_1|\boldsymbol{\lambda})=\alpha_1(j)a_{ji}$。不管$y_1$为什么状态，都可能转移到状态$y_2=q_i$，所以需要对$y_1$求和:$\sum\limits_{j=1}^N\alpha_1(j)a_{ji}$。根据$\alpha_t(i)$的定义，$t=2$时刻，由状态$y_2=q_i$产生观测值$x_2$的概率为$b_{ix_2}$，所以：<br>$$\alpha_2(i)=[\sum\limits_{j=1}^N\alpha_1(j)a_{ji}]b_{ix_2}$$</li></ol><p>对$t=1,2,3,…T-1$，有：<br>$$\alpha_{t+1}(i)=[\sum\limits^N_{j=1}\alpha_t(j)a_{ji}]b_{ix_{t+1}},\quad i=1,2,3,…N$$<br><img src="https://img-blog.csdnimg.cn/20200318090901998.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="3"><li>终止：<br> 时刻$t=T$：<br> 根据定义可知：$\alpha_T(i)$表示，$t=T$时刻，处于状态$y_T=q_i$，并且观察到的序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$的概率：<br> $$\begin{aligned}<br> \alpha_T(i)&amp;=p(x_1,x_2,x_3,…,x_T,y_T=q_i|\boldsymbol{\lambda})\<br> &amp;=p(\boldsymbol{x},y_T=q_i|\boldsymbol{\lambda})<br> \end{aligned}$$</li></ol><p>所以：</p><p>$$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits^N_{i=1}\alpha_T(i)$$</p><p>前向算法实际是基于“状态序列的路径结构”递推计算$p(\boldsymbol{x}|\boldsymbol{\lambda})$的算法。前向算法高效的关键是其局部计算前向概率，然后利用路径结构将前向概率递推到全局，得到$p(\boldsymbol{x}|\boldsymbol{\lambda})$。在$t=1$时刻计算$\alpha_1(i)$的$N$个值，而当$t\geq2$时，计算每一个\alpha_t(i)的值都会利用前$N$个$\alpha_{t-1}(i)$的值，减少计算量的原因在于每一次计算都直接引用前一个时刻的计算结果，避免了概率的重复计算，复杂度为$O(N<em>N</em>T)$，而不是直接算法的$O(TN^T)$。</p><p>例：上文中的盒子和球模型，状态集合为$Q=\{1,2,3\}$，观测集合为$V=\{红，白\}$，<br>$A=\begin{bmatrix}<br>0.5 &amp; 0.2 &amp; 0.3\<br>0.3 &amp; 0.5 &amp; 0.2\<br>0.2 &amp; 0.3 &amp; 0.5<br>\end{bmatrix}$ ，  $B=\begin{bmatrix}<br>0.5 &amp; 0.5 \<br>0.4 &amp; 0.6\<br>0.7 &amp; 0.3<br>\end{bmatrix}$ ， $\pi=(0.2,0.4,0.4)^T$<br>设$T=3$，$\boldsymbol{x}=(红，白，红)$，用前向算法来计算$p(\boldsymbol{x}|\boldsymbol{\lambda})$。<br>解：</p><ol><li>计算初值：<br> $$\alpha_1(1)=\pi_1b_{1x_1}=0.2\times0.5=0.10$$</li></ol><p>$$\alpha_1(2)=\pi_2b_{2x_1}=0.4\times0.4=0.16$$</p><p>$$\alpha_1(3)=\pi_3b_{3x_1}=0.4\times0.7=0.28$$</p><ol start="2"><li>递推计算：<br> $$\alpha_2(1)=[\sum\limits_{i=1}^3\alpha_1(i)a_{i1}]b_{1x_2}=[0.10\times0.5+0.16\times0.3+0.28\times0.2)]\times0.5=0.077$$</li></ol><p>$$\alpha_2(2)=[\sum\limits_{i=1}^3\alpha_1(i)a_{i2}]b_{2x_2}=[0.10\times0.2+0.16\times0.5+0.28\times0.3)]\times0.5=0.1104$$</p><p>$$\alpha_2(3)=[\sum\limits_{i=1}^3\alpha_1(i)a_{i3}]b_{3x_2}=[0.10\times0.3+0.16\times0.2+0.28\times0.5)]\times0.5=0.0606$$</p><p>$$\alpha_3(1)=[\sum\limits_{i=1}^3\alpha_2(i)a_{i1}]b_{1x_3}=0.04187$$</p><p>$$\alpha_3(2)=[\sum\limits_{i=1}^3\alpha_2(i)a_{i2}]b_{2x_3}=0.03551$$</p><p>$$\alpha_3(3)=[\sum\limits_{i=1}^3\alpha_2(i)a_{i3}]b_{3x_3}=0.05284$$</p><ol start="3"><li>终止： </li></ol><p>$$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{i=1}^3\alpha_3(i)=0.13022$$</p><h2 id="2-3-后向算法"><a href="#2-3-后向算法" class="headerlink" title="2.3 后向算法"></a>2.3 后向算法</h2><p>类似前向算法，定义：给定HMM模型$\boldsymbol{\lambda}$，定义时刻$t$状态为$q_i$的条件下，从$t+1到T$的部分观测序列为$x_{t+1},x_{t+2},x_{t+3},…,x_T$的概率为<strong>后向概率</strong>，记作：<br>$$\beta_t(i)=p(x_{t+1},x_{t+2},x_{t+3},…,x_T|y_t=q_i,\boldsymbol{\lambda})$$<br>后向算法：</p><ul><li><p>输入：HMM模型参数$\boldsymbol{\lambda}$，观测序列$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$；</p></li><li><p>输出：观测序列概率$p(\boldsymbol{x}|\boldsymbol{\lambda})$。</p><ol><li><strong>当$t=T$时</strong>:<br>$$\beta_T(i)=p(这里没东西|y_T=q_i,\boldsymbol{\lambda})=1$$<br>这里可以这么理解：按理说好需要看看T时刻过后是什么观测值，但是T时刻之后没有观测值了，不管T时刻状态是什么，之后就是没有观测值，所以没有观测值的概率为1，且与状态无关。</li></ol></li></ul><ol start="2"><li><strong>对$t=T-1$</strong>:<br>已知$\beta_T(j)$，根据HMM模型可知，$y_T=q_j$是由$y_{T-1}$转移而来的，假设$y_{T-1}=q_i$，转移的概率为$a_{ij}$:<br>根据$\beta_{T-1}(i)$的定义，时刻$T-1$的状态$y_{T-1}=q_i$，从T到T的观测序列为$x_T$，T时刻状态$y_T=q_j$生成$x_T$的概率为$b_{jx_T}$，则：<br>$$\beta_{T-1}(i)=\sum\limits^N_{j=1}a_{ij}b_{jx_T}\beta_T(j)$$<br>对$t=T-1,T-2,…,1$:<br>$$b_{jx_t}=p(x_t|y_t=q_j,\boldsymbol{\lambda}))$$</li></ol><p>$$a_{ij}=p(y_{t+1}=q_j|y_t=q_i,\boldsymbol{\lambda}))$$</p><p>$$\beta_{t+1}(j)=p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})$$</p><p>由状态$y_{t+1}=q_j$生成$t+1$时刻的观测值$x_{t+1}$:<br>$$\begin{aligned}<br>b_{jx_{t+1}}\beta_{t+1}(j)&amp;=p(x_{t+1}|y_{t+1}=q_j,\boldsymbol{\lambda})p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})\<br>&amp;=p(x_{t+1},x_{t+2},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})\end{aligned}$$</p><p>按照条件概率来理解：在$t+1$时刻状态为$y_{t+1}=q_j$的条件下，观察到$x_{t+1},x_{t+2},…,x_T$的概率为$b_{jx_{t+1}}\beta_{t+1}(j)$，由于$\beta_t(i)$与$t$时刻的状态$y_t$有关，根据HMM模型的第一个假设，$y_{t+1}$仅仅与$y_t$有关，且概率$a_{ij}$由状态转移矩阵矩阵A提供。$\beta_t(i)$表示在$t$时刻状态为$y_t=q_i$的条件下，观察到$x_{t+1},x_{t+2},…,x_T$的概率，这个概率若要用$\beta_{t+1}(j)$来表示，针对每一个$y_{t+1}=q_j$，都要乘以从$y_t=q_i$到$y_{t+1}=q_j$的状态转移概率$a_{ij}$，$y_{t+1}$一共有N种状态，所以：<br>$$\begin{aligned}<br>a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)&amp;=p(x_{t+1},x_{t+2},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})a_{ij}\<br>&amp;=p(x_{t+1},x_{t+2},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})p(y_{t+1}=q_j|y_t=q_i,\boldsymbol{\lambda}))\<br>&amp;=p(x_{t+1},x_{t+2},…,x_T,y_{t+1}=q_j,|y_t=q_i,\boldsymbol{\lambda})<br>\end{aligned}$$</p><p>$$\begin{aligned}\beta_t(i)&amp;=p(x_{t+1},x_{t+2},…,x_T|y_t=q_i,\boldsymbol{\lambda})\<br>&amp;=\sum\limits_{y_{t+1}}p(x_{t+1},x_{t+2},…,x_T,y_{t+1}=q_j,|y_t=q_i,\boldsymbol{\lambda})\<br>&amp;=\sum\limits_{j=1}^Na_{ij}b_{jx_{t+1}}\beta_{t+1}(j)<br>\end{aligned}$$<br>所以：<br>$$\beta_t(i)=\sum\limits_{j=1}^Na_{ij}b_{jx_{t+1}}\beta_{t+1}(j),\quad i=1,2,3,…,N$$<br><img src="https://img-blog.csdnimg.cn/20200318090609546.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="3"><li><strong>当$t=1$时</strong>，$\beta_1(i)=p(x_2,x_3,x_4,…,x_T|y_1=q_i,\boldsymbol{\lambda})$；按照步骤2的思想，针对$t=1$时刻的每一种状态$y_1=q_i$，都需要乘上初始状态概率向量和相应的发射概率矩阵。<br> $$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{i=1}^N\pi_ib_{ix_1}\beta_{t+1}(i)$$</li></ol><h2 id="2-4-一些概率与期望值的计算"><a href="#2-4-一些概率与期望值的计算" class="headerlink" title="2.4 一些概率与期望值的计算"></a>2.4 一些概率与期望值的计算</h2><p>利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式。</p><ol><li>给定模型$\boldsymbol{\lambda}$和观测$\boldsymbol{x}$，在时刻$t$处于状态$q_i$的概率，记作$\gamma_t(i)$：<br> $$\gamma_t(i)=p(y_t=q_i|\boldsymbol{x},\boldsymbol{\lambda})$$</li></ol><p>可以用过前向和后向算法来算：</p><p>$$\begin{aligned}<br>\gamma_t(i)=p(y_t=q_i|\boldsymbol{x},\boldsymbol{\lambda})=\frac{p(y_t=q_i,\boldsymbol{x}|\boldsymbol{\lambda})}{p(\boldsymbol{x}|\boldsymbol{\lambda})}<br>\end{aligned}$$</p><p>$\alpha_t(i)$定义为时刻$t$部分观测序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$且状态为$q_i$的概率，$\beta_t(i)$定义为时刻$t$状态为$q_i$的条件下，从$t+1到T$的部分观测序列为$x_{t+1},x_{t+2},x_{t+3},…,x_T$的概率，则两者相乘表示，观察序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$且状态为$q_i$的概率，数学表示为：<br>$$\alpha_t(i)\beta_t(i)=p(y_t=q_i,\boldsymbol{x}|\boldsymbol{\lambda})$$</p><p>$$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{y_t}p(y_t=q_i,\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{j=1}^N\alpha_t(i)\beta_t(i)$$</p><p>所以：</p><p>$$\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{p(\boldsymbol{x}|\boldsymbol{\lambda})}=\frac{\alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N\alpha_t(i)\beta_t(i)}$$</p><ol start="2"><li>给定HMM模型参数$\boldsymbol{\lambda}$和观测序列$\boldsymbol{x}$，在时刻$t$处于状态$q_i$且在时刻$t+1$处于状态$q_j$的概率，记作$\xi_t(i,j)$：<br> $$\xi_t(i,j)=p(y_t=q_i,y_{t+1}=q_j|\boldsymbol{x},\boldsymbol{\lambda})$$</li></ol><p>使用前向和后向算法来计算：<br>$$\begin{aligned}<br>\xi_t(i,j)&amp;=p(y_t=q_i,y_{t+1}=q_j|\boldsymbol{x},\boldsymbol{\lambda})\<br>&amp;=\frac{p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})}{p(\boldsymbol{x},\boldsymbol{\lambda)}}\<br>&amp;=\frac{p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})}{\sum\limits_{i=1}^N\sum\limits_{j=1}^Np(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})}<br>\end{aligned}$$</p><p>而$p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})$表示在模型参数下，观测序列$\boldsymbol{x}$以及$t$时刻状态为$q_i$且时刻$t+1$处于状态$q_j$的概率，$\alpha_t(i)=p(y_t=q_i,x_1,x_2,x_3,…,x_t|\boldsymbol{\lambda})$表示在模型参数下，观测序列$x_1,x_2,x_3,…,x_t$以及$t$时刻状态为$q_i$的概率，$\beta_{t+1}(j)=p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})$表示在模型参数和$t+1$时刻状态为$q_j$的条件下，观察序列为$x_{t+2},x_{t+3},…,x_T$，两者之间差一个从时刻$t到t+1$的状态转移概率以及时刻$t+1$状态$q_j$产生序列$x_{t+1}$的概率：<br>$$\begin{aligned}<br>p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})&amp;=p(y_t=q_i,x_1,x_2,…,x_t|\boldsymbol{\lambda})p(y_{t+1}|y_t,\boldsymbol{\lambda})\\&amp;\qquad p(x_{t+1}|y_{t+1},\boldsymbol{\lambda})p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})\<br>&amp;=\alpha_t(i)a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)<br>\end{aligned}$$</p><p>所以：<br>$$\xi_t(i,j)=\frac{\alpha_t(i)a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)}{\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_t(i)a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)}$$</p><p>3.将$\xi_t(i,j)$和$\gamma_t(i)$对各个时刻求和，可以得到一些有用的期望值：</p><ul><li>在观测$\boldsymbol{x}$下，状态$q_i$出现的期望值：$\sum\limits_{t=1}^T\gamma_t(i)$</li><li>在观测$\boldsymbol{x}$下,由状态$q_i$转移的期望值：$\sum\limits_{t=1}^{T-1}\gamma_t(i)$</li><li>在观测$\boldsymbol{x}$下,由状态$q_i$转移到状态$q_j$的期望值：$\sum\limits_{t=1}^{T-1}\xi_t(i,j)$</li></ul><h1 id="3-模型训练问题及算法"><a href="#3-模型训练问题及算法" class="headerlink" title="3 模型训练问题及算法"></a>3 模型训练问题及算法</h1><p>给定训练集$(\boldsymbol{x}^{(i)},\boldsymbol{y}^{(i)})$，估计模型参数$\lambda=(\boldsymbol{\pi},A,B)$，使得在该模型下观测序列概率$p(\boldsymbol{x}|\lambda)$最大。根据训练数据是否有状态序列数据分为：完全数据和非完全数据，分别使用监督学习和非监督学习实现。</p><h2 id="3-1-监督学习——最大似然估计"><a href="#3-1-监督学习——最大似然估计" class="headerlink" title="3.1 监督学习——最大似然估计"></a>3.1 监督学习——最大似然估计</h2><p>在监督学习中，我们使用极大似然法来估计HMM模型参数。<br>假设给定训练数据包含S个长度相同的观测序列$\{(\boldsymbol{x}^1,\boldsymbol{y}^1),(\boldsymbol{x}^2,\boldsymbol{y}^2),…,(\boldsymbol{x}^S,\boldsymbol{y}^S)\}$，使用极大似然法来估计HMM模型参数。</p><p><strong>初始状态概率向量的估计</strong>：<br>统计S个样本中，初始状态为$q_i$的频率。<br>$$\hat{\pi}_i=\frac{N_{q_i}}{S}$$<br>其中，$N_{q_i}$是初始状态为$q_i$的样本数量，S是样本的数量。</p><p><strong>状态转移概率矩阵的估计</strong>：<br>设样本中时刻t处于状态$q_i$，时刻t+1处于状态$q_j$的频数为$A_{ij}$，那么状态转移概率矩阵的估计为：<br>$$\hat{a}_{ij}=\frac{A_{ij}}{\sum\limits_{j=1}^NA_{ij}},\quad j=1,2,3,…,N;\quad i=1,2,3,…,N$$</p><p><strong>发射概率矩阵的估计</strong>：<br>设样本中状态为$i$并观测值为$j$的频数$B_{ij}$，那么状态为$i$观测为$j$的概率$b_{ij}$的估计为：<br>$$\hat{b}_{ij}=\frac{B_{ij}}{\sum\limits_{j=1}^MB_{ij}},\quad j=1,2,3,…,M;\quad i=1,2,3,…,N$$</p><p><strong>监督学习的方法就是拿频率来估计概率</strong>。</p><h2 id="3-2-非监督学习——EM算法"><a href="#3-2-非监督学习——EM算法" class="headerlink" title="3.2 非监督学习——EM算法"></a>3.2 非监督学习——EM算法</h2><p>假设给定训练数据只包含S个长度为T的观测序列$\boldsymbol{x}=\{\boldsymbol{x}^1,\boldsymbol{x}^2,,…,\boldsymbol{x}^S\}$而没有对应的状态序列，目标是学习HMM模型的参数$\boldsymbol{\lambda}=(\boldsymbol{\pi},A,B)$。将状态序列看作不可观测的隐数据$\boldsymbol{Y}$，HMM模型事实上是一个含有隐变量的概率模型：<br>$$p(\boldsymbol{X}|\boldsymbol{\lambda})=\sum\limits_\boldsymbol{Y}p(\boldsymbol{X}|\boldsymbol{Y},\boldsymbol{\lambda})p(\boldsymbol{Y}|\boldsymbol{\lambda})$$<br>这个参数可以由<strong>EM算法</strong>实现。</p><ol><li><p><strong>确定数据的对数似然函数</strong><br> 所有观测数据写成$\boldsymbol{x}=\{x_1,x_2,x_3,…,x_T\}$，所有的隐藏状态数据写成$\boldsymbol{y}=\{y_1,y_2,,…,y_T\}$，则完全数据是$(\boldsymbol{x},\boldsymbol{y})=(x_1,x_2,x_3,…,x_T,y_1,y_2,,…,y_T)$，完全数据的对数似然函数是$\log{p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})}$。</p></li><li><p><strong>Em算法的E步</strong>：求Q函数$$\begin{aligned}<br> Q(\boldsymbol{\lambda},\overline{\boldsymbol{\lambda}})&amp;=E_\boldsymbol{y}[\log{p(\boldsymbol{y},\boldsymbol{y}|\boldsymbol{\lambda})}|\boldsymbol{x},\overline{\boldsymbol{\lambda}}]\<br> &amp;=\sum\limits_\boldsymbol{y}\log{p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})}p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}})<br> \end{aligned}$$</p></li></ol><p>其中$\overline{\boldsymbol{\lambda}}$是HMM模型参数的当前估计值，$\boldsymbol{\lambda}$是要极大化的HMM模型参数。<br>$$p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})=\pi_{y_1}b_{y_1x_1}a_{y_1y_2}b_{y_2x_2}\cdot …\cdot a_{y_{T-1}y_T}b_{y_Tx_T}$$<br>所以:<br>$Q(\boldsymbol{\lambda},\overline{\boldsymbol{\lambda}})=\sum\limits_\boldsymbol{y}\log{\pi_{y_1}p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})+\sum\limits_\boldsymbol{y}[\sum\limits^{T-1}_{t=1}\log{a_{y_ty_{t+1}}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})+\sum\limits_\boldsymbol{y}[\sum\limits_{t=1}^T\log{b_{y_tx_t}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})$</p><ol start="3"><li><strong>EM算法的M步</strong>：极大化Q函数求模型的参数<br> <strong>第一项：</strong><br> $$\sum\limits_\boldsymbol{y}\log{\pi_{y_1}p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})=\sum\limits_{i=1}^N\log{\pi_ip(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}$$<br> 注意到$\sum\limits^{N}_{i=1}{\pi_i}=1$，利用拉格朗日乘子法，写出拉格朗日函数：</li></ol><p>$$\sum\limits_{i=1}^N\log{\pi_ip(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}+\gamma(\sum\limits^{N}_{i=1}-1)$$</p><p>求偏导并令其等于0：</p><p>$$\frac{\partial}{\partial \pi_i}[\sum\limits_{i=1}^N\log{\pi_ip(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}+\gamma(\sum\limits^{N}_{i=1}-1)]=0$$</p><p>得到：</p><p>$$p(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})+\gamma\pi_i=0$$</p><p>对i求和：<br>$$\gamma=-p(\boldsymbol{x}|\overline{\boldsymbol{\lambda}})$$</p><p>所以，得到$\pi_i$：</p><p>$$\pi_i=\frac{p(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}{p(\boldsymbol{x}|\overline{\boldsymbol{\lambda}})}$$</p><p><strong>第二项：</strong><br>$$\sum\limits_\boldsymbol{y}[\sum\limits^{T-1}_{t=1}\log{a_{y_ty_{t+1}}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})=\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}\log{a_{ij}p(\boldsymbol{x},y_t=q_i,y_{t+1}=q_j|\overline{\boldsymbol{\lambda}})}$$</p><p>类似第一项，应用$\sum\limits_{j=1}^N=1$的拉格朗日乘子法可以求出：<br>$$a_{ij}=\frac{\sum\limits_{t=1}^{T-1}p(\boldsymbol{x},y_t=q_i,y_{t+1}=q_j|\overline{\boldsymbol{\lambda}})}{\sum\limits_{t=1}^{T-1}p(\boldsymbol{x},y_t=q_i|\overline{\boldsymbol{\lambda}})}$$</p><p><strong>第三项：</strong><br>$$\sum\limits_\boldsymbol{y}[\sum\limits_{t=1}^T\log{b_{y_tx_t}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})=\sum\limits_{j=1}^N\sum\limits_{t=1}^{T}\log{b_{jx_t}p(\boldsymbol{x},y_t=q_j|\overline{\boldsymbol{\lambda}})}$$</p><p>同样使用拉格朗日乘子法，求得：<br>$$b_{jk}=\frac{\sum\limits_{t=1}^Tp(\boldsymbol{x},y_t=q_j|\overline{\boldsymbol{\lambda}})I(x_t=v_k)}{\sum\limits_{t=1}^Tp(\boldsymbol{x},y_t=q_j|\overline{\boldsymbol{\lambda}})}$$</p><h2 id="3-3-Baum-Welch算法"><a href="#3-3-Baum-Welch算法" class="headerlink" title="3.3 Baum-Welch算法"></a>3.3 Baum-Welch算法</h2><p>将EM算法的参数式子分别用前向和后向概率算出的$\gamma_t(i),\xi_t(i,j)$表示则:<br>$$a_{ij}=\frac{\sum\limits_{t=1}^{T-1}\xi_t(i,j)}{\sum\limits_{t=1}^{T-1}\gamma_t(i)}$$</p><p>$$b_{ij}=\frac{\sum\limits_{t=1,x_t=v_j}^T\gamma_t(i)}{\sum\limits_{t=1}^T\gamma_t(i)}$$</p><p>$$\pi_i=\gamma_1(i)$$</p><h1 id="4-序列预测问题及算法"><a href="#4-序列预测问题及算法" class="headerlink" title="4 序列预测问题及算法"></a>4 序列预测问题及算法</h1><p>序列预测问题就是已知模型参数$\lambda=(\boldsymbol{\pi},A,B)$，给定观测序列$\boldsymbol{x}$，求最有可能的状态序列$\boldsymbol{y}$，即求$p(\boldsymbol{y}|\boldsymbol{x})$的最大值。一般有两种解法：近似解法与维特比解法。</p><h2 id="4-1-近似解法"><a href="#4-1-近似解法" class="headerlink" title="4.1 近似解法"></a>4.1 近似解法</h2><p>近似算法的思想是，在每个时刻$t$选择该时刻最有可能出现的状态$y_t^<em>$，从而得到一个状态序列$\boldsymbol{y}=(y_i^</em>,y_2^<em>,…,y_t^</em>)$，将它作为预测的结果。<br>给定模型$\boldsymbol{\lambda}$和观测$\boldsymbol{x}$，在时刻$t$处于状态$q_i$的概率:<br>$$\gamma_t(i)==\frac{\alpha_t(i)\beta_t(i)}{p(\boldsymbol{x}|\boldsymbol{\lambda})}=\frac{\alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N\alpha_t(i)\beta_t(i)}$$</p><p>在每一个时刻最有可能的状态$y_t^<em>$是：<br>$$y_t^</em>=\arg\max_{1\leq i\leq N}[\gamma_t(i)],\quad t=1,2,3…,T$$<br>从而得到整个序列。<br>近似算法的优点是计算简单，其缺点是<strong>不能保证预测的状态序列整体是最有可能的状态序列</strong>，因为预测的状态序列可能有<strong>实际不发生</strong>的部分。事实上，上述方法得到的状态序列中有可能存在<strong>转移概率为0的相邻状态</strong>。尽管如此，近似算法仍然是有用的。</p><h2 id="4-2-维特比算法（Viterbi-algorithm）"><a href="#4-2-维特比算法（Viterbi-algorithm）" class="headerlink" title="4.2 维特比算法（Viterbi algorithm）"></a>4.2 维特比算法（Viterbi algorithm）</h2><p><strong>维特比算法</strong>实际是用<strong>动态规划</strong>解HMM模型序列预测问题，用动态规划求解概率最大路径（最优路径），一条路径对应一个状态序列。<br>根据图论，假设最优路径为$\boldsymbol{y}^<em>$，其中从起点到时刻$t$的一段最优路径是$(y_1^</em>,y_2^<em>,…,y_t^</em>)$，则这部分路径对于后序最优路径（$y_t^<em>,y_{t+1}^</em>,y_{t+2}^<em>,…,y_T^</em>$）的选取来说一定也是最优的。可以使用反证法来证明：假设存在另一条局部路径$(y_1^,,y_2^,,…,y_t^,)$要优于$(y_1^<em>,y_2^</em>,…,y_t^<em>)$，那么它与（$y_t^</em>,y_{t+1}^<em>,y_{t+2}^</em>,…,y_T^<em>$）拼接起来蝴蝶刀另一条更优的全局最优路径，与定义矛盾。<br>根据HMM模型的第一个假设，$y_{t+1}$仅仅只与$y_t$相关，所以网状图可以动态规划地搜索。<em>*定义</em></em>：二维数组$\delta_t(i)$表示在时刻$t$以$q_j$结尾的所有局部路径的最大概率。从第一步推到第T步，每次递推都在上一次的N条局部路径中挑选，所以复杂度为$O(TN)$。为了得到路径，还需要定义一个二维数组$\psi_t(i)$，记录每个状态的前驱。</p><p>$$\delta_t(i)=\max_{y_1,y_2,…,y_{t-1}}p(y_t=q_i,y_{t-1},…y_1,x_t,x_{t-1},…,x_1|\boldsymbol{\lambda})$$</p><p>$$\psi_t(i)=\arg\max_{1\leq j \leq N}[\delta_{t-1}(j)a_{ji}]$$</p><p>维特比算法：</p><ol><li><strong>初始化</strong>，当$t=1$时，最优路径的备选由N个状态组成，它的前驱为空：<br> $$\delta_1(i)=\pi_ib_{ix_1},\quad i=1,2,3,..,N$$</li></ol><p>$$\psi_1(i)=0,\quad i=1,2,3,..,N$$</p><ol start="2"><li><strong>递推</strong>，当$t\geq 2$时，每条备选的路径像贪吃蛇一样吃入一个状态，长度增加一个单位，根据状态转移概率和发射概率计算<strong>花费</strong>。找出新的局部最优路径，更新两个数组。<br> $$\delta_t(i)=\max_{1\leq j \leq N}[\delta_{t-1}(j)a_{ji}]b_{ix_t},\quad i=1,2,3,..,N$$</li></ol><p>$$\psi_t(i)=\arg\max_{1\leq j \leq N}(\delta_t(j)a_{ji}),\quad i=1,2,3,..,N$$</p><ol start="3"><li><strong>终止</strong>，找出最终时刻$(\delta_t(i)$数组中最大概率$p^<em>$，以及相应的结尾状态下表$y_T^</em>$<br> $$P^*=\max_{1\leq j \leq N}\delta_T(i)$$</li></ol><p>$$y_T^*=\arg\max_{1\leq j \leq N}\delta_T(i)$$</p><ol start="4"><li>回溯，根据前驱数组$\psi_t$回溯前驱状态，取得最优路径状态下标$\boldsymbol{y}^<em>=(y_1^</em>,y_2^<em>,…,y_T^</em>)$。<br> $$y_t^<em>=\psi_{t+1}(y_{t+1}^</em>),\quad t=T-1,T-2,…,1$$</li></ol><p>举个例子：<br>上文中的盒子和球模型，状态集合为$Q=\{1,2,3\}$，观测集合为$V=\{红，白\}$，<br>$A=\begin{bmatrix}<br>0.5 &amp; 0.2 &amp; 0.3\<br>0.3 &amp; 0.5 &amp; 0.2\<br>0.2 &amp; 0.3 &amp; 0.5<br>\end{bmatrix}$ ，  $B=\begin{bmatrix}<br>0.5 &amp; 0.5 \<br>0.4 &amp; 0.6\<br>0.7 &amp; 0.3<br>\end{bmatrix}$ ， $\pi=(0.2,0.4,0.4)^T$<br>设$T=3$，$\boldsymbol{x}=(红，白，红)$，求最优状态序列，即最优路径$\boldsymbol{y}^<em>=(y_1^</em>,y_2^<em>,…,y_T^</em>)$。<br>解：</p><ol><li>初始化，在$t=1$时，对每一个状态$q_i，i=1,2,3$，求状态为$q_i$观测为$x_1$为红的概率，记此概率为$\delta_1(i)$。<br> $$\delta_1(i)=\pi_ib_{ix_1}=\pi_ib_{i红}，\quad i=1,2,3$$</li></ol><p>$$\delta_1(1)=0.2\times0.5=0.1,\delta_1(2)=0.4\times0.4=0.16,\delta_1(3)=0.4\times0.7=0.28$$</p><p>$$\psi_1(i)=0,\quad i=1,2,3$$</p><ol start="2"><li>在$t=2$时，对每个状态$i，i=1,2,3$:<br> $$\delta_2(i)=\max_{1\leq j \leq 3}[\delta_{1}(j)a_{ji}]b_{i白},\quad i=1,2,3,..,N$$</li></ol><p>$$\begin{aligned}<br>\delta_2(1)&amp;=\max_{1\leq j \leq 3}[\delta_{1}(j)a_{ji}]b_{i白}\<br>&amp;=\max_j\{o.1\times0.5,0.16\times0.3,0.28\times0.2\}\times0.5\<br>&amp;=0.028<br>\end{aligned}$$</p><p>$$\psi_2(1)=3$$</p><p>同理：</p><p>$$\delta_2(2)=0.0504,\quad\psi_2(3)=3$$</p><p>$$\delta_2(3)=0.042,\quad\psi_2(3)=3$$</p><p>同样，$t=3$：</p><p>$$\delta_3(1)=0.00756,\quad\psi_3(1)=2$$</p><p>$$\delta_3(2)=0.01008,\quad\psi_3(2)=2$$</p><p>$$\delta_3(3)=0.0417,\quad\psi_3(3)=3$$</p><ol start="3"><li>最优路径概率:<br> $$P^*=\max_{1\leq i\leq 3}\delta_3(i)=0.0147$$</li></ol><p>$$y_3^*=\arg\max_{1\leq j \leq 3}\delta_3(i)=3$$</p><ol start="4"><li>回溯：<br> $$t=2,\quad y_2^<em>=\psi_3(y_3^</em>)=\psi_3(3)=3$$</li></ol><p>$$t=1,\quad y_1^<em>=\psi_2(y_2^</em>)=\psi_2(3)=3$$</p><p>所以，最优路径为$\boldsymbol{y}^<em>=(y_1^</em>,y_2^<em>,y_3^</em>)=(3,3,3)$。</p><h1 id="5-hmmlearn使用"><a href="#5-hmmlearn使用" class="headerlink" title="5 hmmlearn使用"></a>5 hmmlearn使用</h1><p><a href="https://hmmlearn.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">hmmlearn</a>是一个实现了hmm的python库，安装很简单，使用<code>pip install hmmlearn</code>就行。<br>hmmlearn实现了三种HMM模型，分成两类：</p><ul><li>针对观测状态是连续的：GaussianHMM和GMMHMM（广泛用于语音识别）</li><li>针对观测状态是离散的：MultinomialHMM，也就是上文中提到的。</li></ul><p>对于MultinomialHMM的模型，使用比较简单，”startprob_”参数对应我们的初始状态概率向量$\pi$， “transmat_”对应我们的状态转移矩阵$A$, “emissionprob_”对应我们的发射概率矩阵$B$。</p><p>对于连续观测状态的HMM模型，GaussianHMM类假设观测状态符合高斯分布，而GMMHMM类则假设观测状态符合混合高斯分布。一般情况下我们使用GaussianHMM即高斯分布的观测状态即可。以下对于连续观测状态的HMM模型，我们只讨论GaussianHMM类。</p><p>在GaussianHMM类中，”startprob_”参数对应我们的隐藏状态初始分布$\pi$ , “transmat_”对应我们的状态转移矩阵A, 比较特殊的是发射概率的表示方法，此时由于观测状态是连续值，我们无法像MultinomialHMM一样直接给出矩阵B。而是采用给出各个隐藏状态对应的观测状态高斯分布的概率密度函数的参数。</p><p>如果观测序列是一维的，则观测状态的概率密度函数是一维的普通高斯分布。如果观测序列是<br>N维的，则隐藏状态对应的观测状态的概率密度函数是N维高斯分布。高斯分布的概率密度函数参数可以用μ表示高斯分布的期望向量，Σ表示高斯分布的协方差矩阵。在GaussianHMM类中，“means”用来表示各个隐藏状态对应的高斯分布期望向量μ形成的矩阵，而“covars”用来表示各个隐藏状态对应的高斯分布协方差矩阵Σ形成的三维张量。</p><p>参考：<a href="https://blog.csdn.net/tostq/article/details/70851531?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">深度剖析HMM</a></p><h2 id="5-1-pythonceshi工具unnitest"><a href="#5-1-pythonceshi工具unnitest" class="headerlink" title="5.1 pythonceshi工具unnitest"></a>5.1 pythonceshi工具unnitest</h2><p>测试工具unnitest非常容易使用，首先是建立一个继承自TestCase的测试类，然后通过覆盖setUp()完成相关初始化，最后通过覆盖tearDown()方法清除测试中产生的数据，为以后的TestCase留下一个干净的环境。我们需要在测试类中编写以test_开头的测试函数，unnitest会在测试中自动执行以test_开头的测试函数，unnitest的使用框架：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> unittest  <span class="token keyword">class</span> <span class="token class-name">XXXXX</span><span class="token punctuation">(</span>unittest<span class="token punctuation">.</span>TestCase<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">setUp</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># 自行编写</span>    <span class="token keyword">pass</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>      unittest<span class="token punctuation">.</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-2-离散HMM测试"><a href="#5-2-离散HMM测试" class="headerlink" title="5.2 离散HMM测试"></a>5.2 离散HMM测试</h2><p>对离散HMM模型的测试代码：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算平方误差</span><span class="token keyword">def</span> <span class="token function">s_error</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> sqrt<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">(</span>A<span class="token operator">-</span>B<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>A<span class="token operator">-</span>B<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">DiscreteHMM_Test</span><span class="token punctuation">(</span>unittest<span class="token punctuation">.</span>TestCase<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">setUp</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 建立两个HMM，隐藏状态个数为4，X可能分布为10类</span>        n_state <span class="token operator">=</span><span class="token number">4</span>        n_feature <span class="token operator">=</span> <span class="token number">10</span>        X_length <span class="token operator">=</span> <span class="token number">1000</span>        n_batch <span class="token operator">=</span> <span class="token number">100</span> <span class="token comment" spellcheck="true"># 批量数目</span>        self<span class="token punctuation">.</span>n_batch <span class="token operator">=</span> n_batch        self<span class="token punctuation">.</span>X_length <span class="token operator">=</span> X_length        self<span class="token punctuation">.</span>test_hmm <span class="token operator">=</span> hmm<span class="token punctuation">.</span>DiscreteHMM<span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>comp_hmm <span class="token operator">=</span> ContrastHMM<span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Z <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>train<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Z<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_train_batch</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        Z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> b <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_batch<span class="token punctuation">)</span><span class="token punctuation">:</span>            b_X<span class="token punctuation">,</span> b_Z <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>            X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>b_X<span class="token punctuation">)</span>            Z<span class="token punctuation">.</span>append<span class="token punctuation">(</span>b_Z<span class="token punctuation">)</span>        batch_hmm <span class="token operator">=</span> hmm<span class="token punctuation">.</span>DiscreteHMM<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>n_state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>x_num<span class="token punctuation">)</span>        batch_hmm<span class="token punctuation">.</span>train_batch<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Z<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 判断概率参数是否接近</span>        <span class="token comment" spellcheck="true"># 初始概率判定没有通过！！！</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>batch_hmm<span class="token punctuation">.</span>start_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>batch_hmm<span class="token punctuation">.</span>transmat_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>batch_hmm<span class="token punctuation">.</span>emission_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_train</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 判断概率参数是否接近</span>        <span class="token comment" spellcheck="true"># 单批量的初始概率一定是不准的</span>        <span class="token comment" spellcheck="true"># self.assertAlmostEqual(s_error(self.test_hmm.start_prob, self.comp_hmm.module.startprob_), 0, 1)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>transmat_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>emission_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_X_prob</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X<span class="token punctuation">,</span>_ <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>        prob_test <span class="token operator">=</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>X_prob<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        prob_comp <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>prob_test<span class="token punctuation">,</span> prob_comp<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_predict</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>        prob_next <span class="token operator">=</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">,</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>x_num<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertEqual<span class="token punctuation">(</span>prob_next<span class="token punctuation">.</span>shape<span class="token punctuation">,</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>n_state<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_decode</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X<span class="token punctuation">,</span>_ <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>        test_decode <span class="token operator">=</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> comp_decode <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>test_decode<span class="token punctuation">,</span> comp_decode<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    unittest<span class="token punctuation">.</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-3-HMM测试"><a href="#5-3-HMM测试" class="headerlink" title="5.3 HMM测试"></a>5.3 HMM测试</h2><p>利用hmmlearn初始化一个高斯模型</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ContrastHMM</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>module <span class="token operator">=</span> hmmlearn<span class="token punctuation">.</span>hmm<span class="token punctuation">.</span>GaussianHMM<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_state<span class="token punctuation">,</span>covariance_type<span class="token operator">=</span><span class="token string">"full"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>n_state<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 转换概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 高斯发射概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>means_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">10</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>covars_ <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>利用hmmlearn初始化一个离散HMM模型:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ContrastHMM</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>module <span class="token operator">=</span> hmmlearn<span class="token punctuation">.</span>hmm<span class="token punctuation">.</span>MultinomialHMM<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_state<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>n_state<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print self.module.startprob_</span>        <span class="token comment" spellcheck="true"># 转换概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print self.module.transmat_</span>        <span class="token comment" spellcheck="true"># 发射概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_ <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print self.module.emissionprob_</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码详情请见我的<a href="https://github.com/CodingMarathon/All_Algorithm/tree/master/HMM/easyhmm" target="_blank" rel="noopener">github</a>，请大家帮忙点个star。<br><img src="https://img-blog.csdnimg.cn/20200318170910751.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2020/03/25/hello-world/"/>
      <url>/2020/03/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
