<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>词袋模型详解</title>
      <link href="/2020/04/04/ci-dai-mo-xing-xiang-jie/"/>
      <url>/2020/04/04/ci-dai-mo-xing-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>&emsp;&emsp;自然语言处理面临的文本数据往往是非结构化杂乱无章的文本数据，而机器学习算法处理的数据往往是固定长度的输入和输出。因而机器学习并不能直接处理原始的文本数据。必须把文本数据转换成数字，比如向量。</p><p>在Neural Network Methods in Natural Language Processing, 2017一书65页有一句话：<br><code>在语言处理中，往往使用向量x来表示文本的大量语言学特性。</code><br>这个过程就叫做特征提取或者特征编码。一种流行并且简单的特征提取方法就是词袋模型。<br>&emsp;&emsp;在自然语言处理和文本分析的问题中，词袋模型和词向量（Word Embedding）是两种最常用的模型。更准确地说，词向量只能表征单个词，如果要表示文本，需要做一些额外的处理。下面详细聊一聊词袋模型。</p><h1 id="词袋模型"><a href="#词袋模型" class="headerlink" title="词袋模型"></a>词袋模型</h1><p>&emsp;&emsp;词袋模型（Bag-of-Words model，BOW）BoW(Bag of Words)词袋模型最初被用在文本分类中，将文档表示成<strong>特征矢量</strong>。它的基本思想是假定对于一个文本，忽略其词序和语法、句法，仅仅将其看做是一些词汇的集合，而文本中的每个词汇都是独立的。简单说就是讲每篇文档都看成一个袋子（因为里面装的都是词汇，所以称为词袋，Bag of words即因此而来），然后看这个袋子里装的都是些什么词汇，将其分类。如果文档中猪、马、牛、羊、山谷、土地、拖拉机这样的词汇多些，而银行、大厦、汽车、公园这样的词汇少些，我们就倾向于判断它是一篇描绘乡村的文档，而不是描述城镇的。</p><p>例如三个句子如下：</p><pre><code>句子1：小孩喜欢吃零食。句子2：小孩喜欢玩游戏，不喜欢运动。句子3 ：大人不喜欢吃零食，喜欢运动。</code></pre><p>&emsp;&emsp;首先根据<strong>语料</strong>中出现的句子分词，然后构建<strong>词袋</strong>（每一个出现的词都加进来）。计算机不认识字，只认识数字，那在计算机中怎么表示词袋模型呢？其实很简单，给每个词一个位置索引就可以了。小孩放在第一个位置，喜欢放在第二个位置，以此类推。</p><pre><code>{“小孩”:1，“喜欢”:2，“吃”:3，“零食”:4，“玩”:5，“游戏”:6，“大人”:7，“不”:8，“运动”:9}</code></pre><p>其中key为词，value为词的索引，预料中共有9个单词， 那么每个文本我们就可以使用一个9维的向量来表示。<br>如果文本中含有的一个词出现了一次，就让那个词的位置置为1，词出现几次就置为几，那么上述文本可以表示为：</p><pre><code>句子1：[1,1,1,1,0,0,0,0,0]句子2：[1,2,0,0,1,1,0,1,1]句子3：[0,2,1,1,0,0,1,1,1]</code></pre><p>该向量与原来文本中单词出现的顺序没有关系，仅仅是词典中每个单词在文本中出现的频率。<br>&emsp;&emsp;与词袋模型非常类似的一个模型是词集模型(Set of Words,简称SoW)，和词袋模型唯一的不同是它仅仅考虑词是否在文本中出现，而不考虑词频。也就是一个词在文本在文本中出现1次和多次特征处理是一样的。在大多数时候，我们使用词袋模型。</p><h1 id="词袋模型的作用"><a href="#词袋模型的作用" class="headerlink" title="词袋模型的作用"></a>词袋模型的作用</h1><p>&emsp;&emsp;将两篇文本通过词袋模型变为向量模型，通过计算向量的余弦距离来计算两个文本间的相似度。<br>词袋模型的缺点：</p><ul><li>词袋模型最重要的是构造词库，需要维护一个很大的词库。</li><li>词袋模型严重缺乏相似词之间的表达。<ul><li>“我喜欢北京”“我不喜欢北京”其实这两个文本是严重不相似的。但词袋模型会判为高度相似。</li><li>“我喜欢北京”与“我爱北京”其实表达的意思是非常非常的接近的，但词袋模型不能表示“喜欢”和“爱”之间严重的相似关系。（当然词袋模型也能给这两句话很高的相似度，但是注意我想表达的含义）</li></ul></li><li>向量稀疏</li></ul><p>为了让词袋模型能够表达更多语义，尝试使用n元语法来构建词袋模型。n表示聚合的词个数，比如2就表示2个2个聚合在一起，叫做2元模型。<br>例如：</p><pre><code>“我，喜欢”“喜欢，北京”...</code></pre><p>n元模型比词袋模型在某些任务上表现得更好，比如文档分类，但也会带来麻烦。</p><h1 id="词袋模型的实现"><a href="#词袋模型的实现" class="headerlink" title="词袋模型的实现"></a>词袋模型的实现</h1><p>&emsp;&emsp;对于中文来说，词袋模型首先会进行分词，在分词之后，通过统计每个词在文本中出现的次数，我们就可以得到该文本基于词的特征，如果将各个文本样本的这些词与对应的词频放在一起，就是我们常说的向量化。向量化完毕后一般也会使用TF-IDF进行特征的权重修正，再将特征进行标准化。 再进行一些其他的特征工程后，就可以将数据带入机器学习算法进行分类聚类了。</p><p>为了实现方便，本文使用英文来介绍怎么实现。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#  操作词袋模型：</span><span class="token comment" spellcheck="true"># CountVectorizer：对语料库中出现的词汇进行词频统计，相当于词袋模型。</span><span class="token comment" spellcheck="true"># 操作方式：将语料库当中出现的词汇作为特征，将词汇在当前文档中出现的频率（次数）作为特征值。</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizercount <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 语料库</span>docs <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span>    <span class="token string">"Where there is a will, there is a way."</span><span class="token punctuation">,</span>    <span class="token string">"There is no royal road to learning."</span><span class="token punctuation">,</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># bag是一个稀疏的矩阵。因为词袋模型就是一种稀疏的表示。</span>bag <span class="token operator">=</span> count<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>docs<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 输出单词与编号的映射关系。</span><span class="token keyword">print</span><span class="token punctuation">(</span>count<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 调用稀疏矩阵的toarray方法，将稀疏矩阵转换为ndarray对象。</span><span class="token keyword">print</span><span class="token punctuation">(</span>bag<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>bag<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># where映射为编号8  there映射为编号5······</span><span class="token comment" spellcheck="true"># 编号也是bag.toarray转换来的ndarray数组的索引</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>TF-IDF详解</title>
      <link href="/2020/04/04/tf-idf-xiang-jie/"/>
      <url>/2020/04/04/tf-idf-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h1><p>前面介绍了词向量的One-Hot模型以及词袋模型，这都是为了将离散符号表示的文本转换成数字表示，以提供给后续机器学习算法的使用。<br><a href="https://blog.csdn.net/Elenstone/article/details/105110110" target="_blank" rel="noopener">词向量之One-Hot编码详解</a><br><a href="https://blog.csdn.net/Elenstone/article/details/105134863" target="_blank" rel="noopener">词向量之词袋模型（BOW）详解</a><br>本文主要介绍另一种词向量的表示方法：TF-IDF。</p><h1 id="1-TF-IDF模型"><a href="#1-TF-IDF模型" class="headerlink" title="1 TF-IDF模型"></a>1 TF-IDF模型</h1><p>&emsp;&emsp;现有一个问题，怎么表示一个词在文档中的重要性？<br>&emsp;&emsp;首先想的是文档中出现越多的词对文章来说就越重要，也就是说词在文档中的频率越大越重要，这里用TF（term frequency）来表示。但是又出现了另一个问题，“的”，“了”，“吗”这种词出现在文档中的频率一般也很高，但是它们又不带有实际意义，所以为了降低这些词的重要性，引入了IDF（inverse document frequency），若一个词在语料库中的很多文档中都出现，那么就让这个词的重要性降低一些。由此便产生了TF-IDF模型。</p><h2 id="1-1-TF-IDF数学形式"><a href="#1-1-TF-IDF数学形式" class="headerlink" title="1.1 TF-IDF数学形式"></a>1.1 TF-IDF数学形式</h2><ul><li>TF：单词在某一篇文章中的词频，一个词在文章中出现的频率，此处文章可以是一个句子，也可以是一整篇文章。</li></ul><p>$$<br>TF(d,w)=\frac{N(w)}{N(d)}<br>$$</p><p>其中$N(w)$表示$d$这篇文章中词$w$的总数，$N(d)$表示文档$d$中的词的总数。<br>除此之外还有其他计算方式：</p><pre><code>- 二值：如果词出现在一篇文档中，则$N(w)$为1，否则为0- 对数化：$TF(d,w)=\log{(1+N(w))}$- 双重标准化0.5：让词的频数除以该篇文档中最高频词的频数$TF(d,w)=0.5+0.5\times\frac{N(w)}{\max\limits_wN(w)}$，这样做是为了避免偏向语料库中的长文档。- 双重标准化K：将上面方法中的0.5更换成$K\in [0,1]$，则$TF(d,w)=K+(1-K)\times\frac{N(w)}{\max\limits_wN(w)}$</code></pre><ul><li><p>IDF：逆文档频率，指的是含有这个单词的文章占语料库中所有文章总数的比例的倒数。<br>$$IDF(w)=\log{\frac{N}{N(w)}}$$<br>其中，$N$指的是语料库中所有文档的总数，$N(w)$指的是语料库中含有词$w$的文档的总数。<br>除此之外还有其他计算方式：</p><ul><li>一元化：恒为1，不考虑IDF只考虑TF。</li><li>平滑的IDF：为了避免由于词没有出现在语料库中而发生的除0错误$IDF(w)=\log{(\frac{N}{1+N(w)})}$。</li><li>最大逆文档频率（inverse document frequency max）：遍历指定文档中的词，计算在语料库中包含该词的文档总数，取最大值，则$IDF(w)=\log{(\frac{\max\limits_{t\in d}N(t)}{1+N(w)})}$</li><li>概率逆文档频率：将分母替换成$N-N(w)$，则$IDF(w)=\log{\frac{N-N(w)}{N(w)}}$</li></ul></li></ul><p>TF-IDF是TF乘以IDF：<br>$$TF-IDF(w)=TF(d,w)\times IDF(w)$$</p><h2 id="1-2-举例"><a href="#1-2-举例" class="headerlink" title="1.2 举例"></a>1.2 举例</h2><p>举例输入：<br>文档1：我 爱 技术 人民。<br>文档2：我 爱 电脑。<br>文档3：我 爱 加班 的 人民。<br>计算TF：每篇文档的词频，例如文档1，“我”的词频就是$1/4=0.5$<br>计算DF：文档频率，“人民”在三篇文档中出现了两次，所以$DF=2/3=0.66667$<br>计算IDF:逆文档频率，”人民”$IDF=\frac{1}{DF}=3/2=1.5$</p><p>注意：<br>==TF是针对一篇文档而言；IDF是对整个语料库而言的。==</p><h1 id="2-TF-IDF的实现"><a href="#2-TF-IDF的实现" class="headerlink" title="2 TF-IDF的实现"></a>2 TF-IDF的实现</h1><h2 id="2-1-TF-IDF简单python实现"><a href="#2-1-TF-IDF简单python实现" class="headerlink" title="2.1 TF-IDF简单python实现"></a>2.1 TF-IDF简单python实现</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">class</span> <span class="token class-name">TFIDF</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    手写一个TFIDF统计类,只写最简单的一个实现    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        初始化        self.vob:词汇个数统计，dict格式        self.word_id:词汇编码id，dict格式        self.smooth_idf：平滑系数，关于平滑不多解释了        :param corpus:输入的语料        """</span>        self<span class="token punctuation">.</span>word_id <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>vob <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span>        self<span class="token punctuation">.</span>corpus <span class="token operator">=</span> corpus        self<span class="token punctuation">.</span>smooth_idf <span class="token operator">=</span> <span class="token number">0.01</span>    <span class="token keyword">def</span> <span class="token function">fit_transform</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> corpus<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">get_vob_fre</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        计算文本特特征的出现次数，也就是文本频率term frequency，但是没有除token总数，因为后面bincount计算不支持float        :return: 修改self.vob也就是修改词频统计字典        """</span>        <span class="token comment" spellcheck="true"># 统计各词出现个数</span>        id <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">for</span> single_corpus <span class="token keyword">in</span> self<span class="token punctuation">.</span>corpus<span class="token punctuation">:</span>            <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>single_corpus<span class="token punctuation">,</span> list<span class="token punctuation">)</span><span class="token punctuation">:</span>                <span class="token keyword">pass</span>            <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>single_corpus<span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>                single_corpus <span class="token operator">=</span> single_corpus<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>            <span class="token keyword">for</span> word <span class="token keyword">in</span> single_corpus<span class="token punctuation">:</span>                <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> self<span class="token punctuation">.</span>vob<span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>vob<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>                    self<span class="token punctuation">.</span>word_id<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> id                    id <span class="token operator">+=</span> <span class="token number">1</span>                <span class="token keyword">else</span><span class="token punctuation">:</span>                    self<span class="token punctuation">.</span>vob<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">+=</span> <span class="token number">1</span>        <span class="token comment" spellcheck="true"># 生成矩阵</span>        X <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>corpus<span class="token punctuation">)</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vob<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token keyword">if</span> isinstance<span class="token punctuation">(</span>self<span class="token punctuation">.</span>corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">,</span> str<span class="token punctuation">)</span><span class="token punctuation">:</span>                single_corpus <span class="token operator">=</span> self<span class="token punctuation">.</span>corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">" "</span><span class="token punctuation">)</span>            <span class="token keyword">else</span><span class="token punctuation">:</span>                single_corpus <span class="token operator">=</span> self<span class="token punctuation">.</span>corpus<span class="token punctuation">[</span>i<span class="token punctuation">]</span>            <span class="token keyword">for</span> j <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>single_corpus<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>                feature <span class="token operator">=</span> single_corpus<span class="token punctuation">[</span>j<span class="token punctuation">]</span>                feature_id <span class="token operator">=</span> self<span class="token punctuation">.</span>word_id<span class="token punctuation">[</span>feature<span class="token punctuation">]</span>                X<span class="token punctuation">[</span>i<span class="token punctuation">,</span> feature_id<span class="token punctuation">]</span> <span class="token operator">=</span> self<span class="token punctuation">.</span>vob<span class="token punctuation">[</span>feature<span class="token punctuation">]</span>        <span class="token keyword">return</span> X<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>int<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 需要转化成int</span>    <span class="token keyword">def</span> <span class="token function">get_tf_idf</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        计算idf并生成最后的TFIDF矩阵        :return:        """</span>        X <span class="token operator">=</span> self<span class="token punctuation">.</span>get_vob_fre<span class="token punctuation">(</span><span class="token punctuation">)</span>        n_samples<span class="token punctuation">,</span> n_features <span class="token operator">=</span> X<span class="token punctuation">.</span>shape        df <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>n_features<span class="token punctuation">)</span><span class="token punctuation">:</span>            <span class="token triple-quoted-string string">"""            这里是统计每个特征的非0的数量，也就是逆文档频率指数的分式中的分母，是为了计算idf            """</span>            df<span class="token punctuation">.</span>append<span class="token punctuation">(</span>n_samples <span class="token operator">-</span> np<span class="token punctuation">.</span>bincount<span class="token punctuation">(</span>X<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        df <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>df<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># perform idf smoothing if required</span>        df <span class="token operator">+=</span> int<span class="token punctuation">(</span>self<span class="token punctuation">.</span>smooth_idf<span class="token punctuation">)</span>        n_samples <span class="token operator">+=</span> int<span class="token punctuation">(</span>self<span class="token punctuation">.</span>smooth_idf<span class="token punctuation">)</span>        idf <span class="token operator">=</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>n_samples <span class="token operator">/</span> df<span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span>        <span class="token keyword">return</span> X<span class="token operator">*</span>idf<span class="token operator">/</span>len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>vob<span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    corpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token string">"我"</span><span class="token punctuation">,</span> <span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token string">"e"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"我"</span><span class="token punctuation">,</span> <span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token string">"c"</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token string">"我"</span><span class="token punctuation">,</span> <span class="token string">"a"</span><span class="token punctuation">,</span> <span class="token string">"b"</span><span class="token punctuation">]</span><span class="token punctuation">]</span>    test <span class="token operator">=</span> TFIDF<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">.</span>get_tf_idf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>为了避免分词，直接采用这种形式。</p><h2 id="2-2-TF-IDF的gesim实现："><a href="#2-2-TF-IDF的gesim实现：" class="headerlink" title="2.2 TF-IDF的gesim实现："></a>2.2 TF-IDF的gesim实现：</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora<span class="token punctuation">,</span>similarities<span class="token punctuation">,</span>models<span class="token keyword">import</span> jieba<span class="token comment" spellcheck="true">#第一步：确定语料库的语料和要进行判断的句子：</span><span class="token comment" spellcheck="true">#wordlist作为语料库，语料库中有三句话，相当于三篇文章.比较sentences和wordlist中三句话的相似度</span>wordlist<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'我喜欢编程'</span><span class="token punctuation">,</span><span class="token string">'我想变漂亮'</span><span class="token punctuation">,</span><span class="token string">'今天吃午饭了吗'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true">#注意这里是个字符串</span>sentenses<span class="token operator">=</span><span class="token string">'我喜欢什么'</span><span class="token comment" spellcheck="true">#第二步：使用语料库建立词典，也就是给预料库中的每个单词标上序号，类似：{'我'：1，'喜欢'：2，'编程'：3,....}首先进行中文分词</span>text<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word  <span class="token keyword">in</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>words<span class="token punctuation">)</span> <span class="token punctuation">]</span><span class="token keyword">for</span> words <span class="token keyword">in</span> wordlist<span class="token punctuation">]</span>dictionary<span class="token operator">=</span>corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第三步，对语料中的每个词进行词频统计,doc2bow是对每一句话进行词频统计，传入的是一个list</span><span class="token comment" spellcheck="true">#corpus得到的是一个二维数组[[(0, 1), (1, 1), (2, 1)], [(3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1), (9, 1)]]，意思是编号为0的词出现的频率是1次，编号为2的词出现的频率是1次</span>corpus<span class="token operator">=</span><span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>word<span class="token punctuation">)</span> <span class="token keyword">for</span> word <span class="token keyword">in</span> text<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#第四步：使用corpus训练tfidf模型</span>model<span class="token operator">=</span>models<span class="token punctuation">.</span>TfidfModel<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#要是想要看tfidf的值的话可以:</span>tfidf<span class="token operator">=</span>model<span class="token punctuation">[</span>corpus<span class="token punctuation">]</span><span class="token triple-quoted-string string">'''tfidf的结果是语料库中每个词的tfidf值[(0, 0.5773502691896258), (1, 0.5773502691896258), (2, 0.5773502691896258)][(3, 0.7071067811865475), (4, 0.7071067811865475)][(5, 0.4472135954999579), (6, 0.4472135954999579), (7, 0.4472135954999579), (8, 0.4472135954999579), (9, 0.4472135954999579)]'''</span><span class="token comment" spellcheck="true">#第五步：为tfidf模型中的每个句子建立索引，便于进行相似度查询,传入的时候语料库的tfidf值</span>similarity<span class="token operator">=</span>similarities<span class="token punctuation">.</span>MatrixSimilarity<span class="token punctuation">(</span>tfidf<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第六步，处理要比较的句子，首先分词，其次获得词频，jieba只能传入字符串</span>sen<span class="token operator">=</span><span class="token punctuation">[</span>word <span class="token keyword">for</span> word <span class="token keyword">in</span> jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>sentenses<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">//</span>doc2传入的是一个列表sen2<span class="token operator">=</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>sen<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#然后计算它的tfidf值</span>sen_tfidf<span class="token operator">=</span>model<span class="token punctuation">[</span>sen2<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#获得与所有句子的相似度，sim输出的是一个数组</span>sim<span class="token operator">=</span>similarity<span class="token punctuation">[</span>sen_tfidf<span class="token punctuation">]</span><span class="token comment" spellcheck="true">#获得最大值,转化成list进行操作，使用list的max函数</span>max_sim<span class="token operator">=</span>max<span class="token punctuation">(</span>list<span class="token punctuation">(</span>sim<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#获得最大值索引，使用list的index函数</span>max_index<span class="token operator">=</span>sim<span class="token punctuation">.</span>index<span class="token punctuation">(</span>max<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p> tfidf如何表示一个句子：<br>加入一个句子有n个单词，每个单词计算出它的tfidf值，即每个单词用一个标量表示，则句子的维度是1<em>n<br>如果是用embedding表示法，每个单词用m维向量表示，句子的维度是m</em>n<br>保存和加载模型的方法：</p><pre><code># 保存词典：dictionary.save(DICT_PATH)# 保存tfidf模型model.save(MODEL_PATH)# 保存相似度similarity.save(SIMILARITY_PATH)# 加载词典：dictionary = corpora.Dictionary.load(&#39;require_files/dictionary.dict&#39;)# 加载模型tfidf = models.TfidfModel.load(&quot;require_files/my_model.tfidf&quot;)# 加载相似度index=similarities.MatrixSimilarity.load(&#39;require_files/similarities.0&#39;)</code></pre><h2 id="2-3-TF-IDF的sklearn实现"><a href="#2-3-TF-IDF的sklearn实现" class="headerlink" title="2.3 TF-IDF的sklearn实现"></a>2.3 TF-IDF的sklearn实现</h2><p>&emsp;&emsp;==sklearn==中计算TF-IDF的函数是<code>TfidfTransformer</code>和<code>TfidfVectorizer</code>，严格来说后者等于<code>CountVectorizer + TfidfTransformer</code>。<code>TfidfTransformer</code>和<code>TfidfVectorizer</code>有一些共同的参数，这些参数的不同影响了tfidf的计算方式：</p><ul><li><code>norm</code>：归一化，L1、L2（默认值）或者 None。L1 范数是向量中每个值除以所有值的绝对值的和，L2范数是向量中每个值除以所有值的平方开根号，即对于 L1：<br>$$x_i=\frac{x_i}{∣∣\boldsymbol{x}∣∣_1}=\frac{x_i}{\sum\limits_j∣x_j}$$<br>对于 l2：<br>$$x_i=\dfrac{x_i}{||\pmb x||_2} = \dfrac{x_i}{\sqrt{\sum\limits_j x^2_j}}$$</li><li><code>use_idf</code>：bool，默认 True，是否使用 IDF;</li><li><code>smooth_idf</code>：bool，默认True，是否平滑IDF，默认分子和分母都+1，和上述任何一种都不一样，防止除零错误;</li><li><code>sublinear_tf</code>：bool，默认False，是否对TF使用 sublinear，即使用$1 + \log(TF)$来替换原始的TF。</li></ul><p>所以，默认参数下<code>（norm=&#39;l2&#39;, use_idf=True, smooth_idf=True, sublinear_tf=False）</code>，sklearn 是这么计算TF-IDF的：</p><p>$$\begin{aligned}<br>tfidf(t,d,D)&amp;=tf(t,d)\times idf(t,D)\<br>&amp;=tf(t,d)\times (\log{(\frac{1+N}{1+N(w)})}+1)<br>\end{aligned}$$</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_extraction<span class="token punctuation">.</span>text <span class="token keyword">import</span> CountVectorizer<span class="token punctuation">,</span> TfidfVectorizer<span class="token punctuation">,</span> TfidfTransformercorpus <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'This is the first document.'</span><span class="token punctuation">,</span>          <span class="token string">'This is the second second document.'</span><span class="token punctuation">,</span>          <span class="token string">'And the third one.'</span><span class="token punctuation">,</span>          <span class="token string">'Is this the first document?'</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># CountVectorizer是通过fit_transform函数将文本中的词语转换为词频矩阵</span><span class="token comment" spellcheck="true"># get_feature_names()可看到所有文本的关键字</span><span class="token comment" spellcheck="true"># vocabulary_可看到所有文本的关键字和其位置</span><span class="token comment" spellcheck="true"># toarray()可看到词频矩阵的结果</span>vectorizer <span class="token operator">=</span> CountVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>count <span class="token operator">=</span> vectorizer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vectorizer<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vectorizer<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>count<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># TfidfTransformer是统计CountVectorizer中每个词语的tf-idf权值</span>transformer <span class="token operator">=</span> TfidfTransformer<span class="token punctuation">(</span><span class="token punctuation">)</span>tfidf_matrix <span class="token operator">=</span> transformer<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>count<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tfidf_matrix<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># TfidfVectorizer可以把CountVectorizer, TfidfTransformer合并起来，直接生成tfidf值</span><span class="token comment" spellcheck="true"># TfidfVectorizer的关键参数：</span><span class="token comment" spellcheck="true"># max_df：这个给定特征可以应用在 tf-idf 矩阵中，用以描述单词在文档中的最高出现率。假设一个词（term）在 80% 的文档中都出现过了，那它也许（在剧情简介的语境里）只携带非常少信息。</span><span class="token comment" spellcheck="true"># min_df：可以是一个整数（例如5）。意味着单词必须在 5 个以上的文档中出现才会被纳入考虑。设置为 0.2；即单词至少在 20% 的文档中出现 。</span><span class="token comment" spellcheck="true"># ngram_range：这个参数将用来观察一元模型（unigrams），二元模型（ bigrams） 和三元模型（trigrams）。</span>tfidf_vec <span class="token operator">=</span> TfidfVectorizer<span class="token punctuation">(</span><span class="token punctuation">)</span>tfidf_matrix <span class="token operator">=</span> tfidf_vec<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>corpus<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tfidf_vec<span class="token punctuation">.</span>get_feature_names<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tfidf_vec<span class="token punctuation">.</span>vocabulary_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tfidf_matrix<span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>One-hot编码详解</title>
      <link href="/2020/04/04/one-hot-bian-ma-xiang-jie/"/>
      <url>/2020/04/04/one-hot-bian-ma-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h1><p>&emsp;&emsp;在回归，分类，聚类等机器学习算法中，各个特征之间的距离(相似度)计算是非常重要的，然而常用的距离计算都是在欧式空间内计算，例如计算余弦相似性。但是在欧式空间内计算相似性要求数据是连续的，有序的。在很多机器学习的任务中，数据都是离散的，例如星期一，星期二，···，星期天，人的性别有男女，祖国有中国，美国，法国等。这些特征值并不是连续的，而是<strong>离散的，无序的</strong>。<br>如果要作为机器学习算法的输入，通常我们需要对其进行特征数字化。什么是特征数字化呢？例如：</p><pre><code>       性别特征：[&quot;男&quot;，&quot;女&quot;]       祖国特征：[&quot;中国&quot;，&quot;美国，&quot;法国&quot;]       运动特征：[&quot;足球&quot;，&quot;篮球&quot;，&quot;羽毛球&quot;，&quot;乒乓球&quot;]</code></pre><p>   怎么将上诉特征数字化呢？有个人他的特征是 [“男”,”中国”,”乒乓球”]，怎么表示他呢？</p><h1 id="1-独热编码"><a href="#1-独热编码" class="headerlink" title="1. 独热编码"></a>1. 独热编码</h1><p>&emsp;&emsp;独热编码即 One-Hot Encoding，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，<strong>并且在任意时候，其中只有一位有效</strong>。one-hot向量将类别变量转换为机器学习算法易于利用的一种形式的过程，这个向量的表示为一项属性的特征向量，也就是同一时间只有一个激活点（不为0），这个向量只有一个特征是不为0的，其他都是0，特别稀疏。</p><h2 id="1-1-独热编码例子"><a href="#1-1-独热编码例子" class="headerlink" title="1.1 独热编码例子"></a>1.1 独热编码例子</h2><p><strong>例1：</strong><br>&emsp;&emsp;我们有四个样本，每个样本有三个特征，如图：<br>||特征1|特征2|特征3|<br>|–|–|–|–|<br>样本1|1|4|3|<br>样本2|2|3|2|<br>样本3|1|2|2|<br>样本4|2|1|1|<br>上诉样本特征1有两种可能的取值，若代表性别，比如1代表男性2代表女性，特征2有4种，可以代表另一种特征，同样的特征3也可以有他的含义。<br>独热编码保证每个样本中的单个特征只有1位数字为1，其余全部为0，编码后表示为：<br>||特征1|特征2|特征3|<br>|–|–|–|–|<br>样本1|01|1000|100|<br>样本2|10|0100|010|<br>样本3|01|0010|010|<br>样本4|10|0001|001|<br>对每个特征都使用独热编码表示，特征有2种取值就用两位表示，4种取值就用4位表示<br>对于<strong>前言</strong>中的例子，可以将特征与具体的特征对应：</p><ul><li>性别特征：[“男”,”女”] （这里只有两个特征，所以 N=2）：<br>男  =&gt;  10<br>女  =&gt;  01</li><li>祖国特征：[“中国”，”美国，”法国”]（N=3）：<br>中国  =&gt;  100<br>美国  =&gt;  010<br>法国  =&gt;  001</li><li>运动特征：[“足球”，”篮球”，”羽毛球”，”乒乓球”]（N=4）：<br>足球  =&gt;  1000<br>篮球  =&gt;  0100<br>羽毛球  =&gt;  0010<br>乒乓球  =&gt;  0001<br>所以，当一个样本为 [“男”,”中国”,”乒乓球”] 的时候，完整的特征数字化的结果为：<br>[1，0，1，0，0，0，0，0，1]<br>前两位代表<strong>性别</strong>，中间三位代表<strong>国家</strong>，后四位代表<strong>运动</strong>。</li></ul><h2 id="1-2-独热编码的优点"><a href="#1-2-独热编码的优点" class="headerlink" title="1.2 独热编码的优点"></a>1.2 独热编码的优点</h2><ul><li>能够处理机器学习算法不好处理的离散特征值。</li><li>在一定程度上增加了特征的维度，比如性别本身是一个特征，经过one hot编码以后，就变成了男或女两个特征。</li><li>将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。</li></ul><h2 id="1-3-独热编码的缺点"><a href="#1-3-独热编码的缺点" class="headerlink" title="1.3 独热编码的缺点"></a>1.3 独热编码的缺点</h2><ul><li>如果原本的标签编码是有序的，那么one-hot编码就会丢失顺序信息。</li><li>如果特征值的数目特别多，就会产生大量冗余的稀疏矩阵</li><li>维度（单词）间的关系没有得到体现，每个单词都是一个维度，彼此相互独立，然而每个单词彼此无关这个特点明显不符合现实情况。大量的单词都是有关的。比如：<ul><li>语义：girl和woman虽然用在不同年龄上，但指的都是女性。</li><li>复数：word和words仅仅是复数和单数的差别。</li><li>时态：buy和bought表达的都是“买”，但发生的时间不同。</li><li>所以用one hot representation的编码方式，上面的特性都没有被考虑到。</li></ul></li></ul><h2 id="1-4-独热编码适用的情况"><a href="#1-4-独热编码适用的情况" class="headerlink" title="1.4 独热编码适用的情况"></a>1.4 独热编码适用的情况</h2><ul><li>One Hot Encoding用来解决类别数据的离散值问题，如果特征是离散的，并且不用One Hot Encoding就可以很合理的计算出距离，那么就没必要进行One Hot Encoding。</li><li>有些基于树的算法在处理变量时，并不是基于向量空间度量，数值只是类别符号，即没有偏序关系，所以不用One Hot Encoding，树模型不太需要One Hot Encoding，对于决策树来说，没有特征大小的概念，只有特征处于哪个部分的概念，One Hot Encoding的本质是增加树的深度。如GBDT处理高维稀疏矩阵的时候效果并不好，即使是低维的稀疏矩阵也未必比SVM好。</li></ul><h1 id="2-独热编码的实现"><a href="#2-独热编码的实现" class="headerlink" title="2. 独热编码的实现"></a>2. 独热编码的实现</h1><h2 id="2-1-python简单实现one-hot编码"><a href="#2-1-python简单实现one-hot编码" class="headerlink" title="2.1 python简单实现one-hot编码"></a>2.1 python简单实现one-hot编码</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> npsamples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'I like playing basketball'</span><span class="token punctuation">,</span> <span class="token string">'I played football yesterday morning'</span><span class="token punctuation">]</span>token_index <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token keyword">for</span> sample <span class="token keyword">in</span> samples<span class="token punctuation">:</span>    <span class="token keyword">for</span> word <span class="token keyword">in</span> sample<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> word <span class="token operator">not</span> <span class="token keyword">in</span> token_index<span class="token punctuation">:</span>            token_index<span class="token punctuation">[</span>word<span class="token punctuation">]</span> <span class="token operator">=</span> len<span class="token punctuation">(</span>token_index<span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span>max_length <span class="token operator">=</span> <span class="token number">10</span>results <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>shape<span class="token operator">=</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>samples<span class="token punctuation">)</span><span class="token punctuation">,</span>                          max_length<span class="token punctuation">,</span>                          max<span class="token punctuation">(</span>token_index<span class="token punctuation">.</span>values<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">for</span> i<span class="token punctuation">,</span> sample <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>samples<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">for</span> j<span class="token punctuation">,</span> word <span class="token keyword">in</span> list<span class="token punctuation">(</span>enumerate<span class="token punctuation">(</span>sample<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token punctuation">:</span>max_length<span class="token punctuation">]</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>j<span class="token punctuation">)</span>        index <span class="token operator">=</span> token_index<span class="token punctuation">.</span>get<span class="token punctuation">(</span>word<span class="token punctuation">)</span>        results<span class="token punctuation">[</span>i<span class="token punctuation">,</span> j<span class="token punctuation">,</span> index<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span><span class="token keyword">print</span><span class="token punctuation">(</span>results<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-2-sklearn"><a href="#2-2-sklearn" class="headerlink" title="2.2 sklearn"></a>2.2 sklearn</h2><p>通过sklearn的OneHotEncoder()来得到独热编码，但是只适用于数值型的数据。OneHotEncoder()的 feature_indices_ 可以知道哪几列对应哪个原来的特征。<br>使用 numpy.hstack() 将多次结果拼接起来得到变换后的结果<br>问题：不能直接编码字符串类型数据（LabelEncoder() + OneHotEncoder() 可实现，但需数据格式转换）</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> preprocessingenc <span class="token operator">=</span> OneHotEncoder<span class="token punctuation">(</span><span class="token punctuation">)</span>enc<span class="token punctuation">.</span>fit<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"enc.n_values_ is:"</span><span class="token punctuation">,</span>enc<span class="token punctuation">.</span>n_values_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"enc.feature_indices_ is:"</span><span class="token punctuation">,</span>enc<span class="token punctuation">.</span>feature_indices_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>enc<span class="token punctuation">.</span>transform<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>enc<span class="token punctuation">.</span>transform<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>enc<span class="token punctuation">.</span>transform<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>toarray<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出的结果：</p><pre class="line-numbers language-python"><code class="language-python">enc<span class="token punctuation">.</span>n_values_ <span class="token keyword">is</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">2</span> <span class="token number">3</span> <span class="token number">4</span><span class="token punctuation">]</span>          enc<span class="token punctuation">.</span>feature_indices_ <span class="token keyword">is</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span> <span class="token number">2</span> <span class="token number">5</span> <span class="token number">9</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true">#特征坐标</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>enc.n_values_ is ：每个特征值的特征数目，第一个特征数目是2，第二个特征数目是3，第三个特征数目是4。<br>enc.feature_indices_ is ：表明每个特征在one-hot向量中的坐标范围，0-2 是第一个特征，2-5就是第二个特征，5-9是第三个特征。<br>后面三个就是把特征值转换为 one-hot编码，我们可以对比结果看看one-hot差别。</p><h2 id="2-3-Keras"><a href="#2-3-Keras" class="headerlink" title="2.3 Keras"></a>2.3 Keras</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> keras<span class="token punctuation">.</span>preprocessing<span class="token punctuation">.</span>text <span class="token keyword">import</span> Tokenizersamples <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'I like playing basketball'</span><span class="token punctuation">,</span> <span class="token string">'I played football yesterday morning'</span><span class="token punctuation">]</span>tokenizer <span class="token operator">=</span> Tokenizer<span class="token punctuation">(</span>num_words<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>tokenizer<span class="token punctuation">.</span>fit_on_texts<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>sequences <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>texts_to_sequences<span class="token punctuation">(</span>samples<span class="token punctuation">)</span>one_hot_results <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>text_to_matrix<span class="token punctuation">(</span>samples<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">"binary"</span><span class="token punctuation">)</span>word_index <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>word_index<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Found %s unique tokens.'</span> <span class="token operator">%</span> len<span class="token punctuation">(</span>word_index<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-4-tensorflow"><a href="#2-4-tensorflow" class="headerlink" title="2.4 tensorflow"></a>2.4 tensorflow</h2><p>官方文档：</p><pre class="line-numbers language-python"><code class="language-python">tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>    indices<span class="token punctuation">,</span>    depth<span class="token punctuation">,</span>    on_value<span class="token operator">=</span>None<span class="token punctuation">,</span>    off_value<span class="token operator">=</span>None<span class="token punctuation">,</span>    axis<span class="token operator">=</span>None<span class="token punctuation">,</span>    dtype<span class="token operator">=</span>None<span class="token punctuation">,</span>    name<span class="token operator">=</span>None<span class="token punctuation">)</span>Returns a one<span class="token operator">-</span>hot tensor<span class="token punctuation">(</span>返回一个one_hot张量<span class="token punctuation">)</span><span class="token punctuation">.</span>The locations represented by indices <span class="token keyword">in</span> indices take value on_value<span class="token punctuation">,</span> <span class="token keyword">while</span> all other locations take value off_value<span class="token punctuation">.</span><span class="token punctuation">(</span>由indices指定的位置将被on_value填充<span class="token punctuation">,</span> 其他位置被off_value填充<span class="token punctuation">)</span><span class="token punctuation">.</span>on_value <span class="token operator">and</span> off_value must have matching data types<span class="token punctuation">.</span> If dtype <span class="token keyword">is</span> also provided<span class="token punctuation">,</span> they must be the same data type <span class="token keyword">as</span> specified by dtype<span class="token punctuation">.</span><span class="token punctuation">(</span>on_value和off_value必须具有相同的数据类型<span class="token punctuation">)</span><span class="token punctuation">.</span>If on_value <span class="token keyword">is</span> <span class="token operator">not</span> provided<span class="token punctuation">,</span> it will default to the value <span class="token number">1</span> <span class="token keyword">with</span> type dtype<span class="token punctuation">.</span>If off_value <span class="token keyword">is</span> <span class="token operator">not</span> provided<span class="token punctuation">,</span> it will default to the value <span class="token number">0</span> <span class="token keyword">with</span> type dtype<span class="token punctuation">.</span>If the input indices <span class="token keyword">is</span> rank N<span class="token punctuation">,</span> the output will have rank N<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">.</span> The new axis <span class="token keyword">is</span> created at dimension axis <span class="token punctuation">(</span>default<span class="token punctuation">:</span> the new axis <span class="token keyword">is</span> appended at the end<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token punctuation">(</span>如果indices是N维张量，那么函数输出将是N<span class="token operator">+</span><span class="token number">1</span>维张量<span class="token punctuation">,</span>默认在最后一维添加新的维度<span class="token punctuation">)</span><span class="token punctuation">.</span>If indices <span class="token keyword">is</span> a scalar the output shape will be a vector of length depth<span class="token punctuation">.</span><span class="token punctuation">(</span>如果indices是一个标量<span class="token punctuation">,</span> 函数输出将是一个长度为depth的向量<span class="token punctuation">)</span>If indices <span class="token keyword">is</span> a vector of length features<span class="token punctuation">,</span> the output shape will be<span class="token punctuation">:</span>  features x depth <span class="token keyword">if</span> axis <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">(</span>如果indices是一个长度为features的向量<span class="token punctuation">,</span>则默认输出一个features<span class="token operator">*</span>depth形状的张量<span class="token punctuation">)</span>  depth x features <span class="token keyword">if</span> axis <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">(</span>如果indices是一个长度为features的向量<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>则输出一个depth<span class="token operator">*</span>features形状的张量<span class="token punctuation">)</span>If indices <span class="token keyword">is</span> a matrix <span class="token punctuation">(</span>batch<span class="token punctuation">)</span> <span class="token keyword">with</span> shape <span class="token punctuation">[</span>batch<span class="token punctuation">,</span> features<span class="token punctuation">]</span><span class="token punctuation">,</span> the output shape will be<span class="token punctuation">:</span>  batch x features x depth <span class="token keyword">if</span> axis <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">(</span>如果indices是一个形状为<span class="token punctuation">[</span>batch<span class="token punctuation">,</span> features<span class="token punctuation">]</span>的矩阵<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">(</span>默认<span class="token punctuation">)</span><span class="token punctuation">,</span>则输出一个batch <span class="token operator">*</span> features <span class="token operator">*</span> depth形状的张量<span class="token punctuation">)</span>  batch x depth x features <span class="token keyword">if</span> axis <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">(</span>如果indices是一个形状为<span class="token punctuation">[</span>batch<span class="token punctuation">,</span> features<span class="token punctuation">]</span>的矩阵<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>则输出一个batch <span class="token operator">*</span> depth <span class="token operator">*</span> features形状的张量<span class="token punctuation">)</span>  depth x batch x features <span class="token keyword">if</span> axis <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">(</span>如果indices是一个形状为<span class="token punctuation">[</span>batch<span class="token punctuation">,</span> features<span class="token punctuation">]</span>的矩阵<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>则输出一个depth <span class="token operator">*</span> batch <span class="token operator">*</span> features形状的张量<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实现：</p><pre class="line-numbers language-python"><code class="language-python">indices <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#输入数据(是个向量)需要编码的索引是[0,1,2]</span>depth <span class="token operator">=</span> <span class="token number">3</span>tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> depth<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># output: [3 x 3]</span><span class="token comment" spellcheck="true"># [[1., 0., 0.],</span><span class="token comment" spellcheck="true">#  [0., 1., 0.],</span><span class="token comment" spellcheck="true">#  [0., 0., 1.]]</span>indices <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#输入数据(是个向量)的需要编码的索引是[0,2,-1,1]</span>depth <span class="token operator">=</span> <span class="token number">3</span>tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> depth<span class="token punctuation">,</span>           on_value<span class="token operator">=</span><span class="token number">5.0</span><span class="token punctuation">,</span> off_value<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>           axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># output: [4 x 3]</span><span class="token comment" spellcheck="true"># [[5.0, 0.0, 0.0],  # one_hot(0)  对位置0处的数据进行one_hot编码</span><span class="token comment" spellcheck="true">#  [0.0, 0.0, 5.0],  # one_hot(2)  对位置2处的数据进行one_hot编码</span><span class="token comment" spellcheck="true">#  [0.0, 0.0, 0.0],  # one_hot(-1) 对位置-1处的数据进行one_hot编码</span><span class="token comment" spellcheck="true">#  [0.0, 5.0, 0.0]]  # one_hot(1)  对位置1处的数据进行one_hot编码</span>indices <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>   <span class="token comment" spellcheck="true">#输入数据是个矩阵</span>depth <span class="token operator">=</span> <span class="token number">3</span>tf<span class="token punctuation">.</span>one_hot<span class="token punctuation">(</span>indices<span class="token punctuation">,</span> depth<span class="token punctuation">,</span>           on_value<span class="token operator">=</span><span class="token number">1.0</span><span class="token punctuation">,</span> off_value<span class="token operator">=</span><span class="token number">0.0</span><span class="token punctuation">,</span>           axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># output: [2 x 2 x 3]</span><span class="token comment" spellcheck="true"># [[[1.0, 0.0, 0.0],   # one_hot(0)  对位置(0,0)处的数据进行one_hot编码</span><span class="token comment" spellcheck="true">#   [0.0, 0.0, 1.0]],  # one_hot(2)  对位置(0,2)处的数据进行one_hot编码</span><span class="token comment" spellcheck="true">#  [[0.0, 1.0, 0.0],   # one_hot(1)  对位置(1,1)处的数据进行one_hot编码</span><span class="token comment" spellcheck="true">#   [0.0, 0.0, 0.0]]]  # one_hot(-1) 对位置(1,-1)处的数据进行one_hot编码</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="3-NLP中的独热表示"><a href="#3-NLP中的独热表示" class="headerlink" title="3 NLP中的独热表示"></a>3 NLP中的独热表示</h1><p>独热表示以往在NLP中很流行，但是随着TF-IDF以及词向量的出现，已经渐渐变得不再适用了，主要的缺点：</p><ul><li>不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）；</li><li>假设词与词相互独立，每个词之间的距离都是$\sqrt 2$。（在大多数情况下，词与词是相互影响的）；</li><li>它得到的 特征是离散稀疏的，词表多少个单词，向量的维度就是多少。 (这个问题最严重)。</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>gensim训练word2vec词向量</title>
      <link href="/2020/04/03/gensim-xun-lian-word2vec-ci-xiang-liang/"/>
      <url>/2020/04/03/gensim-xun-lian-word2vec-ci-xiang-liang/</url>
      
        <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h1><p><a href="https://blog.csdn.net/Elenstone/article/details/105180518" target="_blank" rel="noopener">《词向量之Word2Vec数学原理以及源代码详解》</a>很好的讲解了Word2Vec的原理以及一些源码的解读，Word2Vec的词向量有两种方式实现，一种是谷歌大佬们自己写的word2vec，还有一个是gensim库，由于大多数人使用的是python，所以使用gensim库的人很多。本文会详细介绍gensim里的word2vec模型，gensim包含的其他模型不介绍。</p><h1 id="1-gensim库"><a href="#1-gensim库" class="headerlink" title="1 gensim库"></a>1 gensim库</h1><p>  Gensim（<a href="http://pypi.python.org/pypi/gensim）是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。" target="_blank" rel="noopener">http://pypi.python.org/pypi/gensim）是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。</a> 主要用于主题建模和文档相似性处理，它支持包括TF-IDF，LSA，LDA，和word2vec在内的多种主题模型算法。Gensim在诸如获取单词的词向量等任务中非常有用。</p><h2 id="1-1-gensim-models-word2vec-API概述"><a href="#1-1-gensim-models-word2vec-API概述" class="headerlink" title="1.1 gensim.models.word2vec API概述"></a>1.1 gensim.models.word2vec API概述</h2><p>首先使用<code>pip install gensim</code>安装gensim库。<br>然后倒入word2vec模块<code>from gensim.models.word2vec</code>。==注意这里倒入的是小写的word2vec这个<code>.py</code>文件，其中大写的Word2Vec模型实现在这个文件中，使用模型时需要使用语句<code>word2vec.Word2Vec()</code>来创建模型。==</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Word2Vec</span><span class="token punctuation">(</span>utils<span class="token punctuation">.</span>SaveLoad<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>            self<span class="token punctuation">,</span> sentences<span class="token operator">=</span>None<span class="token punctuation">,</span> size<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">,</span> alpha<span class="token operator">=</span><span class="token number">0.025</span><span class="token punctuation">,</span> window<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> min_count<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>            max_vocab_size<span class="token operator">=</span>None<span class="token punctuation">,</span> sample<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span> seed<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> workers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> min_alpha<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span>            sg<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> hs<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> negative<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> cbow_mean<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> hashfxn<span class="token operator">=</span>hash<span class="token punctuation">,</span> iter<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span> null_word<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>            trim_rule<span class="token operator">=</span>None<span class="token punctuation">,</span> sorted_vocab<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> batch_words<span class="token operator">=</span>MAX_WORDS_IN_BATCH<span class="token punctuation">)</span><span class="token punctuation">:</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong>sentences</strong>：可以是一个list，对于大语料集，建议使用LneSentence以及PathLineSentence来构建。创建模型的时候如果没有这个参数，会传入一个None对象，在后续训练时可以再传入训练的语料。</li><li><strong>size</strong>：是指词向量的维度，默认为100。这个维度的取值一般与我们的语料的大小相关，如果是不大的语料，比如小于100M的文本语料，则使用默认值一般就可以了。如果是超大的语料，建议增大维度。大的size需要更多的训练数据,但是效果会更好. 推荐值为100到300。</li><li><strong>window</strong>：窗口大小，即词向量上下文最大距离，即包含中心词的前window个单词和后window个单词，<strong>注意在word2vec中是在[1,windows]之间随机取值，并不是固定的window个</strong>。window越大，则和某一词较远的词也会产生上下文关系。默认值为5。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。个人理解应该是某一个中心词可能与前后多个词相关，也有的词在一句话中可能只与少量词相关（如短文本可能只与其紧邻词相关）。</li><li><strong>min_count</strong>: 需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。可以对字典做截断， 词频少于min_count次数的单词会被丢弃掉。</li><li><strong>negative</strong>：即使用Negative Sampling时负采样的个数，默认是5。推荐在[3,10]之间。</li><li><strong>cbow_mean</strong>: 仅用于CBOW在做投影的时候，为0，则算法中的为上下文的词向量之和，为1则为上下文的词向量的平均值。默认值是1，不推荐修改默认值。<ul><li><strong>iter</strong>: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值，在下文的语料中，我使用5次，训练的效果不是很好。</li></ul></li><li><strong>alpha</strong>: 是初始的学习速率，在训练过程中会线性地递减到min_alpha。在随机梯度下降法中迭代的初始学习率，默认是0.025。</li><li><strong>min_alpha</strong>: 由于算法支持在迭代的过程中逐渐减小学习率，min_alpha给出了最小的学习率。随机梯度下降中每轮的迭代步长可以由iter，alpha， min_alpha一起得出。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。</li><li><strong>max_vocab_size</strong>: 设置词向量构建期间的RAM限制，设置成None则没有限制。</li><li><strong>sample</strong>: 高频词汇的随机降采样的配置阈值，默认为1e-3，范围是(0,1e-5)。</li><li><strong>seed</strong>：用于随机数发生器。与初始化词向量有关。</li><li><strong>workers</strong>：用于控制训练的并行数。</li><li><strong>sg</strong>: 即我们的word2vec两个模型的选择了。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。</li><li><strong>hs</strong>: 即我们的word2vec两个解法的选择了，如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。</li><li><strong>negative</strong>:如果大于零，则会采用negativesampling，用于设置多少个noise words（一般是5-20）。</li><li><strong>hashfxn</strong>：hash函数来初始化权重，默认使用python的hash函数。</li><li><strong>batch_words</strong>：每一批的传递给线程的单词的数量，默认为10000。</li><li><strong>trim_rule</strong>：用于设置词汇表的整理规则，指定那些单词要留下，哪些要被删除。可以设置为None（min_count会被使用）。</li><li><strong>sorted_vocab</strong>： 如果为1（默认），则在分配word index 的时候会先对单词基于频率降序排序。</li></ul><h2 id="1-2-gensim训练word2vec词向量步骤"><a href="#1-2-gensim训练word2vec词向量步骤" class="headerlink" title="1.2 gensim训练word2vec词向量步骤"></a>1.2 gensim训练word2vec词向量步骤</h2><p>使用Gensim训练Word2vec十分方便，训练步骤如下：</p><ul><li>1）将语料库预处理：一行一个文档或句子，将文档或句子分词（以空格分割，英文可以不用分词，英文单词之间已经由空格分割，中文预料需要使用分词工具进行分词，常见的分词工具有StandNLP、ICTCLAS、Ansj、FudanNLP、HanLP、结巴分词等）；</li><li>2）将原始的训练语料转化成一个sentence的迭代器，每一次迭代返回的sentence是一个word（utf8格式）的列表。可以使用Gensim中word2vec.py中的LineSentence()方法实现；</li><li>3）将上面处理的结果输入Gensim内建的word2vec对象进行训练即可。</li></ul><h1 id="2-训练搜狗语料"><a href="#2-训练搜狗语料" class="headerlink" title="2 训练搜狗语料"></a>2 训练搜狗语料</h1><p>语料：搜狗实验室新闻作为训练语料   搜狗实验室：<a href="http://www.sogou.com/labs/resource/ca.php" target="_blank" rel="noopener">http://www.sogou.com/labs/resource/ca.php</a><br>我下载的是完整版，一共600多M，解压后1个多G，可以在首页看看数据的格式。<br>==注意下载时需要登记信息==<br>解压后的数据是<code>news_sohusite_xml.dat</code>，这是gbk格式的数据，需要将编码转换成utf-8格式，并且只需要content的内容，其他的都不需要，这个时候就需要使用linux命令来解决，由于我的系统是windows10，需要下载一个WSL来运行linux命令。具体的见我的博客：<br><a href="https://blog.csdn.net/Elenstone/article/details/105285524" target="_blank" rel="noopener">windows下使用Linux命令</a><br>取出<content>  </content> 中的内容，在linux下执行如下命令：<br><code>cat news_sohusite_xml.dat | iconv -f gbk -t utf-8 -c | grep &quot;&lt;content&gt;&quot;  &gt; corpus.txt</code><br>得到了1.71个G的<code>corpus.txt</code>文件，文件太大记事本打不开这个文件。<br>注意以下所有程序都在jupyter notebook中运行，jupyter notebook下载可以通过Anaconda来实现，见：<br><a href="https://blog.csdn.net/Elenstone/article/details/105120661" target="_blank" rel="noopener">Windows 10下的Anaconda安装与使用</a></p><h2 id="2-1-分词"><a href="#2-1-分词" class="headerlink" title="2.1 分词"></a>2.1 分词</h2><p>送给word2vec的文件是需要分词的，分词可以采用jieba分词实现，安装jieba 分词<br>使用<code>pip install jieba</code>对原始文本进行分词：</p><pre class="line-numbers language-python"><code class="language-python">file_path <span class="token operator">=</span> <span class="token string">'./corpus.txt'</span>file_segment_path <span class="token operator">=</span> <span class="token string">'./corpus_segment.txt'</span>train_file_read <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>file_path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> line <span class="token keyword">in</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        train_file_read<span class="token punctuation">.</span>append<span class="token punctuation">(</span>line<span class="token punctuation">)</span>train_file_read<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>得到：<br><img src="https://img-blog.csdnimg.cn/2020040310062574.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><code>print(len(train_file_read))</code>查看一共有多少段话，输出1411996。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> jieba<span class="token comment" spellcheck="true"># jieba分词后保存在列表中</span>train_file_seg <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>train_file_read<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    train_file_seg<span class="token punctuation">.</span>append<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">' '</span><span class="token punctuation">.</span>join<span class="token punctuation">(</span>list<span class="token punctuation">(</span>jieba<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>train_file_read<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">:</span><span class="token operator">-</span><span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">,</span> cut_all<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> i <span class="token operator">%</span> <span class="token number">100</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span>train_file_seg<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>分词会花费一个多小时</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 保存分词结果到文件中</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>file_segment_path<span class="token punctuation">,</span> <span class="token string">'w'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>train_file_seg<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>train_file_seg<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 加载分词</span>seg_sentences <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">with</span> open<span class="token punctuation">(</span>file_segment_path<span class="token punctuation">,</span> <span class="token string">'r'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">'utf-8'</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    seg_sentences <span class="token operator">=</span> f<span class="token punctuation">.</span>readlines<span class="token punctuation">(</span><span class="token punctuation">)</span>seg_sentences<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><img src="https://img-blog.csdnimg.cn/20200403100845878.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>可以看到有空白段<br>除去空白段<br><img src="https://img-blog.csdnimg.cn/20200403100934910.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>一共还剩1298156段话<br><img src="https://img-blog.csdnimg.cn/20200403100953318.png" alt="在这里插入图片描述"></p><h2 id="2-2-构建词向量"><a href="#2-2-构建词向量" class="headerlink" title="2.2 构建词向量"></a>2.2 构建词向量</h2><p><img src="https://img-blog.csdnimg.cn/20200403101053202.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>训练会花费一个多小时</p><h2 id="2-3-保存和加载模型"><a href="#2-3-保存和加载模型" class="headerlink" title="2.3 保存和加载模型"></a>2.3 保存和加载模型</h2><p><img src="https://img-blog.csdnimg.cn/20200403101133792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h2 id="2-4-词向量使用"><a href="#2-4-词向量使用" class="headerlink" title="2.4 词向量使用"></a>2.4 词向量使用</h2><p><img src="https://img-blog.csdnimg.cn/2020040310121470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>注意<code>model.similarity</code>在以前也是可以使用的，新版本需要<code>model.wv.similarity</code></p><h1 id="3-二维空间显示词向量"><a href="#3-二维空间显示词向量" class="headerlink" title="3 二维空间显示词向量"></a>3 二维空间显示词向量</h1><p>见我的<a href="https://github.com/CodingMarathon/NLP/tree/master/%E8%AF%8D%E5%90%91%E9%87%8F/word2vec" target="_blank" rel="noopener">github</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec详解</title>
      <link href="/2020/03/29/word2vec-xiang-jie/"/>
      <url>/2020/03/29/word2vec-xiang-jie/</url>
      
        <content type="html"><![CDATA[<p>﻿本文主要转自<a href="https://blog.csdn.net/itplus/article/details/37969519" target="_blank" rel="noopener">word2vec 中的数学原理详解（一）目录和前言</a>，原文讲的十分透彻但是是分开的，所以将它合并在一起，记录在自己的专栏里，防止忘记。<br><img src="https://img-blog.csdnimg.cn/20200329160551487.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020032916061275.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160651619.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020032916070765.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160721869.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020032916073880.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160752449.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160803667.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160818107.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160858512.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160910158.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160922970.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160933282.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160945841.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329160957816.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161008327.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161018612.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161031754.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161044304.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2020032916105775.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161107588.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161118933.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161129956.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161141716.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161152682.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161238998.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161252728.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161306384.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161316534.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161328812.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161338498.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161349310.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161400810.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161412983.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161423118.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161434643.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161445284.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161457388.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161512147.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161523562.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161534737.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161544249.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161600234.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161622676.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161635434.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/20200329161645265.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CRF详解</title>
      <link href="/2020/03/26/crf/"/>
      <url>/2020/03/26/crf/</url>
      
        <content type="html"><![CDATA[<h1 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h1><p>​        条件随机场（conditional random field，CRF）是给定一组输入<strong>随机变量</strong>条件下另一组输出<strong>随机变量</strong>的条件概率分布模型，其特点是假设输出随机变量构成马尔科夫随机场。本文主要讨论它在标注问题上的应用，因此主要讲<strong>线性链（linear chain）条件随机场</strong>，这时问题变成了由输入序列对输出序列预测的判别模型，形式为对数线性模型，学习模型参数的方法通常是极大似然估计或者正则化的极大似然估计。</p><p>​        本文参考李航老师的《统计学习方法》,加上自己的理解。</p><h1 id="1-马尔科夫随机场"><a href="#1-马尔科夫随机场" class="headerlink" title="1 马尔科夫随机场"></a>1 马尔科夫随机场</h1><p>​        概率无向图模型（probabilistic undirected graphical model），又称为马尔科夫随机场（Markov random field），是一个可以由无向图表示的联合概率分布。</p><h2 id="1-1-概率图模型的定义"><a href="#1-1-概率图模型的定义" class="headerlink" title="1.1 概率图模型的定义"></a>1.1 概率图模型的定义</h2><p>​         图是由节点及连接节点的边组成的集合。节点和边分别记作$v$和$e$，节点和边的集合分别记作$V$和$E$，图记作$G=(V,E)$。无向图是指边没有方向的图，有向图是指边有方向的图。<br><img src="https://img-blog.csdnimg.cn/20200323164917172.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>        概率图模型是用图来表示<strong>概率分布</strong>，设有联合概率分布$P(\boldsymbol{Y})$，$\boldsymbol{Y}$是一组随机变量。用图$G=(V,E)$来表示概率分布$P(\boldsymbol{Y})$。在图$G$中，节点$v \in V $表示一个随机变量$Y_v$，则$\boldsymbol{Y}=(Y_v)_{v \in V}$；边$e \in E$表示随机变量之间的概率依赖关系。常用的概率图模型分为两类：</p><ul><li>有向图：贝叶斯网络。信念网络</li><li>无向图：马尔科夫随机场、马尔科夫网络  </li></ul><p><strong>概率图模型是一门很深的学问，在大学里面也是专门的一门课来讲。这篇文章仅仅是代表我自己的认知，也仅仅限于标注模型。</strong><br><img src="https://img-blog.csdnimg.cn/2020032316495293.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>现在已经用图来表示概率分布，那怎么根据图来计算概率分布呢？</p><ol><li><p>概率有向图模型的联合概率：</p><p>概率有向图模型的联合概率一般按照如下所示的公式求解：</p></li></ol><pre><code>$$P(X_1,X_2,...,X_N)=\prod\limits_{i=1}^NP(X_i|\pi(X_i))$$其中， $\pi(X_i)$是$X_i$的所有父节点，**按理说应该是$P(\boldsymbol{Y})$来表示概率，但是画图时用的是$X$，这是一个小错误**。![在这里插入图片描述](https://img-blog.csdnimg.cn/2020032316502081.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center) 由图可得：</code></pre><p>$$<br>P(X_1,X_2,…,X_5)=P(X_1)P(X_2|X_1)P(X_3|X_2)P(X_4|X_2)P(X_5|X_3,X_4)​<br>$$</p><ol start="2"><li><p>概率无向图模型的联合概率：</p><p>无向图一般就指的是马尔科夫网路，无向图的无向性导致不能用条件概率参数化表示联合概率，<em><u>而是需要从一组条件独立的原则中找出一系列局部函数的乘积来表示联合概率。</u></em></p></li></ol><p><img src="https://img-blog.csdnimg.cn/20200323165635475.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>如果无向图是一个很大的图，可以使用<strong>因子分解</strong>将$P(\boldsymbol{X})$写成若干个联合概率的乘积，所以问题来了，怎么分解呢？将图分解为若干个“小团”，注意每一个团必须是<strong>最大团</strong>（定义在后面介绍），用$Q\in C$表示，$C$是最大团的集合。下面给出$P(\boldsymbol{X})$的公式，<strong>也叫因子分解式</strong>:<br>$$<br>P(\boldsymbol{X})=\frac{1}{Z}\prod\limits_{Q \in C}\psi_Q(\boldsymbol{x}_Q)<br>$$<br>其中，$Z=\sum\limits_\boldsymbol{x}\prod\limits_{Q\in C}\psi_Q(\boldsymbol{x_Q})$为规范化因子，是为了保证概率之和为1，类似于softmax。因为在计算上式时并<strong>不是按照概率</strong>来计算的，而是定义了<strong>特征函数</strong>(见后文)，数值代表不了概率。在实际应用中，精确计算$Z$是很困难的，但许多任务往往并不需要获得$Z$的精确值。$\psi_Q(\boldsymbol{x_Q})$是一个最大团$Q$所包含的<strong>所有随机变量</strong>的联合概率，也称作<strong>势函数</strong>，这个函数必须是非负实数函数，一般取指数函数：<br>$$<br>\psi_Q(\boldsymbol{x}_Q)=\exp\{-E(Y_Q)\}<br>$$</p><h2 id="1-2-马尔科夫随机场"><a href="#1-2-马尔科夫随机场" class="headerlink" title="1.2 马尔科夫随机场"></a>1.2 马尔科夫随机场</h2><p>先给一个简单的马尔科夫随机场。<br><img src="https://img-blog.csdnimg.cn/2020032316533986.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ol><li><p><strong>团的定义</strong>：</p><p>对于图中节点的一个子集，若任意子集中任意两个节点间都有边连接，则称该节点子集为一个“团”（clique）。若在一个团中加入任何一个节点都不再形成团，则称这个团为“最大团”。最小的最大团只有两个节点，如上图中的$\{x_2,x_4\}$。</p><p>团：$\{x_1,x_2\},\{x_1,x_3\},\{x_2,x_4\},\{x_2,x_5\},\{x_2,x_6\},\{x_3,x_5\},\{x_5,x_6\},\{x_2,x_5,x_6\}$</p><p>最大团：$\{x_1,x_2\},\{x_1,x_3\},\{x_2,x_4\},\{x_3,x_5\},\{x_2,x_5,x_6\}$</p></li></ol><p>按照上述的公式求解概率，只需用到最大团。原因：若$Q$不是最大团，则它一定会被一个另一个最大团$Q^<em>$完全包含，即$Q\in Q^</em>$，这意味着$Q$包含的变量之间的关系不仅体现在势函数$\psi_Q$中，还体现在$\psi_{Q^<em>}$中，这样就乱套了，所以联合概率<em>*必须使用最大团</em></em>来定义。</p><p>上述这个图的联合概率为：<br>$$<br>P(\boldsymbol{X})=\frac{1}{Z}\psi_{12}(x_1,x_2)\psi_{13}(x_1,x_3)\psi_{24}(x_2,x_4)\psi_{35}(x_3,x_5)\psi_{256}(x_2,x_5,x_6)<br>$$<br>其中，$\psi_{256}(x_2,x_5,x_6)$定义在最大团$\{x_2,x_5,x_6\}$上，由于它的存在不需要为团$\{x_2,x_5\},\{x_2,x_6\},\{x_5,x_6\}$构建势函数。</p><p>​        在马尔科夫随机场中，还需要得到“条件独立性”，即马尔科夫性，同样借助分离的概念。马尔科夫性是保证或者判断概率图是否为概率无向图的条件，一共三点内容：</p><ul><li><p>全局马尔科夫性</p></li><li><p>局部马尔科夫性</p></li><li><p>成对马尔科夫性</p><p>注：这个只作为判断一个概率图是不是马尔科夫随机场，可以不用重点关注。</p></li></ul><ol start="2"><li><p><strong>全局马尔科夫性</strong>：</p><p>分离集的定义：若从节点集A的节点到B中的节点都必须经过节点集C中的节点，则称节点集A和B被节点集C分离，C成为分离集。</p><p><img src="https://img-blog.csdnimg.cn/20200323165359343.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>全局马尔科夫性：给定两个变量子集的分离集，则这两个变量子集条件独立。记作：<br>$$<br>P(x_A,x_B|x_C)=P(x_A|x_C)P(x_B|x_C)<br>$$<br>相关证明可以查看《机器学习_周志华》</p></li><li><p><strong>局部马尔科夫性</strong>：</p><p>给定某变量的邻接变量，则该变量条件独立于其他变量，设$W$是与v有边连接的所有节点集合，$O$是$v$，$W$以外的其他所有节点集合，则：<br>$$<br>P(X_V,X_O|X_W)=P(X_V|X_W)P(X_O|X_W)<br>$$<br>若$P(X_O|X_W)&gt;0$，则有：<br>$$<br>P(X_V|X_W)=P(X_V|X_W,X_O)<br>$$</p></li><li><p><strong>成对马尔科夫性</strong>：</p><p>设$u$和$v$是无向图中的任意两个没有边连接的节点，其他所有节点集合为$O$，成对马尔科夫性是指给定随机变量组$X_O$的条件下随机变量$X_u$和$X_v$是条件独立的，即：<br>$$<br>P(X_u,X-v|X_O)=P(X_u|X_O)P(X_v|X_O)<br>$$</p></li></ol><p>针对上述的定义，现在给出概率无向图模型的定义：</p><p>​        <strong>设有联合概率分布$P(\boldsymbol{X})$，由无向图$G=(V,E)$表示，在途中，节点表示随机变量，边表示随机变量之间的依赖关系。如果连个概率分布满足成对、局部和全局马尔科夫性，就称此联合概率分布为概率无向图模型，或马尔科夫随机场。</strong></p><h1 id="2-条件随机场"><a href="#2-条件随机场" class="headerlink" title="2 条件随机场"></a>2 条件随机场</h1><p>​        条件随机场是给定随机变量$\boldsymbol{X}$条件下，随机变量$\boldsymbol{Y}$的马尔科夫随机场。这里主要介绍定义在线性链上的特殊的条件随机场，成为线性链条件随机场。注意条件随机场是<strong>判别模型</strong>，是对条件概率$P(\boldsymbol{Y}|\boldsymbol{X})$进行建模。线性链条件随机场可以用于标注问题，这时，在条件概率模型$P(\boldsymbol{Y}|\boldsymbol{X})$中，$\boldsymbol{Y}$是输出变量，表示标记序列，也可以称为状态序列，$\boldsymbol{X}$是输入变量，表示需要标注的观测序列。在学习模型参数时，利用训练数据及通过极大似然估计或正则化的极大似然估计得到条件概率模型$\hat{P}(\boldsymbol{Y}|\boldsymbol{X})$；预测时，对于给定的输入序列$\boldsymbol{x}$，求出条件概率$\hat{P}(\boldsymbol{y}|\boldsymbol{x})$最大的输出序列$\hat{\boldsymbol{y}}$。</p><h2 id="2-1-条件随机场的定义"><a href="#2-1-条件随机场的定义" class="headerlink" title="2.1 条件随机场的定义"></a>2.1 条件随机场的定义</h2><p>​        <strong>定义</strong>：设$\boldsymbol{X}$与$\boldsymbol{Y}$是随机变量，$P(\boldsymbol{Y}|\boldsymbol{X})$是在给定$\boldsymbol{X}$的条件下$\boldsymbol{Y}$的条件概率分布，若随机变量$\boldsymbol{Y}$构成了一个由无向图$G=(V,E)$表示的马尔科夫随机场，即<br>$$<br>P(Y_v|X,Y_w,w\neq v)=P(Y_v|X,Y_w,w \sim v)<br>$$<br>对任意节点$v$成立，则称条件概率分布$P(\boldsymbol{Y}|\boldsymbol{X})$为条件随机场，其中$w\neq v$表示节点$v$以外的所有节点，$w \sim v$表示在图中与节点$v$有边连接的所有节点$w$，$Y_w,Y_v,Y_u$是与节点对应的随机变量。</p><p>​        在定义中并没有要求$\boldsymbol{X}$与$\boldsymbol{Y}$具有想听的结构，现实中，一般假设$\boldsymbol{X}$与$\boldsymbol{Y}$有$\color{red}相同的图结构$，本文也只考虑无向图为下图所示的线性链的情况，用符号表示为：<br>$$<br>G=(V=\{1,2,3,…,n\},\quad E=\{(i,i+1)\})<br>$$<br><img src="https://img-blog.csdnimg.cn/20200323165429546.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>​        在此情况下，最大团集合就是两个相邻的节点的集合。</p><p><strong>线性链条件随机场的定义</strong>：设$\boldsymbol{X}=(X_1,X_2,X_3,…,X_n),\boldsymbol{Y}=(Y_1,Y_2,Y_3,…,Y_n)$均为线性链表示的随机变量序列，若在给定随机变量序列$\boldsymbol{X}$的条件下，随机变量序列$\boldsymbol{Y}$的条件概率分布$P(\boldsymbol{Y}|\boldsymbol{X})$构成条件随机场，即满足马尔科夫性：<br>$$<br>P(Y_i|\boldsymbol{X},Y_1,Y_2,..,Y_{i-1},Y_{i+1},…,Y_n)=P(Y_i|\boldsymbol{X},Y_{i-1},Y_{i+1})<br>$$<br>则称$P(\boldsymbol{Y}|\boldsymbol{X})$为线性链条件随机场，可以看出，$Y_i$<strong>只与观测序列当前值和前后状态相关</strong>。在标注问题中，$\boldsymbol{Y}$是输出变量，表示标记序列，也可以称为状态序列，$\boldsymbol{X}$是输入变量，表示需要标注的观测序列。</p><p>​        这里引用博文<a href="https://blog.csdn.net/zhoubl668/article/details/7786290" target="_blank" rel="noopener">随机场</a>中的一个例子：</p><pre class="line-numbers language-txt"><code class="language-txt">随机场包含两个要素：位置（site），相空间（phase space）。当给每一个位置中按照某种分布随机赋予相空间的一个值之后，其全体就叫做随机场。我们不妨拿种地来打个比方。“位置”好比是一亩亩农田； “相空间”好比是种的各种庄稼。我们可以给不同的地种上不同的庄稼，这就好比给随机场的每个“位置”，赋予相空间里不同的值。所以，俗气点说，随机场就是在哪块地里种什么庄稼的事情。好了，明白了上面两点，就可以讲马尔可夫随机场了。还是拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。马尔可夫随机场，描述了具有某种特性的集合。拿种地打比方，如果任何一块地里种的庄稼的种类仅仅与它邻近的地里种的庄稼的种类有关，与其它地方的庄稼的种类无关，那么这些地里种的庄稼的集合，就是一个马尔可夫随机场。<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="2-2-条件随机场的参数化形式"><a href="#2-2-条件随机场的参数化形式" class="headerlink" title="2.2 条件随机场的参数化形式"></a>2.2 条件随机场的参数化形式</h2><p>​        根据前面介绍的概率无向图模型的因子分解式，线性链条件随机场的各因子是定义在相邻两个节点上的函数。设$P(\boldsymbol{Y}|\boldsymbol{X})$为线性链条件随机场，则在随机变量$\boldsymbol{X}$取值为$x$的条件下，随机变量$\boldsymbol{Y}$取值为$y$的条件概率具有如下形式：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\prod \limits_Q\psi_Q(x,y)\<br>&amp;=\frac{1}{Z(x)}\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}<br>\end{aligned}<br>$$<br>其中，<br>$$<br>Z(x)=\sum\limits_y\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}<br>$$<br>$t_k$和$s_l$是特征函数，$\lambda_k$和$\mu_l$是对应的权值，$Z(x)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p><strong>推导过程</strong>（一定要看下面的过程，可以帮助你很好理解）：</p><p>上文介绍的概率无向图模型只有$X$一种随机变量，现在图中有$X,Y$两种随机变量，所以$P(X)=\frac{1}{Z}\prod\limits_Q\psi_Q(X_Q)$变成：<br>$$<br>\begin{aligned}<br>P(X,Y)&amp;=\frac{1}{Z}\prod\limits_Q\psi_Q(X_Q,Y_Q)\<br>&amp;=\frac{\prod\limits_Q\psi_Q(X_Q,Y_Q)}{\sum\limits_{X,Y}\prod\limits_Q\psi_Q(X_Q,Y_Q)}\<br>则：\<br>P(x,y)&amp;=\frac{\prod\limits_Q\psi_Q(x_Q,y_Q)}{\sum\limits_{x,y}\prod\limits_Q\psi_Q(x_Q,y_Q)}<br>\end{aligned}<br>$$</p><p>令$Z(x)=\sum\limits_y\prod\limits_Q\psi_Q(x_Q,y_Q)$，在归一化的时候，只需对每种$x$求一个归一化因子<br>$$<br>P(y|x)=\frac{1}{Z(x)}\prod \limits_Q\psi_Q(x_Q,y_Q)<br>$$<br>根据线性链条件随机场的定义，对于每一个因子，有两种情况：</p><ol><li>因子处于两个输出序列上，和当前位置的观测，以及前一个状态有关，还要对每一个对应的特征求连积$\psi_Q(x_Q,y_Q)=\psi_Q(y_{i-1},y_i,x,i)=\exp(\lambda_1t_1(y_{i-1},y_i,x,i))\exp(\lambda_2t_2(y_{i-1},y_i,x,i))···\exp(\lambda_kt_k(y_{i-1},y_i,x,i))=\exp(\sum\limits_k\lambda_kt_k(y_{i-1},y_i,x,i))$</li></ol><p><img src="https://img-blog.csdnimg.cn/2020032316552818.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ol start="2"><li><p>因子处于观测序列和输出序列之间，和当前位置的观测有关，还要对每一个对应的特征求连积$\psi_Q(x_Q,y_Q)=\psi_Q(y_i,x,i)=\exp(\mu_1s_1(y_i,x,i))\exp(\mu_2s_2(y_i,x,i))···\exp(\mu_ls_l(y_i,x,i))=\exp(\sum\limits_l\mu_ls_l((y_i,x,i))$</p><p><img src="https://img-blog.csdnimg.cn/20200323165541835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p></li></ol><p>则：$\psi_Q(x_Q,y_Q)=\exp(\sum\limits_k\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_l\mu_ls_l((y_i,x,i))$</p><p>对每一个因子求连积，即对每一个位置$i$求乘积，对指数来说是求和：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\prod \limits_Q\psi_Q(x_Q,y_Q)\<br>&amp;=\frac{1}{Z(x)}\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}\<br>&amp;=\frac{\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}}{\sum\limits_y\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}}<br>\end{aligned}<br>$$<br>定义特征的原因：根据<strong>最大熵原则</strong>，希望找到一个符合所有边缘分布，同时使得熵达到最大的模型，这个模型就是指数函数，每一个边缘分布对应指数模型中的一个特征。最大熵会在后序博文给出。</p><ul><li>$t_k$是定义在边上的特征函数，称为转移特征，依赖于当前位置和前一个位置，是人为定义的特征；</li><li>$s_l$是定义在边上的特征函数，称为状态特征，依赖于当前位置，是人为定义的特征；</li><li>$t_k,s_l$是都依赖于位置，是局部特征函数，通常取值为1或0；</li><li>条件随机场完全由特征函数$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$确定。</li></ul><h2 id="2-3-条件随机场的例子"><a href="#2-3-条件随机场的例子" class="headerlink" title="2.3 条件随机场的例子"></a>2.3 条件随机场的例子</h2><p>​        设有一标注问题：输入观测序列为$\boldsymbol{X}=(X_1,X_2,X_3)$，输出标记序列为$\boldsymbol{Y}=(Y_1,Y_2,Y_3)$，$Y_1,Y_2,Y_3$取值空间为$\{1,2\}$。<strong>==假设==特征函数</strong>$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$如下：<br>$$<br>t_1=t_1(y_{i-1}=1,y_i=2,x,i),\quad i=2,3,\quad \lambda_1=1<br>$$<br>这表示当当前位置$y$取值为2，上一位置为1，则特征函数$t$取值为1，取值为0的条件省略，即：<br>$$<br>t_1(y_{i-1}=1,y_i=2,x,i)=\begin{cases}1,&amp;  y_{i-1}=1,y_i=2,x,i,(i=2,3)\<br>0,&amp; 其他\end{cases}<br>$$<br><img src="https://img-blog.csdnimg.cn/2020032316571777.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一共有5个特征函数$t$，4个特征函数$s$。</p><p>对给定的观测序列$x$，求标记序列为$y=(y_1,y_2,y_3)=(1,2,2)$的非规范化条件概率，也就是没有除以规范化因子的条件概率，即:</p><p>$\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}$  </p><p>解：</p><p>​        根据因子分解式：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;\propto\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}\<br>&amp;\propto \exp{[\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1,y_i,x,i})+\sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i,x,i)}]<br>\end{aligned}<br>$$<br>计算过程：</p><p>当$k=1$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=2}^3t_1(y_{i-1,y_i,x,i})&amp;=t_1(y_1=1,y_2=2,x,i=2)+t_1(y_2=2,y_3=2,x,i=3)\<br>&amp;=1+0=1<br>\end{aligned}<br>$$<br>当$k=2$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=2}^3t_2(y_{i-1,y_i,x,i})&amp;=t_2(y_1=1,y_2=2,x,i=2)+t_2(y_2=2,y_3=2,x,i=3)\<br>&amp;=0+0=0<br>\end{aligned}<br>$$<br>当$k=3$时：<br>$$<br>\begin{aligned}\sum\limits_{i=2}^3t_3(y_{i-1,y_i,x,i})&amp;=t_3(y_1=1,y_2=2,x,i=2)+t_3(y_2=2,y_3=2,x,i=3)\\&amp;=0+0=0\end{aligned}<br>$$<br>当$k=4$时：<br>$$<br>\begin{aligned}\sum\limits_{i=2}^3t_4(y_{i-1,y_i,x,i})&amp;=t_4(y_1=1,y_2=2,x,i=2)+t_4(y_2=2,y_3=2,x,i=3)\\&amp;=0+0=0\end{aligned}<br>$$<br>当$k=5$时：<br>$$<br>\begin{aligned}\sum\limits_{i=2}^3t_5(y_{i-1,y_i,x,i})&amp;=t_5(y_1=1,y_2=2,x,i=2)+t_5(y_2=2,y_3=2,x,i=3)\\&amp;=0+1=1\end{aligned}<br>$$<br>所以：<br>$$<br>\begin{aligned}<br>\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1,y_i,x,i})&amp;=\lambda_1\sum\limits_{i=2}^3t_1(y_{i-1,y_i,x,i})+\lambda_5\sum\limits_{i=2}^3t_5(y_{i-1,y_i,x,i})\<br>&amp;=1\times 1+0.2\times 1\<br>&amp;=1.2<br>\end{aligned}<br>$$<br>同理：</p><p>当$l=1$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_1(y_1=1,x,i=1)+s_1(y_2=2,x,i=2)+s_1(y_3=2,x,i=3)\<br>&amp;=1+0+0\<br>&amp;=1<br>\end{aligned}<br>$$<br>当$l=2$时：<br>$$<br>\begin{aligned}\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_2(y_1=1,x,i=1)+s_2(y_2=2,x,i=2)+s_2(y_3=2,x,i=3)\<br>&amp;=0+1+0\<br>&amp;=1\end{aligned}<br>$$<br>当$l=3$时：<br>$$<br>\begin{aligned}\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_3(y_1=1,x,i=1)+s_3(y_2=2,x,i=2)+s_3(y_3=2,x,i=3)\<br>&amp;=0+0+0\<br>&amp;=0<br>\end{aligned}<br>$$<br>当$l=4$时：<br>$$<br>\begin{aligned}<br>\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=s_4(y_1=1,x,i=1)+s_4(y_2=2,x,i=2)+s_4(y_3=2,x,i=3)\<br>&amp;=0+0+1\<br>&amp;=1<br>\end{aligned}<br>$$<br>所以：<br>$$<br>\begin{aligned}<br>\sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i,x,i)&amp;=\mu_1s_1+\mu_2s_2+\mu_4s_4\<br>&amp;=1\times 1+0.5\times 1+0.5\times 1\<br>&amp;=2<br>\end{aligned}<br>$$<br>故：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;\propto\exp{\big(\sum\limits_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum\limits_{i,l}\mu_ls_l(y_i,x,i)\big)}\<br>&amp;\propto \exp{[\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1,y_i,x,i})+\sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i,x,i)}]\<br>&amp;\propto\exp(1.2+2)=\exp(3.2)<br>\end{aligned}<br>$$</p><h2 id="2-4-条件随机场的简化形式"><a href="#2-4-条件随机场的简化形式" class="headerlink" title="2.4 条件随机场的简化形式"></a>2.4 条件随机场的简化形式</h2><p>​        条件随机场还可以由简化形式表示.注意到条件随机场因子分解式中，同一特征在各个位置都有定义，可以对同一个特征在各个位置求和，将局部特征函数转化为一个全局特征函数，这样就可以将条件随机场写成权值向量和特征向量的内积形式，即条件随机场的简化形式。</p><p>​        为简便起见，首先将转移特征和状态特征及其权值用统一的符号表示，设有$K_1$个转移特征，$K_2$个状态特征，$K＝K_1＋K_2$，记<br>$$<br>f_k(y_{i-1},y_i,x,i)=\begin{cases}t_k(y_{i_1},y_i,x,i),&amp;k=1,2,..,K_1\<br>s_l(y_i,x,i),&amp;k=K_1+l;\quad l=1,2,…,K_2<br>\end{cases}<br>$$<br>然后，对转移与状态特征在各个位置$i$求和，记作：<br>$$<br>f_k(y,x)=\sum\limits_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,…,K<br>$$<br>用$w_k$表示特征$f_k(y,x)$的权值，即：<br>$$<br>w_k=\begin{cases}\lambda_k,&amp;k=1,2,…,K_1\<br>\mu_l,&amp;k=K1+l;\quad l=1,2,…,K_2<br>\end{cases}<br>$$<br>于是，条件随机场就可以表示成：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\<br>Z(x)&amp;=\sum\limits_y\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}<br>\end{aligned}<br>$$<br>若以$\boldsymbol{w}$表示权值向量，即$\boldsymbol{w}=(w_1,w_2,…,w_k)^T$，以$F(\boldsymbol{y},\boldsymbol{x})$来表示全局特征向量，即$F(\boldsymbol{y},\boldsymbol{x})=(f_1(\boldsymbol{y},\boldsymbol{x}),f_2(\boldsymbol{y},\boldsymbol{x}),…,f_k(\boldsymbol{y},\boldsymbol{x}))^T$</p><p>则条件随机场可以写成向量$\boldsymbol{w}$与$F(\boldsymbol{y},\boldsymbol{x})$的内积的形式：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\exp{(\boldsymbol{w}\cdot F(\boldsymbol{y},\boldsymbol{x}))}\<br>Z(x)&amp;=\sum\limits_y\exp{(\boldsymbol{w}\cdot F(\boldsymbol{y},\boldsymbol{x}))}<br>\end{aligned}<br>$$</p><p>符号“$\cdot$”点积表示对应元素相乘后再相加。</p><h2 id="2-5-条件随机场的矩阵形式"><a href="#2-5-条件随机场的矩阵形式" class="headerlink" title="2.5 条件随机场的矩阵形式"></a>2.5 条件随机场的矩阵形式</h2><p>​        条件随机场还可以用矩阵来表示，假设$P_w(\boldsymbol{y}|\boldsymbol{x})$是由<br>$$<br>\begin{aligned}P(y|x)&amp;=\frac{1}{Z(x)}\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\\Z(x)&amp;=\sum\limits_y\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\end{aligned}<br>$$<br>给出的线性链条件随机场，表示对给定观测序列$\boldsymbol{x}$，相应的标记序列$\boldsymbol{y}$的条件概率。引进特殊的起点和终点状态标记$y_0=start,y_{n+1}=stop$，这时$P_w(\boldsymbol{y}|\boldsymbol{x})$可以通过矩阵形式表示。</p><p>​        对观测序列$\boldsymbol{x}$的每一个位置$i=1,2,…,n+1$，定义一个$m$阶矩阵（$m$是标记$\boldsymbol{y}$的取值的个数）：</p><p>$$<br>\begin{aligned}<br>M_i(x)&amp;=[M_i(y_{i-1},y_i|x)]\<br>M_i(y_{i-1},y_i|x)&amp;=\exp(W_i((y_{i-1},y_i|x))\<br>W_i((y_{i-1},y_i|x)&amp;=\sum\limits_{i=1}^Kw_kf_k(y_{i-1},y_i|x)<br>\end{aligned}<br>$$<br>​        这样，给定观测序列$\boldsymbol{x}$，标记序列$\boldsymbol{y}$的非规范化概率可以通过$n+1$个矩阵的乘积$\prod\limits_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$表示，每一次乘就代表了一次状态转移，于是，条件概率$P_w(\boldsymbol{y}|\boldsymbol{x})$是：<br>$$<br>P_w(\boldsymbol{y}|\boldsymbol{x})=\frac{1}{Z_w(x)}\prod\limits_{i=1}^{n+1}M_i(y_{i-1},y_i|x)<br>$$<br>其中，$Z_w(x)$是规范化因子，是$n+1$个矩阵的乘积的(start,stop)元素：<br>$$<br>Z_w(x)=(M_1(x)M_2(x)…M_{n+1}(x))_{start,atop}<br>$$<br>注意，$y_0=start$与$y_{n+1}=stop$表示开始状态与终止状态，规范化因子$Z_w(x)$是以start为起点stop为终点通过状态的所有路径$y_1y_2…y_n$的非规范化概率$\prod\limits_{i=1}^{n+1}M_i(y_{i-1},y_i|x)$之和。下面举个例子来说明。</p><p>例：给定一个由下图所示的线性链条件随机场，观测序列$\boldsymbol{x}$，标记序列$\boldsymbol{y}$，$i=1,2,3,n=3$，标记$y_i\in\{1,2\}$，假设$y_0=start=1,y_4=stop=1$，各个位置的随机矩阵为$M_1(x),M_2(x),M_3(x),M_4(x)$分别是：<br>$$<br>M_1(x)=\begin{bmatrix}<br>a_{01} &amp; a_{02}\<br>0 &amp; 0<br>\end{bmatrix},<br>M_2(x)=\begin{bmatrix}<br>b_{11} &amp; b_{12}\<br>b_{21} &amp; b_{22}<br>\end{bmatrix},<br>M_3(x)=\begin{bmatrix}<br>c_{11} &amp; c_{12}\<br>c_{21} &amp; c_{22}<br>\end{bmatrix},<br>M_4(x)=\begin{bmatrix}<br>1 &amp; 0\<br>1 &amp; 0<br>\end{bmatrix}<br>$$<br>试求状态序列$\boldsymbol{y}$以start为起点stop为终点所有路径的非规范化概率及规范化因子。</p><p><img src="https://img-blog.csdnimg.cn/20200323165759107.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>解：</p><p>从start到stop一共有8条路径：<br>$$<br>(1,1,1),(1,1,2),(1,2,1),(1,2,2),(2,1,1),(2,1,2)(2,2,1),(2,2,2)<br>$$<br>分别对应的非规范化概率为：<br>$$<br>a_{01}b_{11}c_{11},a_{01}b_{11}c_{12},a_{01}b_{12}c_{21},a_{01}b_{12}c_{22}\<br>a_{02}b_{21}c_{11},a_{02}b_{21}c_{12},a_{02}b_{22}c_{21},a_{02}b_{22}c_{12}<br>$$<br>根据规范化因子公式求解：<br>$$<br>\begin{aligned}Z_w(x)&amp;=(M_1(x)M_2(x)…M_{n+1}(x))_{start,atop}\<br>&amp;=M_1(x)M_2(x)M_3(x)M_4(x)\<br>&amp;=\begin{bmatrix}a_{01}b_{11}+a_{02}b_{21} &amp; a_{01}b_{12}+a_{02}b_{22}\<br>0&amp;0\end{bmatrix}M_3(x)M_4(x)\<br>&amp;=\begin{bmatrix}a_{01}b_{11}c_{11}+a_{02}b_{21}c_{11}+a_{01}b_{12}c_{21}+a_{02}b_{22}c_{21}&amp;a_{01}b_{11}c_{12}+a_{02}b_{21}c_{12}+a_{01}b_{12}c_{22}+a_{02}b_{22}c_{22}\<br>0&amp;0\end{bmatrix}\begin{bmatrix}1&amp;0\\1&amp;0\end{bmatrix}\<br>&amp;=\begin{bmatrix}a_{01}b_{11}c_{11}+a_{02}b_{21}c_{11}+a_{01}b_{12}c_{21}+a_{02}b_{22}c_{21}+a_{01}b_{11}c_{12}+a_{02}b_{21}c_{12}+a_{01}b_{12}c_{22}+a_{02}b_{22}c_{22}&amp;0\<br>0&amp;0\end{bmatrix}<br>\end{aligned}<br>$$<br>其第一行第一列的元素为<br>$$<br>a_{01}b_{11}c_{11}+a_{02}b_{21}c_{11}+a_{01}b_{12}c_{21}+a_{02}b_{22}c_{21}+a_{01}b_{11}c_{12}+a_{02}b_{21}c_{12}+a_{01}b_{12}c_{22}+a_{02}b_{22}c_{22}<br>$$<br>恰好是从start到stop的所有路径的非规范概率之和，即$Z(x)$。从这个例子可以看出，要计算$Z(x)$需要考虑到所有可能的路径，当序列长度增加，状态数量增加，计算很困难。</p><h2 id="2-5-条件随机场的三个问题"><a href="#2-5-条件随机场的三个问题" class="headerlink" title="2.5 条件随机场的三个问题"></a>2.5 条件随机场的三个问题</h2><p>类似HMM模型，条件随机场也有三个问题</p><ul><li>概率计算问题</li><li>学习问题</li><li>预测问题</li></ul><h1 id="3-条件随机场的概率计算问题"><a href="#3-条件随机场的概率计算问题" class="headerlink" title="3 条件随机场的概率计算问题"></a>3 条件随机场的概率计算问题</h1><p>​        <strong>条件随机场的概率计算问题是给定条件随机场$P(Y|X)$，输入序列$\boldsymbol{x}$和输出序列$\boldsymbol{y}$，计算条件概率$P(Y_i=y_i|x),P(Y_{i-1}=y_{i-1},Y_i=y_i|x)$以及相应的数学期望的问题</strong>。为了方便起见，像HMM模型那样，引进前向-后向向量，递归的计算以上概率及期望值，这个算法被称作前向-后向算法。这里的前向-后向算法和HMM模型很像，具体参考<a href="https://blog.csdn.net/Elenstone/article/details/104902120" target="_blank" rel="noopener">一文读懂NLP之隐马尔科夫模型</a>。</p><h2 id="3-1-前向-后向算法"><a href="#3-1-前向-后向算法" class="headerlink" title="3.1 前向-后向算法"></a>3.1 前向-后向算法</h2><p>​        对每个指标$i=0,1,2,…,n+1$，定义前向向量$\alpha_i(x)$:<br>$$<br>\alpha_0(y|x)=\begin{cases}1,&amp;y=start\<br>0,&amp;否则<br>\end{cases}<br>$$<br>​        递推公式：<br>$$<br>\alpha_i^T(y_i|x)=\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x),\quad i=1,2,3,…,n+1<br>$$<br>​        也可以表示为：<br>$$<br>\alpha_i^T(x)=\alpha_{i-1}^T(x)M_i(x)<br>$$<br>$\alpha_i(y_i|x)$表示在位置$i$的标记是$y_i$并且到位置$i$的<strong>前部分标记序列的非规范化概率</strong>（只是一个有概率意义的数值，不是真正意义的概率），$y_i$可取的值有$m$个，所以$\alpha_i(y_i|x)$是m维列向量。</p><p>​        同样，对每个指标$i=0,1,2,…,n+1$，定义后向向量$\beta_i(x)$:<br>$$<br>\beta_{n+1}(y_{n+1}|x)=\begin{cases}1,&amp;y_{n+1}=stop\<br>0,&amp;否则\end{cases}\<br>\beta_i(y_i|x)=M_i(y_i,y_{i+1}|x)\beta_{i-1}(y_{i+1}|x)<br>$$<br>​        又可以表示为：<br>$$<br>\beta_i(x)=M_{i+1}(x)\beta_{i+1}(x)<br>$$<br>$\beta_i(y_i|x)$表示在位置$i$的标记为$y_i$并且从$i+1$到$n$的后部分标记序列的非规范化概率。</p><p>​        由前向-后向向量定义可以得到：<br>$$<br>Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}=\boldsymbol{1}^T\cdot \beta_1(x)<br>$$<br>​        这里，$\boldsymbol{1}$是元素均为1的m维列向量。</p><h2 id="3-2-概率计算"><a href="#3-2-概率计算" class="headerlink" title="3.2 概率计算"></a>3.2 概率计算</h2><p>​            按照前向-后向向量的定义，很容易计算标记序列在位置$i$是标记$y_i$的条件概率和在位置$i-1$与$i$是标记$y_{i-1}$和$y_i$的条件概率:<br>$$<br>\begin{aligned}<br>P(Y_i=y_i|x)&amp;=\frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)}\<br>P(Y_{i-1}=y_{i-1},Y_i=y_i|x)&amp;=\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}<br>\end{aligned}<br>$$<br>其中，$Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}$</p><h2 id="3-3-期望值的计算"><a href="#3-3-期望值的计算" class="headerlink" title="3.3 期望值的计算"></a>3.3 期望值的计算</h2><p>​        利用前向-后向向量，可以计算特征函数关于联合分布$P(X,Y)$和条件分布$P(Y|X)$的数学期望。</p><p>​        特征函数$f_k$关于条件分布$P(Y|X)$的数学期望是：<br>$$<br>\begin{aligned}<br>E_{P(Y|X)}[f_k]&amp;=\sum\limits_yP(y|x)f_k(y,x)\<br>&amp;=\sum\limits_{i=1}^{n+1}\sum\limits_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)},\quad k=1,2,…,K<br>\end{aligned}<br>$$<br>其中，$Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}$，从公式来理解，对特征函数求期望，就是对图中的每条边求期望，而一条边的出现跟这条边上的两个节点有关，对应特征$f_k(y_{i-1},y_i,x,i)$第$i$时刻的某条边，出现这条边的概率为$P(y_{i-1},y_1|x)$。</p><p>​        假设经验分布为$\tilde P(X)$，特征函数$f_k$关于联合分布$P(X,Y)$的数学期望是：<br>$$<br>\begin{aligned}<br>E_{P(X,Y)}[f_k]&amp;=\sum\limits_{x,y}P(x,y)\sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\<br>&amp;=\sum\limits_x\tilde P(x)\sum\limits_yP(y|x)\sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)\<br>&amp;=\sum\limits_x\tilde P(x)\sum\limits_{i=1}^{n+1}\sum\limits_{y_{i-1}y_i}f_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)},\quad k=1,2,…,K<br>\end{aligned}<br>$$<br>其中，$Z(x)=\alpha_n^T(x)\cdot \boldsymbol{1}$。</p><p>​        上两个式子是特征函数数学期望的一般计算公式。对于转移特征$t_k$和状态特征$s_l$，可以将$f_k$换成$t_k,s_l$求相应的特征函数期望。</p><h1 id="4-条件随机场的学习算法"><a href="#4-条件随机场的学习算法" class="headerlink" title="4. 条件随机场的学习算法"></a>4. 条件随机场的学习算法</h1><p>​        条件随机场的学习算法：给定训练数据集，估计条件随机场模型参数的问题，即条件随机场的学习问题。条件随机场模型实际上是定义在时序数据上的对数线性模型，其学习方法包括极大似然估计和正则化的极大似然估计。具体的优化实现算法有改进的迭代尺度法IIS、梯度下降法以及拟牛顿法。</p><h2 id="4-1-改进的迭代尺度法"><a href="#4-1-改进的迭代尺度法" class="headerlink" title="4.1 改进的迭代尺度法"></a>4.1 改进的迭代尺度法</h2><p>​        已知训练数据集，由此可知经验概率分布$\tilde P(X,Y)$，可以通过极大化训练数据的对数似然函数来求模型参数。</p><p>​        训练数据的对数似然函数为：<br>$$<br>L(w)=L_{\tilde P}(P_w)=\log{\prod \limits_{x,y}P_w(y|x)^{\tilde P(x,y)}}=\sum\limits_{x,y}\tilde P(x,y)\log{P_w(y|x)}<br>$$<br>当$P_w$是下列式子：<br>$$<br>\begin{aligned}<br>P(y|x)&amp;=\frac{1}{Z(x)}\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}\<br>Z(x)&amp;=\sum\limits_y\exp{\sum\limits_{k=1}^Kw_kf_k(y,x)}<br>\end{aligned}<br>$$<br>给出的条件随机场模型时，对数似然函数为：<br>$$<br>\begin{aligned}<br>L(w)&amp;=\sum\limits_{x,y}\tilde P(x,y)\log{P_w(y|x)}\<br>&amp;=\sum\limits_{x,y}[\tilde P(x,y)\sum\limits_{k=1}^Kw_kf_k(y,x)-\tilde P(x,y)\log{Z_w(x)}]\<br>&amp;=\sum\limits_{j=1}^N\sum\limits_{k=1}^Kw_kf_k(y_j,x_j)-\sum\limits_{j=1}^NZ_w(x_j)<br>\end{aligned}<br>$$<br>​        改进的迭代尺度法通过迭代的方法不断优化对数似然函数改变量的下界，达到极大化对数似然函数的目的。假设模型的当前参数向量为$\boldsymbol{w}＝(w_1,w_2,…,w_K)^T$，向量的增量为$\boldsymbol{\delta}＝(\delta)_1,\delta_2,…,\delta_K)^T$，更新参数向量$\boldsymbol{w}＋\boldsymbol{\delta}＝(w_1+\delta_1,w_2+\delta_2,..,w_K+\delta_K)$。在每步迭代过程中，改进的代尺度法通过依次求解下列式子$E_{\tilde P}[t_k]$和$E_{\tilde P}[s_l]$得到$\boldsymbol{\delta}=(\delta_1,\delta_2,…,\delta_K)^T$。</p><p>​        关于转移特征t的更新方程为：<br>$$<br>\begin{aligned}<br>E_{\tilde P}[t_k]&amp;=\sum\limits_{x,y}\tilde P(x,y)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\<br>&amp;=\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_kT(x,y))},\quad k=1,2,…,K_1<br>\end{aligned}<br>$$<br>​        关于状态特征s的更新方程为:<br>$$<br>\begin{aligned}<br>E_{\tilde P}[s_l]&amp;=\sum\limits_{x,y}\tilde P(x,y)\sum\limits_{i=1}^{n+1}s_l(y_i,x,i)\<br>&amp;=\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}s_l(y_i,x,i)\exp{(\delta_{K_1+l}T(x,y))},\quad k=1,2,…,K_2<br>\end{aligned}<br>$$<br>这里，$T(x,y)$是在数据$(x,y)$中出现的所有特征数的总和：<br>$$<br>T(x,y)=\sum\limits_kf_k(y,x)=\sum\limits_{k=1}^K\sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x,i)<br>$$<br>算法过程：</p><ul><li>输入：特征函数$t_1,t_2,…,t_{K_1},s_1,s_2,…,s_{K_2}$；经验分布$\tilde P(x,y)$;</li><li>输出：餐数据及值$\hat{w}$；模型$P_{\hat{w}}$。</li></ul><ol><li><p>对所有$k\in{1,2,…,K}$，取初值$w_k=0$</p></li><li><p>对每一$k\in{1,2,…,K}$：</p><p>(a). 当$k=1,2,…,K_{1}$时，令$\delta_k$是方程<br>$$<br>\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_kT(x,y))}=E_{\tilde P}[t_k]<br>$$<br>的解</p><p>​        当$k=K_1+l,l=1,2,…,K_2$时，令$\delta_{k+l}$是方程<br>$$<br>\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}s_l(y_i,x,i)\exp{(\delta_{K_1+l}T(x,y))}=E_{\tilde P}[s_l]<br>$$<br>(b). 更新$w_k$值：$w_k\leftarrow w_k+\delta_k$</p></li><li><p>如果不是所有$w_k$都收敛，重复步骤2.</p><p>上式中，$T(x,y)$表示数据$(x,y)$中的特征总数，对不同的数据$(x,y)$取值可能不同，为了处理这个问题，定义松弛特征$s(x,y)=S-\sum\limits_{i=1}^{n+1}\sum\limits_{k=1}^Kf_k(y_{i-1},y_i,x,i)$，其中S是一个常数，选择足够大的常数S使得对训练数据集的所有数据$(x,y)$，$s(x,y)\geq 0$成立，这是特征总数可取S。</p><p>因此，对于转移特征$t_k,\delta_k$的更新方程是：<br>$$<br>\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_{K}S)}=E_{\tilde P}[t_k]\<br>\delta_k=\frac{1}{S}\log\frac{E_{\tilde P}[t_k]}{E_P[t_k]}<br>$$<br>其中，<br>$$<br>E_P(t_k)=\sum\limits_x\tilde P(x)\sum\limits_{i=1}^{n+1}\sum\limits_{y_{i-1}y_i}t_k(y_{i-1},y_i,x,i)\frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}<br>$$<br>同样，对于状态特征$s_l,\delta_k$的更新方程是：</p></li></ol><pre><code>$$\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}s_l(y_{i-1},y_i,x,i)\exp{(\delta_{K+l}S)}=E_{\tilde P}[s_l]\\\delta_{k+l}=\frac{1}{S}\log\frac{E_{\tilde P}[s_l]}{E_P[s_l]}$$其中，$$E_P(s_l)=\sum\limits_x\tilde P(x)\sum\limits_{i=1}^{n}\sum\limits_{y_i}s_l(y_i,x,i)\frac{\alpha_{i}^T(y_{i}|x)\beta_i(y_i|x)}{Z(x)}$$以上算法称为算法S，在算法S中需要是常数S取足够大，这样一来，每部迭代的增量向量会变大，算法收敛会变慢。算法T视图解决这个问题，算法T对每个观测序列$x$计算其特征总数最大值$T(x)$:$$T(x)=\max\limits_yT(x,y)$$利用前向-后向递推公式，可以很容易地计算$T(x)=t$这时关于转移特征参数的更新方程可以写成:$$\begin{aligned}E_{\tilde P}[t_k]&amp;=\sum\limits_{x,y}\tilde P(x)P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_{K}T(x))}\\&amp;=\sum\limits_{x}\tilde P(x)\sum\limits_{y}P(y|x)\sum\limits_{i=1}^{n+1}t_k(y_{i-1},y_i,x,i)\exp{(\delta_{K}T(x))}\\&amp;=\sum\limits_{x}\tilde P(x)a_{k,t}\exp(\delta_k\cdot t)\\&amp;=\sum\limits_{t=0}^{\max}a_{k,t}\beta_k^t\end{aligned}$$这里，$a_{k,t}$是特征$t_k$的期待值，$\delta_k=\log{\beta_k},\beta_k$是上述多项式方程唯一的实根，可以用牛顿法求得，从而得到相关的$\delta_k$。同样，可以用于关于状态特征的参数更新。</code></pre><h2 id="4-2-拟牛顿法"><a href="#4-2-拟牛顿法" class="headerlink" title="4.2 拟牛顿法"></a>4.2 拟牛顿法</h2><p>​        条件随机场模型学习还可以应用牛顿法或拟牛顿法。对于条件随机场模型<br>$$<br>P_w(y|x)=\frac{\exp{(\sum\limits_{i=1}^nw_if_i(x,y))}}{\sum\limits_y\exp{(\sum\limits_{i=1}^nw_if_i(x,y))}}<br>$$<br>学习的优化目标函数是：<br>$$<br>\min\limits_{w\in R^n}f(w)=\sum\limits_x\tilde P(x)\log \sum\limits_yexp(\sum\limits_{i=1}^nw_if_i(x,y))<br>$$<br>其梯度函数是：<br>$$<br>g(w)=\sum\limits_{x,y}\tilde P(x)P_w(y|x)f(x,y)-E_{\tilde P}(f)<br>$$<br>拟牛顿法的BFGS算法如下：</p><p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-cNyvgEa2-1584953186799)(F:\Markdown\image\image-20200323164004538.png)]</p><p>​        条件随机场的预测问题是给定条件随机场$P(Y|X)$和输入序列（观测序列）$x$，求条件概率最大的输出序列（标记序列）$y^<em>$，即对观测序列进行标注。条件随机场的预测算法是注明的维特比算法，参考<a href="https://blog.csdn.net/Elenstone/article/details/104902120" target="_blank" rel="noopener">一文读懂NLP之隐马尔科夫模型</a>。<br>$$<br>\begin{aligned}<br>y^</em>&amp;=\arg\max_yP(y|x)\<br>&amp;=\arg\max_y\frac{\exp(w\cdot F(y,x))}{Z_w(x)}\<br>&amp;=\arg\max_y\exp(w\cdot F(y,x))\<br>&amp;=\arg\max_y(w\cdot F(y,x))<br>\end{aligned}<br>$$<br>​        于是，条件随机场的预测问题称为求非规范化概率最大的最优路径问题：$\max\limits_y(w\cdot F(y,x))$，这里，路径表示标记序列。其中，<br>$$<br>\begin{aligned}<br>\boldsymbol{w}&amp;=(w_1,w_2,…,w_K)^T\<br>F(y,x)&amp;=(f_1(y,x),f_2(y,x),…,f_K(y,x))^T\<br>f_k(y,x)&amp;=\sum\limits_{i=1}^nf_k(y_{i-1},y_i,x,i),\quad k=1,2,…,K<br>\end{aligned}<br>$$<br>注意，这时只需计算飞归发话概率，而不必计算概率，可以大大提高效率。为了求解最优路径，可以将$\max\limits_y(w\cdot F(y,x))$写成以下形式：<br>$$<br>\max\limits_y(w\cdot F_i(y_{i-1},y_i,x))<br>$$<br>其中，$F_i(y_{i-1},y_i,x)=(f_1(y_{i-1},y_i,x,i),f_2(y_{i-1},y_i,x,i),…,f_K(y_{i-1},y_i,x,i))^T$是局部特征向量。</p><h2 id="5-1-维特比算法"><a href="#5-1-维特比算法" class="headerlink" title="5.1 维特比算法"></a>5.1 维特比算法</h2><p>首先求出位置1的各个标记$j=1,2,…,m$的非规范化概率：<br>$$<br>\delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x),\quad j=1,2,…m<br>$$<br>一般的，由递推公式，求出到位置$i$的各个标记$l=1,2,…,m$的非规范化概率的最大值，同时记录非规范化概率最大值的路径<br>$$<br>\delta_i(l)=\max\limits_{1\leq j\leq m}\{\delta_{i-1}(j)+w\cdot F_i(y_{i-1}=j,y_i=l,x)\},\quad l=1,2,…,m\<br>\Psi_i(l)=\arg\max\limits_{1\leq j\leq m}\{\delta_{i-1}(j)+w\cdot F_i(y_{i_1}=j,y_i=l,x)\},\quad l=1,2,…,m<br>$$<br>知道$i=n$时终止，这时求得非规范化概率的最大值为$\max\limits_y(w\cdot F(y,x))=\max\limits_{1\leq j\leq m}\delta_n(j)$及最优路径的终点$y^<em>=\arg\max\limits_{1\leq j\leq m}\delta_n(j)$，再由此最优路径终点依次往前回溯$y_i^</em>=\Psi_{i+1}(y_{i+1}^<em>),i=n-1,n-2,…,1$求得最优路径$y^</em>=(y_1^<em>,y_2^</em>,…,y_n^*)^T$。以上就是维特比算法的思路流程。</p><p><img src="https://img-blog.csdnimg.cn/20200323165909892.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="5-2-维特比算法举例"><a href="#5-2-维特比算法举例" class="headerlink" title="5.2 维特比算法举例"></a>5.2 维特比算法举例</h2><p>​        设有一标注问题：输入观测序列为$\boldsymbol{X}=(X_1,X_2,X_3)$，输出标记序列为$\boldsymbol{Y}=(Y_1,Y_2,Y_3)$，$Y_1,Y_2,Y_3$取值空间为$\{1,2\}$。假设特征函数$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$如下：<br>$$<br>t_1=t_1(y_{i-1}=1,y_i=2,x,i),\quad i=2,3,\quad \lambda_1=1<br>$$<br>这表示当当前位置$y$取值为2，上一位置为1，则特征函数$t$取值为1，取值为0的条件省略，即：<br>$$<br>t_1(y_{i-1}=1,y_i=2,x,i)=\begin{cases}1,&amp;  y_{i-1}=1,y_i=2,x,i,(i=2,3)\<br>0,&amp; 其他\end{cases}<br>$$<br><img src="https://img-blog.csdnimg.cn/20200323165949191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一共有5个特征函数$t$，4个特征函数$s$。</p><p>用i特比算法求给定输入序列$x$对应的最有输出序列$y^<em>=(y_1^</em>,y_2^<em>,y_3^</em>)$</p><p>求解最优路径，即求解$\max\sum\limits_{i=1}^3w\cdot F_i(y_{i-1},y_i,x)$</p><ol><li>初始化</li></ol><p>$$<br>\delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x),\quad j=1,2<br>$$</p><p>64422.png” style=”zoom:75%;” /&gt;</p><h2 id="5-2-维特比算法举例-1"><a href="#5-2-维特比算法举例-1" class="headerlink" title="5.2 维特比算法举例"></a>5.2 维特比算法举例</h2><p>​        设有一标注问题：输入观测序列为$\boldsymbol{X}=(X_1,X_2,X_3)$，输出标记序列为$\boldsymbol{Y}=(Y_1,Y_2,Y_3)$，$Y_1,Y_2,Y_3$取值空间为$\{1,2\}$。假设特征函数$t_k,s_l$和对应的权值$\lambda_k$，$\mu_l$如下：<br>$$<br>t_1=t_1(y_{i-1}=1,y_i=2,x,i),\quad i=2,3,\quad \lambda_1=1<br>$$<br>这表示当当前位置$y$取值为2，上一位置为1，则特征函数$t$取值为1，取值为0的条件省略，即：<br>$$<br>t_1(y_{i-1}=1,y_i=2,x,i)=\begin{cases}1,&amp;  y_{i-1}=1,y_i=2,x,i,(i=2,3)\<br>0,&amp; 其他\end{cases}<br>$$<br><img src="https://img-blog.csdnimg.cn/20200323170047742.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>一共有5个特征函数$t$，4个特征函数$s$。</p><p>用i特比算法求给定输入序列$x$对应的最有输出序列$y^<em>=(y_1^</em>,y_2^<em>,y_3^</em>)$</p><p>求解最优路径，即求解$\max\sum\limits_{i=1}^3w\cdot F_i(y_{i-1},y_i,x)$</p><ol><li>初始化</li></ol><p>$$<br>\delta_1(j)=w\cdot F_1(y_0=start,y_1=j,x),\quad j=1,2<br>$$</p><p><img src="https://img-blog.csdnimg.cn/2020032317003261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>代码实现见我的<a href="https://github.com/CodingMarathon/All_Algorithm/tree/master/CRF" target="_blank" rel="noopener">Github</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>HMM详解</title>
      <link href="/2020/03/26/hmm/"/>
      <url>/2020/03/26/hmm/</url>
      
        <content type="html"><![CDATA[<h1 id="1-隐马尔科夫模型"><a href="#1-隐马尔科夫模型" class="headerlink" title="1 隐马尔科夫模型"></a>1 隐马尔科夫模型</h1><p>本文主要介绍NLP领域中很重要的一个模型——<strong>隐马尔科夫模型（Hidden Markov Model，HMM）</strong>。希望读完本文后，大家能够清楚地理解<strong>HMM</strong>并能够应用到实际中。  </p><p>隐马尔科夫模型是结构最简单的<strong>动态贝叶斯网（dynamic Bayesian network，也被称作有向图模型）</strong>，HMM是可以用于标注问题的统计数学模型，描述由隐藏的<strong>马尔科夫链</strong>随机生成观测序列的过程，属于<strong>生成模型</strong>。HMM模型在语音识别、自然语言处理、生物信息、模式识别等领域有广泛的应用。</p><h2 id="1-1-HMM解决的问题"><a href="#1-1-HMM解决的问题" class="headerlink" title="1.1 HMM解决的问题"></a>1.1 HMM解决的问题</h2><p>首先看看什么样的问题可以使用HMM模型解决。</p><p>使用HMM模型来解决的问题一般有两个特征：</p><ul><li>1） 问题是基于序列的，比如时间序列、状态序列。</li><li>2 ）问题中有两类数据，一类序列数据是可以观测到的，即<strong>观测序列</strong>；而另一类数据是不能观察到的，即<strong>隐藏状态序列</strong>，简称<strong>状态序列</strong>。</li></ul><p>如果问题有了这两个特征，那么这个问题一般可以使用HMM模型尝试解决，这样的问题在生活中是很多的。例如，说一句话，发出的声音是观测序列，想表达的意思是隐藏状态序列；写文章的过程可以想象为先在脑海中构思好一个满足语法的词性序列，然后再将每个词性填充为具体的词语。</p><h2 id="1-2-HMM模型的定义"><a href="#1-2-HMM模型的定义" class="headerlink" title="1.2 HMM模型的定义"></a>1.2 HMM模型的定义</h2><p>隐马尔科夫模型是关于时序的概率模型，描述由一个隐藏的<strong>马尔科夫链</strong>随机生成不可观测的状态随机序列，再由各个状态生成一个观测而产生观测随机序列的过程（摘自《统计学习方法》）。隐藏的马尔科夫链随机生成的状态的序列，称为<strong>状态序列（state sequence）</strong>，记作$\boldsymbol{y}$；每个状态生成一个观测，而由此产生的观测的随机序列，称为<strong>观测序列（observation sequence）</strong>，记作$\boldsymbol{x}$。序列的每一个位置又可以看作一个时刻。<br><img src="https://img-blog.csdnimg.cn/20200316182133870.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="HMM模型示意图"></p><center><font size="4"><strong>HMM模型示意图</strong></font></center><h3 id="1-2-1HMM的两个假设"><a href="#1-2-1HMM的两个假设" class="headerlink" title="1.2.1HMM的两个假设"></a>1.2.1HMM的两个假设</h3><p>首先介绍一下马尔科夫假设：每个事件的发生概率只取决于前一个事件。将满足该假设的连续多个事件串联在一起，就构成了<strong>马尔科夫链</strong>。在NLP语境下，可以将事件具象为单词，于是马尔科夫模型就是二元语法。</p><ul><li>第一个假设：将马尔科夫假设作用于状态序列，假设当前状态$y_t$仅仅依赖于前一个状态$y_{t-1}$，连续多个状态构成隐马尔科夫链$\boldsymbol{y}$。数学表达式为：<br>  $$p(y_t|y_{t-1},x_{t-1},y_{t-2},x_{t-2},y_{t-3},x_{t-3},…,y_{1},x_1)=p(y_t|y_{t-1}), t=1,2,3,…,T$$</li></ul><p>有了隐马尔科夫链，如何建立与观测序列$\boldsymbol{x}$的联系呢？HMM做了第二个假设:</p><ul><li>第二个假设：任意时刻的观测$x_t$只依赖于该时刻的状态$y_t$，与其他时刻的状态或观测独立无关。数学表达式为：<br> $$p(x_t|y_T,x_T,y_{t-1},x_{t-1},…,y_{t+1},x_{t+1},y_t,y_{t-1},x_{t-1},…y_1,x_1)=p(x_t|y_t),t=1,2,3,…,T$$</li></ul><h3 id="1-2-2-HMM模型"><a href="#1-2-2-HMM模型" class="headerlink" title="1.2.2 HMM模型"></a>1.2.2 HMM模型</h3><p>设$Q$是所有可能的状态的集合，$V$是所有可能的观测的集合。<br>$$Q=\{q_1,q_2,…,q_N\}，V=\{v_1,v_2,v_3,…,v_N\}$$<br>其中，N是可能的状态数，M是可能的观测数。<br>HMM模型用三元组来表示$\lambda=(\boldsymbol{\pi},A,B)$：</p><ul><li>$\boldsymbol{\pi}$ : 初始状态概率向量</li><li>A：状态转移概率矩阵</li><li>B：发射概率矩阵</li></ul><p><strong>系统怎么从零开始呢？</strong> 观测值是由隐藏状态产生的，所以系统最初应该是生成隐藏状态。</p><p><strong>初始概率向量</strong>指的是系统启动时进入的第一个状态$y_1$成为<strong>初始状态</strong>，$\boldsymbol{y}$有$N$种取值，从$Q$集合中选取一个，即$\boldsymbol{y} \in \{q_1,q_2,…,q_N\}$。由于$y_1$是第一个状态，是一个独立的离散随机变量，可以用$p(y_1|\boldsymbol{\pi})$来描述，$y_1$只由$\boldsymbol{\pi}$来控制，其中$\boldsymbol{\pi}=(\pi_1,\pi_2,\pi_3,…,\pi_N)^T,0\leq\pi_i\leq1,\sum\limits^{N}_{i=1}{\pi_i}$=1。$\boldsymbol{\pi}$是概率分布的参数向量，称为<strong>初始状态概率向量</strong>。给定$\boldsymbol{\pi}$，初始状态$y_1$的取值分布就确定了。以中文分词问题为例，采用{B,M,E,S}标记时，其中B代表一个词的第一个字，M代表词的中间字，E代表词的末尾字，S代表单字成词。$y_1$所有可能的取值及对应的概率如下：</p><p>$$p(y_1=B)=0.7$$</p><p>$$p(y_1=M)=0$$</p><p>$$p(y_1=E)=0$$</p><p>$$p(y_1=s)=0.3$$</p><p>则$\pi=[0.7,0,0,0.3]$，也就是说句子第一个字可能是一个单字或者一个词的首字，不可能是一个词的中间或者尾字。</p><p>$y_1$<strong>确定之后，怎么产生</strong>$x_1$<strong>呢？如何确定</strong>$x_1$<strong>的概率分布呢？</strong></p><p>根据第二个假设：当前观测值$x_1$仅取决于当前的状态$y_1$，对于从$Q$中取出的每一种状态$y_1$，$x_1$都可以从$V$集合中的$M$个值选一个，所以对于每一个$y,x$都是一个独立的离散随机变量，其概率参数对应一个向量，维度为$M$，即$\boldsymbol{x} \in \{v_1,v_2,…,v_N\}$。由于一共有$N$种$y$，所以这些向量构成了一个$N\times M$矩阵，称为<strong>发射概率矩阵</strong>$\boldsymbol{B}$。<br>$$\boldsymbol{B}=[b_{ij}]_{N\times M}=[p(x_t=v_i|y_t=q_j)]_{N\times M}$$</p><p>其中，$p(x_t=v_i|y_t=q_j)$代表t时刻，隐藏状态$y_t$是$q_j$，由这个状态产生的观测值$x_t$等于$v_i$的概率。</p><table><thead><tr><th>状态</th><th>观测值1</th><th>观测值2</th><th>…</th><th>观测值M</th></tr></thead><tbody><tr><td>状态1</td><td></td><td></td><td></td><td></td></tr><tr><td>状态2</td><td></td><td></td><td></td><td></td></tr><tr><td>…</td><td></td><td></td><td></td><td></td></tr><tr><td>状态N</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>发射概率矩阵是一个非常形象的术语：可以将$\boldsymbol{y}$想象成为不同的彩弹枪，$\boldsymbol{x}$为不同颜色的子弹，每把彩弹枪内的颜色子弹比例不一样，导致有的彩弹枪红色子弹较多比较容易发射红色彩弹，一些彩弹枪绿色子弹较多更容易发射绿色彩弹。<br>发射概率在中文分词中也具有实际意义，有些字符构词的位置比较固定，比如一把作为词首的枪，不容易发射出“餮”，因为“餮”一般作为“饕餮”的词尾出现。通过给$p(x_1=餮|y_1=B)$较低的概率，HMM模型可以有效的防止“饕餮”被错误的切分。</p><p>$y_1$<strong>确定之后，如何转移状态到</strong>$y_2$<strong>?乃至</strong>$y_n$<strong>？</strong></p><p>根据HMM模型的第一个假设：$t+1$时刻的状态仅仅取决于$t$时刻的状态。类似发射概率矩阵，对于$t$时刻的每一种状态，$y_{t+1}$是一个离散的随机变量，取值有$N$种。$t$时刻一共可能有$N$种状态，所以从$t$时刻到$t+1$时刻的状态转移矩阵为$N\times N$的方阵，称为<strong>状态转移概率矩阵</strong>$\boldsymbol{A}$:<br>$$\boldsymbol{A}=[a_{ij}]_{N\times N}=[p(y_{t+1}=v_j|y_t=v_i)]_{N\times N}$$</p><p>其中，$p(y_{t+1}=s_j|y_t=s_i)$表示$t$时刻的状态为$v_i$，$t+1$时刻的状态为$v_j$的概率。</p><table><thead><tr><th>当前状态</th><th>下一状态是状态1</th><th>状态2</th><th>…</th><th>状态N</th></tr></thead><tbody><tr><td>状态1</td><td></td><td></td><td></td><td></td></tr><tr><td>状态2</td><td></td><td></td><td></td><td></td></tr><tr><td>…</td><td></td><td></td><td></td><td></td></tr><tr><td>状态N</td><td></td><td></td><td></td><td></td></tr></tbody></table><p>例如，在中文分词中，标签B后面不可能是S，于是给$p(y_{t+1}=S|y_t=B)=0$就可以防止B后面接S的情况出现。</p><p><strong>初始状态概率向量、状态转移概率矩阵与发射概率矩阵</strong>称为HMM模型的三元组$\lambda=(\boldsymbol{\pi},A,B)$，只要三元组确定了，HMM模型就确定了。</p><p>举一个经典的例子：<br>假设有四个盒子，每个盒子里都装有红白两种颜色的球，如下表：</p><table><thead><tr><th>盒子</th><th>1</th><th>2</th><th>3</th><th>4</th></tr></thead><tbody><tr><td>红球数</td><td>5</td><td>3</td><td>6</td><td>8</td></tr><tr><td>白球数</td><td>5</td><td>7</td><td>4</td><td>2</td></tr></tbody></table><p><img src="https://img-blog.csdnimg.cn/20200317083002437.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="盒子示意"><br>按照下面的方法抽球，产生一个球的颜色观察序列：</p><ol><li><p>从4个盒子里以等概率随机选取一个盒子，从这个盒子里随机抽取一个球，记录颜色后，放回；</p></li><li><p>从当前盒子随机转移到下一个盒子，规则是：如果当前盒子是盒子1，那么下一个盒子一定是盒子2，如果当前是盒子2或者3，那么分别以概率0.4和0.6转移到左边或右边的盒子，如果当前盒子是4，那么各以0.5的概率停留在盒子4或者转移到盒子3；</p></li><li><p>确定转移盒子后，再从这个盒子里随机抽取一个球，记录其颜色，放回；</p></li><li><p>重复2,3步骤3次</p><p>一共抽取出来5个球，得到一个球的颜色观察序列：<br>$$\boldsymbol{x}=\{\color{red}红，红，\color{black}白，白，\color{red}红\}$$</p><p>在这个过程中，观察者只能观测到球的颜色序列，观测不到球是从哪个盒子取出的，即观察不到盒子的序列。<br>这个例子中有两个随机序列，一个是盒子的序列（状态序列），一个是球的颜色的观测序列（观测序列），前者是隐藏的，后者是可以观测的。根据所给的条件可以明确状态集合、观测集合、序列长度以及模型的三要素。</p><ul><li>状态集合是$Q=\{盒子1，盒子2，盒子3，盒子4\}，N=4$。</li><li>观测集合是$V=\{\color{red}红，\color{black}白\}$。</li><li>状态序列和观测序列的长度$T=5$，原因是一共观测了5次。</li><li>初始状态概率向量为$\boldsymbol{\pi}=(0.25,0.25,0.25,0.25)^T$，原因是第一次是等概率随机抽取一个盒子。</li><li>状态转移概率矩阵$A=\begin{bmatrix}<br>0 &amp; 1 &amp; 0 &amp; 0 \<br>0.4 &amp; 0 &amp; 0.6 &amp; 0\<br>0 &amp; 0.4 &amp; 0 &amp;0.6\<br>0 &amp; 0 &amp; 0.5 &amp; 0.5<br>\end{bmatrix}$</li><li>发射概率矩阵$B=\begin{bmatrix}<br>0.5 &amp; 0.5 \<br>0.3 &amp; 0.7 \<br>0.6 &amp; 0.4 \<br>0.8 &amp; 0.2 \<br>\end{bmatrix}$</li></ul></li></ol><h2 id="1-3-HMM模型的三个基本问题"><a href="#1-3-HMM模型的三个基本问题" class="headerlink" title="1.3 HMM模型的三个基本问题"></a>1.3 HMM模型的三个基本问题</h2><p>有了HMM模型之后，如何使用模型呢？HMM模型一个可以解决三个问题：</p><ol><li><strong>概率计算问题</strong>：给定模型参数$\lambda=(\boldsymbol{\pi},A,B)$，和一个观测序列$\boldsymbol{x}$,，计算在这个模型参数$\lambda$下，观测序列出现的最大概率，即$p(\boldsymbol{x}|\lambda)$的最大值。</li><li><strong>模型训练问题</strong>：给定训练集$(\boldsymbol{x}^{(i)},\boldsymbol{y}^{(i)})$，估计模型参数$\lambda=(\boldsymbol{\pi},A,B)$，使得在该模型下观测序列概率$p(\boldsymbol{x}|\lambda)$最大，即使用极大似然估计得方法估计参数。</li><li><strong>序列预测问题</strong>：也称为解码问题，已知模型参数$\lambda=(\boldsymbol{\pi},A,B)$，给定观测序列$\boldsymbol{x}$，求最有可能的状态序列$\boldsymbol{y}$，即求$p(\boldsymbol{y}|\boldsymbol{x})$的最大值。</li></ol><h1 id="2-概率计算问题及算法"><a href="#2-概率计算问题及算法" class="headerlink" title="2 概率计算问题及算法"></a>2 概率计算问题及算法</h1><p>概率计算问题，也就是在给定的模型参数三元组的条件生成观测序列的过程。给定模型参数$\lambda=(\boldsymbol{\pi},A,B)$和一个观测序列$\boldsymbol{x}$,，计算在这个模型参数$\lambda$下，观测序列出现的最大概率，即$p(\boldsymbol{x}|\lambda)$的最大值。先介绍概念上可行但计算上不行的<strong>直接计算法（暴力解法）</strong>，然后介绍<strong>前向算法</strong>与<strong>后向算法</strong>。</p><h2 id="2-1-直接计算法"><a href="#2-1-直接计算法" class="headerlink" title="2.1 直接计算法"></a>2.1 直接计算法</h2><p>给定模型参数$\boldsymbol{\lambda}=(\boldsymbol{\pi},A,B)$和一个观测序列$\boldsymbol{x}=\{x_1,x_2,x_3,…x_T\}$，计算观测序列出现的概率$p(\boldsymbol{x}|\lambda)$，最直接的方法就是按照概率公式直接计算，通过列举所有可能的长度为$T$的状态序列$\boldsymbol{y}=\{y_1,y_2,y_3,…,y_T\}$，求各个状态序列$\boldsymbol{y}$与观测序列$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$的联合概率$p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})$,，然后对所有可能的状态序列求和，得到$p(\boldsymbol{x}|\boldsymbol{\lambda})$。<br>状态序列$\boldsymbol{y}=(y_1,y_2,y_3,…,y_T)$发生的概率是：<br>$$p(\boldsymbol{y}|\boldsymbol{\lambda})=\pi_{y_1}a_{y_1y_2}a_{y_2y_3}…a_{y_{T-1}y_T}$$</p><p>对固定的状态序列$\boldsymbol{y}=(y_1,y_2,y_3,…,y_T)$并且观测序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$的概率是：<br>$$p(\boldsymbol{x}|\boldsymbol{y},\boldsymbol{\lambda})=b_{y_1x_1}b_{y_2x_2}b_{y_3x_3}…b_{y_Tx_T}$$</p><p>在给定HMM模型参数$\boldsymbol{\lambda}$的条件下，$\boldsymbol{x},\boldsymbol{y}$同时出现的联合概率为：<br>$$\begin{aligned}p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})&amp;=p(\boldsymbol{x}|\boldsymbol{y},\boldsymbol{\lambda})p(\boldsymbol{y}|\boldsymbol{\lambda})\<br>&amp;=\pi_{y_1}b_{y_1x_1}a_{y_1y_2}b_{y_2x_2}a_{y_2y_3}b_{y_3x_3}…a_{y_{T-1}y_T}b_{y_Tx_T}<br>\end{aligned}$$</p><p>然后，对所有可能的状态序列$\boldsymbol{y}$求和，得到观测序列$\boldsymbol{x}$的概率$p(\boldsymbol{x}|\lambda)$，即：<br>$$\begin{aligned}<br>p(\boldsymbol{x}|\lambda)&amp;=\sum\limits_\boldsymbol{y}p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})\<br>&amp;=\sum\limits_\boldsymbol{y}p(\boldsymbol{x}|\boldsymbol{y},\boldsymbol{\lambda})p(\boldsymbol{y}|\boldsymbol{\lambda})\<br>&amp;=\sum\limits_{y_1,y_2,y_3,…,y_T}\pi_{y_1}b_{y_1x_1}a_{y_1y_2}b_{y_2x_2}a_{y_2y_3}b_{y_3x_3}…a_{y_{T-1}y_T}b_{y_Tx_T}<br>\end{aligned}$$</p><p>利用这个公式计算量很大，复杂度是$O(TN^T)$，在实际中是不可行的。</p><h2 id="2-2-前向算法"><a href="#2-2-前向算法" class="headerlink" title="2.2 前向算法"></a>2.2 前向算法</h2><p>首先需要定义<strong>前向概率</strong>：给定HMM模型$\boldsymbol{\lambda}$，定义时刻$t$部分观测序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$且状态为$q_i$的概率为<strong>前向概率</strong>，记作：<br>$$\alpha_t(i)=p(x_1,x_2,x_3,…,x_t,y_t=q_i|\boldsymbol{\lambda})$$</p><p><strong>前向算法</strong>：</p><ul><li><p>输入：HMM模型参数$\boldsymbol{\lambda}$，观测序列$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$；</p></li><li><p>输出：观测序列概率$p(\boldsymbol{x}|\boldsymbol{\lambda})$。</p><p>算法步骤：</p><ol><li>初始：<br>根据$\boldsymbol{\pi}$生成$t=1$时刻的状态$y_1=q_i$，概率为$\pi_i$，并且根据<strong>发射概率矩阵</strong>$\boldsymbol{B}$由$y_1=q_i$生成$x_1$，概率为$b_{ix_1}$，则：<br>$$\alpha_1(i)=\pi_ib_{ix_1}$$</li></ol></li></ul><ol start="2"><li>递推：<br>当$t=2$时，根据<strong>状态转移概率矩阵</strong>$\boldsymbol{A}$，系统的状态由$y_1=q_j$变为$y_2=q_i$，概率为$a_{ji}$，$p(y_2=q_i,y_1=q_j,x_1|\boldsymbol{\lambda})=\alpha_1(j)a_{ji}$。不管$y_1$为什么状态，都可能转移到状态$y_2=q_i$，所以需要对$y_1$求和:$\sum\limits_{j=1}^N\alpha_1(j)a_{ji}$。根据$\alpha_t(i)$的定义，$t=2$时刻，由状态$y_2=q_i$产生观测值$x_2$的概率为$b_{ix_2}$，所以：<br>$$\alpha_2(i)=[\sum\limits_{j=1}^N\alpha_1(j)a_{ji}]b_{ix_2}$$</li></ol><p>对$t=1,2,3,…T-1$，有：<br>$$\alpha_{t+1}(i)=[\sum\limits^N_{j=1}\alpha_t(j)a_{ji}]b_{ix_{t+1}},\quad i=1,2,3,…N$$<br><img src="https://img-blog.csdnimg.cn/20200318090901998.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="3"><li>终止：<br> 时刻$t=T$：<br> 根据定义可知：$\alpha_T(i)$表示，$t=T$时刻，处于状态$y_T=q_i$，并且观察到的序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$的概率：<br> $$\begin{aligned}<br> \alpha_T(i)&amp;=p(x_1,x_2,x_3,…,x_T,y_T=q_i|\boldsymbol{\lambda})\<br> &amp;=p(\boldsymbol{x},y_T=q_i|\boldsymbol{\lambda})<br> \end{aligned}$$</li></ol><p>所以：</p><p>$$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits^N_{i=1}\alpha_T(i)$$</p><p>前向算法实际是基于“状态序列的路径结构”递推计算$p(\boldsymbol{x}|\boldsymbol{\lambda})$的算法。前向算法高效的关键是其局部计算前向概率，然后利用路径结构将前向概率递推到全局，得到$p(\boldsymbol{x}|\boldsymbol{\lambda})$。在$t=1$时刻计算$\alpha_1(i)$的$N$个值，而当$t\geq2$时，计算每一个\alpha_t(i)的值都会利用前$N$个$\alpha_{t-1}(i)$的值，减少计算量的原因在于每一次计算都直接引用前一个时刻的计算结果，避免了概率的重复计算，复杂度为$O(N<em>N</em>T)$，而不是直接算法的$O(TN^T)$。</p><p>例：上文中的盒子和球模型，状态集合为$Q=\{1,2,3\}$，观测集合为$V=\{红，白\}$，<br>$A=\begin{bmatrix}<br>0.5 &amp; 0.2 &amp; 0.3\<br>0.3 &amp; 0.5 &amp; 0.2\<br>0.2 &amp; 0.3 &amp; 0.5<br>\end{bmatrix}$ ，  $B=\begin{bmatrix}<br>0.5 &amp; 0.5 \<br>0.4 &amp; 0.6\<br>0.7 &amp; 0.3<br>\end{bmatrix}$ ， $\pi=(0.2,0.4,0.4)^T$<br>设$T=3$，$\boldsymbol{x}=(红，白，红)$，用前向算法来计算$p(\boldsymbol{x}|\boldsymbol{\lambda})$。<br>解：</p><ol><li>计算初值：<br> $$\alpha_1(1)=\pi_1b_{1x_1}=0.2\times0.5=0.10$$</li></ol><p>$$\alpha_1(2)=\pi_2b_{2x_1}=0.4\times0.4=0.16$$</p><p>$$\alpha_1(3)=\pi_3b_{3x_1}=0.4\times0.7=0.28$$</p><ol start="2"><li>递推计算：<br> $$\alpha_2(1)=[\sum\limits_{i=1}^3\alpha_1(i)a_{i1}]b_{1x_2}=[0.10\times0.5+0.16\times0.3+0.28\times0.2)]\times0.5=0.077$$</li></ol><p>$$\alpha_2(2)=[\sum\limits_{i=1}^3\alpha_1(i)a_{i2}]b_{2x_2}=[0.10\times0.2+0.16\times0.5+0.28\times0.3)]\times0.5=0.1104$$</p><p>$$\alpha_2(3)=[\sum\limits_{i=1}^3\alpha_1(i)a_{i3}]b_{3x_2}=[0.10\times0.3+0.16\times0.2+0.28\times0.5)]\times0.5=0.0606$$</p><p>$$\alpha_3(1)=[\sum\limits_{i=1}^3\alpha_2(i)a_{i1}]b_{1x_3}=0.04187$$</p><p>$$\alpha_3(2)=[\sum\limits_{i=1}^3\alpha_2(i)a_{i2}]b_{2x_3}=0.03551$$</p><p>$$\alpha_3(3)=[\sum\limits_{i=1}^3\alpha_2(i)a_{i3}]b_{3x_3}=0.05284$$</p><ol start="3"><li>终止： </li></ol><p>$$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{i=1}^3\alpha_3(i)=0.13022$$</p><h2 id="2-3-后向算法"><a href="#2-3-后向算法" class="headerlink" title="2.3 后向算法"></a>2.3 后向算法</h2><p>类似前向算法，定义：给定HMM模型$\boldsymbol{\lambda}$，定义时刻$t$状态为$q_i$的条件下，从$t+1到T$的部分观测序列为$x_{t+1},x_{t+2},x_{t+3},…,x_T$的概率为<strong>后向概率</strong>，记作：<br>$$\beta_t(i)=p(x_{t+1},x_{t+2},x_{t+3},…,x_T|y_t=q_i,\boldsymbol{\lambda})$$<br>后向算法：</p><ul><li><p>输入：HMM模型参数$\boldsymbol{\lambda}$，观测序列$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$；</p></li><li><p>输出：观测序列概率$p(\boldsymbol{x}|\boldsymbol{\lambda})$。</p><ol><li><strong>当$t=T$时</strong>:<br>$$\beta_T(i)=p(这里没东西|y_T=q_i,\boldsymbol{\lambda})=1$$<br>这里可以这么理解：按理说好需要看看T时刻过后是什么观测值，但是T时刻之后没有观测值了，不管T时刻状态是什么，之后就是没有观测值，所以没有观测值的概率为1，且与状态无关。</li></ol></li></ul><ol start="2"><li><strong>对$t=T-1$</strong>:<br>已知$\beta_T(j)$，根据HMM模型可知，$y_T=q_j$是由$y_{T-1}$转移而来的，假设$y_{T-1}=q_i$，转移的概率为$a_{ij}$:<br>根据$\beta_{T-1}(i)$的定义，时刻$T-1$的状态$y_{T-1}=q_i$，从T到T的观测序列为$x_T$，T时刻状态$y_T=q_j$生成$x_T$的概率为$b_{jx_T}$，则：<br>$$\beta_{T-1}(i)=\sum\limits^N_{j=1}a_{ij}b_{jx_T}\beta_T(j)$$<br>对$t=T-1,T-2,…,1$:<br>$$b_{jx_t}=p(x_t|y_t=q_j,\boldsymbol{\lambda}))$$</li></ol><p>$$a_{ij}=p(y_{t+1}=q_j|y_t=q_i,\boldsymbol{\lambda}))$$</p><p>$$\beta_{t+1}(j)=p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})$$</p><p>由状态$y_{t+1}=q_j$生成$t+1$时刻的观测值$x_{t+1}$:<br>$$\begin{aligned}<br>b_{jx_{t+1}}\beta_{t+1}(j)&amp;=p(x_{t+1}|y_{t+1}=q_j,\boldsymbol{\lambda})p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})\<br>&amp;=p(x_{t+1},x_{t+2},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})\end{aligned}$$</p><p>按照条件概率来理解：在$t+1$时刻状态为$y_{t+1}=q_j$的条件下，观察到$x_{t+1},x_{t+2},…,x_T$的概率为$b_{jx_{t+1}}\beta_{t+1}(j)$，由于$\beta_t(i)$与$t$时刻的状态$y_t$有关，根据HMM模型的第一个假设，$y_{t+1}$仅仅与$y_t$有关，且概率$a_{ij}$由状态转移矩阵矩阵A提供。$\beta_t(i)$表示在$t$时刻状态为$y_t=q_i$的条件下，观察到$x_{t+1},x_{t+2},…,x_T$的概率，这个概率若要用$\beta_{t+1}(j)$来表示，针对每一个$y_{t+1}=q_j$，都要乘以从$y_t=q_i$到$y_{t+1}=q_j$的状态转移概率$a_{ij}$，$y_{t+1}$一共有N种状态，所以：<br>$$\begin{aligned}<br>a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)&amp;=p(x_{t+1},x_{t+2},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})a_{ij}\<br>&amp;=p(x_{t+1},x_{t+2},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})p(y_{t+1}=q_j|y_t=q_i,\boldsymbol{\lambda}))\<br>&amp;=p(x_{t+1},x_{t+2},…,x_T,y_{t+1}=q_j,|y_t=q_i,\boldsymbol{\lambda})<br>\end{aligned}$$</p><p>$$\begin{aligned}\beta_t(i)&amp;=p(x_{t+1},x_{t+2},…,x_T|y_t=q_i,\boldsymbol{\lambda})\<br>&amp;=\sum\limits_{y_{t+1}}p(x_{t+1},x_{t+2},…,x_T,y_{t+1}=q_j,|y_t=q_i,\boldsymbol{\lambda})\<br>&amp;=\sum\limits_{j=1}^Na_{ij}b_{jx_{t+1}}\beta_{t+1}(j)<br>\end{aligned}$$<br>所以：<br>$$\beta_t(i)=\sum\limits_{j=1}^Na_{ij}b_{jx_{t+1}}\beta_{t+1}(j),\quad i=1,2,3,…,N$$<br><img src="https://img-blog.csdnimg.cn/20200318090609546.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><ol start="3"><li><strong>当$t=1$时</strong>，$\beta_1(i)=p(x_2,x_3,x_4,…,x_T|y_1=q_i,\boldsymbol{\lambda})$；按照步骤2的思想，针对$t=1$时刻的每一种状态$y_1=q_i$，都需要乘上初始状态概率向量和相应的发射概率矩阵。<br> $$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{i=1}^N\pi_ib_{ix_1}\beta_{t+1}(i)$$</li></ol><h2 id="2-4-一些概率与期望值的计算"><a href="#2-4-一些概率与期望值的计算" class="headerlink" title="2.4 一些概率与期望值的计算"></a>2.4 一些概率与期望值的计算</h2><p>利用前向概率和后向概率，可以得到关于单个状态和两个状态概率的计算公式。</p><ol><li>给定模型$\boldsymbol{\lambda}$和观测$\boldsymbol{x}$，在时刻$t$处于状态$q_i$的概率，记作$\gamma_t(i)$：<br> $$\gamma_t(i)=p(y_t=q_i|\boldsymbol{x},\boldsymbol{\lambda})$$</li></ol><p>可以用过前向和后向算法来算：</p><p>$$\begin{aligned}<br>\gamma_t(i)=p(y_t=q_i|\boldsymbol{x},\boldsymbol{\lambda})=\frac{p(y_t=q_i,\boldsymbol{x}|\boldsymbol{\lambda})}{p(\boldsymbol{x}|\boldsymbol{\lambda})}<br>\end{aligned}$$</p><p>$\alpha_t(i)$定义为时刻$t$部分观测序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_t)$且状态为$q_i$的概率，$\beta_t(i)$定义为时刻$t$状态为$q_i$的条件下，从$t+1到T$的部分观测序列为$x_{t+1},x_{t+2},x_{t+3},…,x_T$的概率，则两者相乘表示，观察序列为$\boldsymbol{x}=(x_1,x_2,x_3,…,x_T)$且状态为$q_i$的概率，数学表示为：<br>$$\alpha_t(i)\beta_t(i)=p(y_t=q_i,\boldsymbol{x}|\boldsymbol{\lambda})$$</p><p>$$p(\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{y_t}p(y_t=q_i,\boldsymbol{x}|\boldsymbol{\lambda})=\sum\limits_{j=1}^N\alpha_t(i)\beta_t(i)$$</p><p>所以：</p><p>$$\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{p(\boldsymbol{x}|\boldsymbol{\lambda})}=\frac{\alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N\alpha_t(i)\beta_t(i)}$$</p><ol start="2"><li>给定HMM模型参数$\boldsymbol{\lambda}$和观测序列$\boldsymbol{x}$，在时刻$t$处于状态$q_i$且在时刻$t+1$处于状态$q_j$的概率，记作$\xi_t(i,j)$：<br> $$\xi_t(i,j)=p(y_t=q_i,y_{t+1}=q_j|\boldsymbol{x},\boldsymbol{\lambda})$$</li></ol><p>使用前向和后向算法来计算：<br>$$\begin{aligned}<br>\xi_t(i,j)&amp;=p(y_t=q_i,y_{t+1}=q_j|\boldsymbol{x},\boldsymbol{\lambda})\<br>&amp;=\frac{p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})}{p(\boldsymbol{x},\boldsymbol{\lambda)}}\<br>&amp;=\frac{p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})}{\sum\limits_{i=1}^N\sum\limits_{j=1}^Np(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})}<br>\end{aligned}$$</p><p>而$p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})$表示在模型参数下，观测序列$\boldsymbol{x}$以及$t$时刻状态为$q_i$且时刻$t+1$处于状态$q_j$的概率，$\alpha_t(i)=p(y_t=q_i,x_1,x_2,x_3,…,x_t|\boldsymbol{\lambda})$表示在模型参数下，观测序列$x_1,x_2,x_3,…,x_t$以及$t$时刻状态为$q_i$的概率，$\beta_{t+1}(j)=p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})$表示在模型参数和$t+1$时刻状态为$q_j$的条件下，观察序列为$x_{t+2},x_{t+3},…,x_T$，两者之间差一个从时刻$t到t+1$的状态转移概率以及时刻$t+1$状态$q_j$产生序列$x_{t+1}$的概率：<br>$$\begin{aligned}<br>p(y_t=q_i,y_{t+1}=q_j,\boldsymbol{x}|\boldsymbol{\lambda})&amp;=p(y_t=q_i,x_1,x_2,…,x_t|\boldsymbol{\lambda})p(y_{t+1}|y_t,\boldsymbol{\lambda})\\&amp;\qquad p(x_{t+1}|y_{t+1},\boldsymbol{\lambda})p(x_{t+2},x_{t+3},…,x_T|y_{t+1}=q_j,\boldsymbol{\lambda})\<br>&amp;=\alpha_t(i)a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)<br>\end{aligned}$$</p><p>所以：<br>$$\xi_t(i,j)=\frac{\alpha_t(i)a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)}{\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_t(i)a_{ij}b_{jx_{t+1}}\beta_{t+1}(j)}$$</p><p>3.将$\xi_t(i,j)$和$\gamma_t(i)$对各个时刻求和，可以得到一些有用的期望值：</p><ul><li>在观测$\boldsymbol{x}$下，状态$q_i$出现的期望值：$\sum\limits_{t=1}^T\gamma_t(i)$</li><li>在观测$\boldsymbol{x}$下,由状态$q_i$转移的期望值：$\sum\limits_{t=1}^{T-1}\gamma_t(i)$</li><li>在观测$\boldsymbol{x}$下,由状态$q_i$转移到状态$q_j$的期望值：$\sum\limits_{t=1}^{T-1}\xi_t(i,j)$</li></ul><h1 id="3-模型训练问题及算法"><a href="#3-模型训练问题及算法" class="headerlink" title="3 模型训练问题及算法"></a>3 模型训练问题及算法</h1><p>给定训练集$(\boldsymbol{x}^{(i)},\boldsymbol{y}^{(i)})$，估计模型参数$\lambda=(\boldsymbol{\pi},A,B)$，使得在该模型下观测序列概率$p(\boldsymbol{x}|\lambda)$最大。根据训练数据是否有状态序列数据分为：完全数据和非完全数据，分别使用监督学习和非监督学习实现。</p><h2 id="3-1-监督学习——最大似然估计"><a href="#3-1-监督学习——最大似然估计" class="headerlink" title="3.1 监督学习——最大似然估计"></a>3.1 监督学习——最大似然估计</h2><p>在监督学习中，我们使用极大似然法来估计HMM模型参数。<br>假设给定训练数据包含S个长度相同的观测序列$\{(\boldsymbol{x}^1,\boldsymbol{y}^1),(\boldsymbol{x}^2,\boldsymbol{y}^2),…,(\boldsymbol{x}^S,\boldsymbol{y}^S)\}$，使用极大似然法来估计HMM模型参数。</p><p><strong>初始状态概率向量的估计</strong>：<br>统计S个样本中，初始状态为$q_i$的频率。<br>$$\hat{\pi}_i=\frac{N_{q_i}}{S}$$<br>其中，$N_{q_i}$是初始状态为$q_i$的样本数量，S是样本的数量。</p><p><strong>状态转移概率矩阵的估计</strong>：<br>设样本中时刻t处于状态$q_i$，时刻t+1处于状态$q_j$的频数为$A_{ij}$，那么状态转移概率矩阵的估计为：<br>$$\hat{a}_{ij}=\frac{A_{ij}}{\sum\limits_{j=1}^NA_{ij}},\quad j=1,2,3,…,N;\quad i=1,2,3,…,N$$</p><p><strong>发射概率矩阵的估计</strong>：<br>设样本中状态为$i$并观测值为$j$的频数$B_{ij}$，那么状态为$i$观测为$j$的概率$b_{ij}$的估计为：<br>$$\hat{b}_{ij}=\frac{B_{ij}}{\sum\limits_{j=1}^MB_{ij}},\quad j=1,2,3,…,M;\quad i=1,2,3,…,N$$</p><p><strong>监督学习的方法就是拿频率来估计概率</strong>。</p><h2 id="3-2-非监督学习——EM算法"><a href="#3-2-非监督学习——EM算法" class="headerlink" title="3.2 非监督学习——EM算法"></a>3.2 非监督学习——EM算法</h2><p>假设给定训练数据只包含S个长度为T的观测序列$\boldsymbol{x}=\{\boldsymbol{x}^1,\boldsymbol{x}^2,,…,\boldsymbol{x}^S\}$而没有对应的状态序列，目标是学习HMM模型的参数$\boldsymbol{\lambda}=(\boldsymbol{\pi},A,B)$。将状态序列看作不可观测的隐数据$\boldsymbol{Y}$，HMM模型事实上是一个含有隐变量的概率模型：<br>$$p(\boldsymbol{X}|\boldsymbol{\lambda})=\sum\limits_\boldsymbol{Y}p(\boldsymbol{X}|\boldsymbol{Y},\boldsymbol{\lambda})p(\boldsymbol{Y}|\boldsymbol{\lambda})$$<br>这个参数可以由<strong>EM算法</strong>实现。</p><ol><li><p><strong>确定数据的对数似然函数</strong><br> 所有观测数据写成$\boldsymbol{x}=\{x_1,x_2,x_3,…,x_T\}$，所有的隐藏状态数据写成$\boldsymbol{y}=\{y_1,y_2,,…,y_T\}$，则完全数据是$(\boldsymbol{x},\boldsymbol{y})=(x_1,x_2,x_3,…,x_T,y_1,y_2,,…,y_T)$，完全数据的对数似然函数是$\log{p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})}$。</p></li><li><p><strong>Em算法的E步</strong>：求Q函数$$\begin{aligned}<br> Q(\boldsymbol{\lambda},\overline{\boldsymbol{\lambda}})&amp;=E_\boldsymbol{y}[\log{p(\boldsymbol{y},\boldsymbol{y}|\boldsymbol{\lambda})}|\boldsymbol{x},\overline{\boldsymbol{\lambda}}]\<br> &amp;=\sum\limits_\boldsymbol{y}\log{p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})}p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}})<br> \end{aligned}$$</p></li></ol><p>其中$\overline{\boldsymbol{\lambda}}$是HMM模型参数的当前估计值，$\boldsymbol{\lambda}$是要极大化的HMM模型参数。<br>$$p(\boldsymbol{x},\boldsymbol{y}|\boldsymbol{\lambda})=\pi_{y_1}b_{y_1x_1}a_{y_1y_2}b_{y_2x_2}\cdot …\cdot a_{y_{T-1}y_T}b_{y_Tx_T}$$<br>所以:<br>$Q(\boldsymbol{\lambda},\overline{\boldsymbol{\lambda}})=\sum\limits_\boldsymbol{y}\log{\pi_{y_1}p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})+\sum\limits_\boldsymbol{y}[\sum\limits^{T-1}_{t=1}\log{a_{y_ty_{t+1}}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})+\sum\limits_\boldsymbol{y}[\sum\limits_{t=1}^T\log{b_{y_tx_t}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})$</p><ol start="3"><li><strong>EM算法的M步</strong>：极大化Q函数求模型的参数<br> <strong>第一项：</strong><br> $$\sum\limits_\boldsymbol{y}\log{\pi_{y_1}p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})=\sum\limits_{i=1}^N\log{\pi_ip(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}$$<br> 注意到$\sum\limits^{N}_{i=1}{\pi_i}=1$，利用拉格朗日乘子法，写出拉格朗日函数：</li></ol><p>$$\sum\limits_{i=1}^N\log{\pi_ip(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}+\gamma(\sum\limits^{N}_{i=1}-1)$$</p><p>求偏导并令其等于0：</p><p>$$\frac{\partial}{\partial \pi_i}[\sum\limits_{i=1}^N\log{\pi_ip(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}+\gamma(\sum\limits^{N}_{i=1}-1)]=0$$</p><p>得到：</p><p>$$p(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})+\gamma\pi_i=0$$</p><p>对i求和：<br>$$\gamma=-p(\boldsymbol{x}|\overline{\boldsymbol{\lambda}})$$</p><p>所以，得到$\pi_i$：</p><p>$$\pi_i=\frac{p(\boldsymbol{x},y_1=q_i|\overline{\boldsymbol{\lambda}})}{p(\boldsymbol{x}|\overline{\boldsymbol{\lambda}})}$$</p><p><strong>第二项：</strong><br>$$\sum\limits_\boldsymbol{y}[\sum\limits^{T-1}_{t=1}\log{a_{y_ty_{t+1}}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})=\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}\log{a_{ij}p(\boldsymbol{x},y_t=q_i,y_{t+1}=q_j|\overline{\boldsymbol{\lambda}})}$$</p><p>类似第一项，应用$\sum\limits_{j=1}^N=1$的拉格朗日乘子法可以求出：<br>$$a_{ij}=\frac{\sum\limits_{t=1}^{T-1}p(\boldsymbol{x},y_t=q_i,y_{t+1}=q_j|\overline{\boldsymbol{\lambda}})}{\sum\limits_{t=1}^{T-1}p(\boldsymbol{x},y_t=q_i|\overline{\boldsymbol{\lambda}})}$$</p><p><strong>第三项：</strong><br>$$\sum\limits_\boldsymbol{y}[\sum\limits_{t=1}^T\log{b_{y_tx_t}]p(\boldsymbol{x},\boldsymbol{y}|\overline{\boldsymbol{\lambda}}})=\sum\limits_{j=1}^N\sum\limits_{t=1}^{T}\log{b_{jx_t}p(\boldsymbol{x},y_t=q_j|\overline{\boldsymbol{\lambda}})}$$</p><p>同样使用拉格朗日乘子法，求得：<br>$$b_{jk}=\frac{\sum\limits_{t=1}^Tp(\boldsymbol{x},y_t=q_j|\overline{\boldsymbol{\lambda}})I(x_t=v_k)}{\sum\limits_{t=1}^Tp(\boldsymbol{x},y_t=q_j|\overline{\boldsymbol{\lambda}})}$$</p><h2 id="3-3-Baum-Welch算法"><a href="#3-3-Baum-Welch算法" class="headerlink" title="3.3 Baum-Welch算法"></a>3.3 Baum-Welch算法</h2><p>将EM算法的参数式子分别用前向和后向概率算出的$\gamma_t(i),\xi_t(i,j)$表示则:<br>$$a_{ij}=\frac{\sum\limits_{t=1}^{T-1}\xi_t(i,j)}{\sum\limits_{t=1}^{T-1}\gamma_t(i)}$$</p><p>$$b_{ij}=\frac{\sum\limits_{t=1,x_t=v_j}^T\gamma_t(i)}{\sum\limits_{t=1}^T\gamma_t(i)}$$</p><p>$$\pi_i=\gamma_1(i)$$</p><h1 id="4-序列预测问题及算法"><a href="#4-序列预测问题及算法" class="headerlink" title="4 序列预测问题及算法"></a>4 序列预测问题及算法</h1><p>序列预测问题就是已知模型参数$\lambda=(\boldsymbol{\pi},A,B)$，给定观测序列$\boldsymbol{x}$，求最有可能的状态序列$\boldsymbol{y}$，即求$p(\boldsymbol{y}|\boldsymbol{x})$的最大值。一般有两种解法：近似解法与维特比解法。</p><h2 id="4-1-近似解法"><a href="#4-1-近似解法" class="headerlink" title="4.1 近似解法"></a>4.1 近似解法</h2><p>近似算法的思想是，在每个时刻$t$选择该时刻最有可能出现的状态$y_t^<em>$，从而得到一个状态序列$\boldsymbol{y}=(y_i^</em>,y_2^<em>,…,y_t^</em>)$，将它作为预测的结果。<br>给定模型$\boldsymbol{\lambda}$和观测$\boldsymbol{x}$，在时刻$t$处于状态$q_i$的概率:<br>$$\gamma_t(i)==\frac{\alpha_t(i)\beta_t(i)}{p(\boldsymbol{x}|\boldsymbol{\lambda})}=\frac{\alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N\alpha_t(i)\beta_t(i)}$$</p><p>在每一个时刻最有可能的状态$y_t^<em>$是：<br>$$y_t^</em>=\arg\max_{1\leq i\leq N}[\gamma_t(i)],\quad t=1,2,3…,T$$<br>从而得到整个序列。<br>近似算法的优点是计算简单，其缺点是<strong>不能保证预测的状态序列整体是最有可能的状态序列</strong>，因为预测的状态序列可能有<strong>实际不发生</strong>的部分。事实上，上述方法得到的状态序列中有可能存在<strong>转移概率为0的相邻状态</strong>。尽管如此，近似算法仍然是有用的。</p><h2 id="4-2-维特比算法（Viterbi-algorithm）"><a href="#4-2-维特比算法（Viterbi-algorithm）" class="headerlink" title="4.2 维特比算法（Viterbi algorithm）"></a>4.2 维特比算法（Viterbi algorithm）</h2><p><strong>维特比算法</strong>实际是用<strong>动态规划</strong>解HMM模型序列预测问题，用动态规划求解概率最大路径（最优路径），一条路径对应一个状态序列。<br>根据图论，假设最优路径为$\boldsymbol{y}^<em>$，其中从起点到时刻$t$的一段最优路径是$(y_1^</em>,y_2^<em>,…,y_t^</em>)$，则这部分路径对于后序最优路径（$y_t^<em>,y_{t+1}^</em>,y_{t+2}^<em>,…,y_T^</em>$）的选取来说一定也是最优的。可以使用反证法来证明：假设存在另一条局部路径$(y_1^,,y_2^,,…,y_t^,)$要优于$(y_1^<em>,y_2^</em>,…,y_t^<em>)$，那么它与（$y_t^</em>,y_{t+1}^<em>,y_{t+2}^</em>,…,y_T^<em>$）拼接起来蝴蝶刀另一条更优的全局最优路径，与定义矛盾。<br>根据HMM模型的第一个假设，$y_{t+1}$仅仅只与$y_t$相关，所以网状图可以动态规划地搜索。<em>*定义</em></em>：二维数组$\delta_t(i)$表示在时刻$t$以$q_j$结尾的所有局部路径的最大概率。从第一步推到第T步，每次递推都在上一次的N条局部路径中挑选，所以复杂度为$O(TN)$。为了得到路径，还需要定义一个二维数组$\psi_t(i)$，记录每个状态的前驱。</p><p>$$\delta_t(i)=\max_{y_1,y_2,…,y_{t-1}}p(y_t=q_i,y_{t-1},…y_1,x_t,x_{t-1},…,x_1|\boldsymbol{\lambda})$$</p><p>$$\psi_t(i)=\arg\max_{1\leq j \leq N}[\delta_{t-1}(j)a_{ji}]$$</p><p>维特比算法：</p><ol><li><strong>初始化</strong>，当$t=1$时，最优路径的备选由N个状态组成，它的前驱为空：<br> $$\delta_1(i)=\pi_ib_{ix_1},\quad i=1,2,3,..,N$$</li></ol><p>$$\psi_1(i)=0,\quad i=1,2,3,..,N$$</p><ol start="2"><li><strong>递推</strong>，当$t\geq 2$时，每条备选的路径像贪吃蛇一样吃入一个状态，长度增加一个单位，根据状态转移概率和发射概率计算<strong>花费</strong>。找出新的局部最优路径，更新两个数组。<br> $$\delta_t(i)=\max_{1\leq j \leq N}[\delta_{t-1}(j)a_{ji}]b_{ix_t},\quad i=1,2,3,..,N$$</li></ol><p>$$\psi_t(i)=\arg\max_{1\leq j \leq N}(\delta_t(j)a_{ji}),\quad i=1,2,3,..,N$$</p><ol start="3"><li><strong>终止</strong>，找出最终时刻$(\delta_t(i)$数组中最大概率$p^<em>$，以及相应的结尾状态下表$y_T^</em>$<br> $$P^*=\max_{1\leq j \leq N}\delta_T(i)$$</li></ol><p>$$y_T^*=\arg\max_{1\leq j \leq N}\delta_T(i)$$</p><ol start="4"><li>回溯，根据前驱数组$\psi_t$回溯前驱状态，取得最优路径状态下标$\boldsymbol{y}^<em>=(y_1^</em>,y_2^<em>,…,y_T^</em>)$。<br> $$y_t^<em>=\psi_{t+1}(y_{t+1}^</em>),\quad t=T-1,T-2,…,1$$</li></ol><p>举个例子：<br>上文中的盒子和球模型，状态集合为$Q=\{1,2,3\}$，观测集合为$V=\{红，白\}$，<br>$A=\begin{bmatrix}<br>0.5 &amp; 0.2 &amp; 0.3\<br>0.3 &amp; 0.5 &amp; 0.2\<br>0.2 &amp; 0.3 &amp; 0.5<br>\end{bmatrix}$ ，  $B=\begin{bmatrix}<br>0.5 &amp; 0.5 \<br>0.4 &amp; 0.6\<br>0.7 &amp; 0.3<br>\end{bmatrix}$ ， $\pi=(0.2,0.4,0.4)^T$<br>设$T=3$，$\boldsymbol{x}=(红，白，红)$，求最优状态序列，即最优路径$\boldsymbol{y}^<em>=(y_1^</em>,y_2^<em>,…,y_T^</em>)$。<br>解：</p><ol><li>初始化，在$t=1$时，对每一个状态$q_i，i=1,2,3$，求状态为$q_i$观测为$x_1$为红的概率，记此概率为$\delta_1(i)$。<br> $$\delta_1(i)=\pi_ib_{ix_1}=\pi_ib_{i红}，\quad i=1,2,3$$</li></ol><p>$$\delta_1(1)=0.2\times0.5=0.1,\delta_1(2)=0.4\times0.4=0.16,\delta_1(3)=0.4\times0.7=0.28$$</p><p>$$\psi_1(i)=0,\quad i=1,2,3$$</p><ol start="2"><li>在$t=2$时，对每个状态$i，i=1,2,3$:<br> $$\delta_2(i)=\max_{1\leq j \leq 3}[\delta_{1}(j)a_{ji}]b_{i白},\quad i=1,2,3,..,N$$</li></ol><p>$$\begin{aligned}<br>\delta_2(1)&amp;=\max_{1\leq j \leq 3}[\delta_{1}(j)a_{ji}]b_{i白}\<br>&amp;=\max_j\{o.1\times0.5,0.16\times0.3,0.28\times0.2\}\times0.5\<br>&amp;=0.028<br>\end{aligned}$$</p><p>$$\psi_2(1)=3$$</p><p>同理：</p><p>$$\delta_2(2)=0.0504,\quad\psi_2(3)=3$$</p><p>$$\delta_2(3)=0.042,\quad\psi_2(3)=3$$</p><p>同样，$t=3$：</p><p>$$\delta_3(1)=0.00756,\quad\psi_3(1)=2$$</p><p>$$\delta_3(2)=0.01008,\quad\psi_3(2)=2$$</p><p>$$\delta_3(3)=0.0417,\quad\psi_3(3)=3$$</p><ol start="3"><li>最优路径概率:<br> $$P^*=\max_{1\leq i\leq 3}\delta_3(i)=0.0147$$</li></ol><p>$$y_3^*=\arg\max_{1\leq j \leq 3}\delta_3(i)=3$$</p><ol start="4"><li>回溯：<br> $$t=2,\quad y_2^<em>=\psi_3(y_3^</em>)=\psi_3(3)=3$$</li></ol><p>$$t=1,\quad y_1^<em>=\psi_2(y_2^</em>)=\psi_2(3)=3$$</p><p>所以，最优路径为$\boldsymbol{y}^<em>=(y_1^</em>,y_2^<em>,y_3^</em>)=(3,3,3)$。</p><h1 id="5-hmmlearn使用"><a href="#5-hmmlearn使用" class="headerlink" title="5 hmmlearn使用"></a>5 hmmlearn使用</h1><p><a href="https://hmmlearn.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">hmmlearn</a>是一个实现了hmm的python库，安装很简单，使用<code>pip install hmmlearn</code>就行。<br>hmmlearn实现了三种HMM模型，分成两类：</p><ul><li>针对观测状态是连续的：GaussianHMM和GMMHMM（广泛用于语音识别）</li><li>针对观测状态是离散的：MultinomialHMM，也就是上文中提到的。</li></ul><p>对于MultinomialHMM的模型，使用比较简单，”startprob_”参数对应我们的初始状态概率向量$\pi$， “transmat_”对应我们的状态转移矩阵$A$, “emissionprob_”对应我们的发射概率矩阵$B$。</p><p>对于连续观测状态的HMM模型，GaussianHMM类假设观测状态符合高斯分布，而GMMHMM类则假设观测状态符合混合高斯分布。一般情况下我们使用GaussianHMM即高斯分布的观测状态即可。以下对于连续观测状态的HMM模型，我们只讨论GaussianHMM类。</p><p>在GaussianHMM类中，”startprob_”参数对应我们的隐藏状态初始分布$\pi$ , “transmat_”对应我们的状态转移矩阵A, 比较特殊的是发射概率的表示方法，此时由于观测状态是连续值，我们无法像MultinomialHMM一样直接给出矩阵B。而是采用给出各个隐藏状态对应的观测状态高斯分布的概率密度函数的参数。</p><p>如果观测序列是一维的，则观测状态的概率密度函数是一维的普通高斯分布。如果观测序列是<br>N维的，则隐藏状态对应的观测状态的概率密度函数是N维高斯分布。高斯分布的概率密度函数参数可以用μ表示高斯分布的期望向量，Σ表示高斯分布的协方差矩阵。在GaussianHMM类中，“means”用来表示各个隐藏状态对应的高斯分布期望向量μ形成的矩阵，而“covars”用来表示各个隐藏状态对应的高斯分布协方差矩阵Σ形成的三维张量。</p><p>参考：<a href="https://blog.csdn.net/tostq/article/details/70851531?depth_1-utm_source=distribute.pc_relevant.none-task&utm_source=distribute.pc_relevant.none-task" target="_blank" rel="noopener">深度剖析HMM</a></p><h2 id="5-1-pythonceshi工具unnitest"><a href="#5-1-pythonceshi工具unnitest" class="headerlink" title="5.1 pythonceshi工具unnitest"></a>5.1 pythonceshi工具unnitest</h2><p>测试工具unnitest非常容易使用，首先是建立一个继承自TestCase的测试类，然后通过覆盖setUp()完成相关初始化，最后通过覆盖tearDown()方法清除测试中产生的数据，为以后的TestCase留下一个干净的环境。我们需要在测试类中编写以test_开头的测试函数，unnitest会在测试中自动执行以test_开头的测试函数，unnitest的使用框架：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> unittest  <span class="token keyword">class</span> <span class="token class-name">XXXXX</span><span class="token punctuation">(</span>unittest<span class="token punctuation">.</span>TestCase<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">setUp</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment" spellcheck="true"># 自行编写</span>    <span class="token keyword">pass</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>      unittest<span class="token punctuation">.</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-2-离散HMM测试"><a href="#5-2-离散HMM测试" class="headerlink" title="5.2 离散HMM测试"></a>5.2 离散HMM测试</h2><p>对离散HMM模型的测试代码：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 计算平方误差</span><span class="token keyword">def</span> <span class="token function">s_error</span><span class="token punctuation">(</span>A<span class="token punctuation">,</span> B<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> sqrt<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">(</span>A<span class="token operator">-</span>B<span class="token punctuation">)</span><span class="token operator">*</span><span class="token punctuation">(</span>A<span class="token operator">-</span>B<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">/</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>B<span class="token punctuation">)</span><span class="token keyword">class</span> <span class="token class-name">DiscreteHMM_Test</span><span class="token punctuation">(</span>unittest<span class="token punctuation">.</span>TestCase<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">setUp</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 建立两个HMM，隐藏状态个数为4，X可能分布为10类</span>        n_state <span class="token operator">=</span><span class="token number">4</span>        n_feature <span class="token operator">=</span> <span class="token number">10</span>        X_length <span class="token operator">=</span> <span class="token number">1000</span>        n_batch <span class="token operator">=</span> <span class="token number">100</span> <span class="token comment" spellcheck="true"># 批量数目</span>        self<span class="token punctuation">.</span>n_batch <span class="token operator">=</span> n_batch        self<span class="token punctuation">.</span>X_length <span class="token operator">=</span> X_length        self<span class="token punctuation">.</span>test_hmm <span class="token operator">=</span> hmm<span class="token punctuation">.</span>DiscreteHMM<span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>comp_hmm <span class="token operator">=</span> ContrastHMM<span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Z <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token operator">*</span><span class="token number">10</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>train<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X<span class="token punctuation">,</span> self<span class="token punctuation">.</span>Z<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_train_batch</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        Z <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> b <span class="token keyword">in</span> range<span class="token punctuation">(</span>self<span class="token punctuation">.</span>n_batch<span class="token punctuation">)</span><span class="token punctuation">:</span>            b_X<span class="token punctuation">,</span> b_Z <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>            X<span class="token punctuation">.</span>append<span class="token punctuation">(</span>b_X<span class="token punctuation">)</span>            Z<span class="token punctuation">.</span>append<span class="token punctuation">(</span>b_Z<span class="token punctuation">)</span>        batch_hmm <span class="token operator">=</span> hmm<span class="token punctuation">.</span>DiscreteHMM<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>n_state<span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>x_num<span class="token punctuation">)</span>        batch_hmm<span class="token punctuation">.</span>train_batch<span class="token punctuation">(</span>X<span class="token punctuation">,</span> Z<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 判断概率参数是否接近</span>        <span class="token comment" spellcheck="true"># 初始概率判定没有通过！！！</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>batch_hmm<span class="token punctuation">.</span>start_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>batch_hmm<span class="token punctuation">.</span>transmat_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>batch_hmm<span class="token punctuation">.</span>emission_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_train</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment" spellcheck="true"># 判断概率参数是否接近</span>        <span class="token comment" spellcheck="true"># 单批量的初始概率一定是不准的</span>        <span class="token comment" spellcheck="true"># self.assertAlmostEqual(s_error(self.test_hmm.start_prob, self.comp_hmm.module.startprob_), 0, 1)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>transmat_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>emission_prob<span class="token punctuation">,</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_X_prob</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X<span class="token punctuation">,</span>_ <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>        prob_test <span class="token operator">=</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>X_prob<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        prob_comp <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>score<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>prob_test<span class="token punctuation">,</span> prob_comp<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_predict</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X<span class="token punctuation">,</span> _ <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>        prob_next <span class="token operator">=</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>X<span class="token punctuation">,</span>np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>x_num<span class="token number">-1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertEqual<span class="token punctuation">(</span>prob_next<span class="token punctuation">.</span>shape<span class="token punctuation">,</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>n_state<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">test_decode</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        X<span class="token punctuation">,</span>_ <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>sample<span class="token punctuation">(</span>self<span class="token punctuation">.</span>X_length<span class="token punctuation">)</span>        test_decode <span class="token operator">=</span> self<span class="token punctuation">.</span>test_hmm<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        _<span class="token punctuation">,</span> comp_decode <span class="token operator">=</span> self<span class="token punctuation">.</span>comp_hmm<span class="token punctuation">.</span>module<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>X<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>assertAlmostEqual<span class="token punctuation">(</span>s_error<span class="token punctuation">(</span>test_decode<span class="token punctuation">,</span> comp_decode<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">'__main__'</span><span class="token punctuation">:</span>    unittest<span class="token punctuation">.</span>main<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="5-3-HMM测试"><a href="#5-3-HMM测试" class="headerlink" title="5.3 HMM测试"></a>5.3 HMM测试</h2><p>利用hmmlearn初始化一个高斯模型</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ContrastHMM</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>module <span class="token operator">=</span> hmmlearn<span class="token punctuation">.</span>hmm<span class="token punctuation">.</span>GaussianHMM<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_state<span class="token punctuation">,</span>covariance_type<span class="token operator">=</span><span class="token string">"full"</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>n_state<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 转换概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 高斯发射概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>means_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">*</span><span class="token number">10</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>covars_ <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token number">5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>tile<span class="token punctuation">(</span>np<span class="token punctuation">.</span>identity<span class="token punctuation">(</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>n_state<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>利用hmmlearn初始化一个离散HMM模型:</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">ContrastHMM</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n_state<span class="token punctuation">,</span> n_feature<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>module <span class="token operator">=</span> hmmlearn<span class="token punctuation">.</span>hmm<span class="token punctuation">.</span>MultinomialHMM<span class="token punctuation">(</span>n_components<span class="token operator">=</span>n_state<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># 初始概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>n_state<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_ <span class="token operator">/</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>startprob_<span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print self.module.startprob_</span>        <span class="token comment" spellcheck="true"># 转换概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_ <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>transmat_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_state<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print self.module.transmat_</span>        <span class="token comment" spellcheck="true"># 发射概率</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_ <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>random<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_ <span class="token operator">=</span> self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_ <span class="token operator">/</span> np<span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>self<span class="token punctuation">.</span>module<span class="token punctuation">.</span>emissionprob_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span>n_state<span class="token punctuation">,</span>n_feature<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment" spellcheck="true"># print self.module.emissionprob_</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>代码详情请见我的<a href="https://github.com/CodingMarathon/All_Algorithm/tree/master/HMM/easyhmm" target="_blank" rel="noopener">github</a>，请大家帮忙点个star。<br><img src="https://img-blog.csdnimg.cn/20200318170910751.PNG?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0VsZW5zdG9uZQ==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
